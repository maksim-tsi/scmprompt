{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maritime Logistics Case Generation Pipeline\n",
    "\n",
    "This notebook implements a multi-stage case generation system that creates realistic maritime logistics case studies with solutions. The pipeline leverages:\n",
    "\n",
    "1. **Vector Database**: Qdrant Cloud collections with 1,346 maritime logistics datapoints and domain-specific guidelines\n",
    "2. **Generative AI**: Gemini models for structured text generation\n",
    "3. **Knowledge Integration**: Retrieval-augmented generation to incorporate accurate domain knowledge\n",
    "\n",
    "The system follows the approach described in ADR 004-case-generation-strategy, creating cases that demonstrate practical applications of logistics regulations and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Setup and Configuration\n",
    "\n",
    "This section initializes connections to the vector database, sets up the Gemini API, and validates access to necessary data collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized: LLM=gemini-2.0-flash-exp, Embeddings=text-embedding-004\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "from utils.qdrant_client import get_qdrant_client, get_embedding\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google Generative AI with API key\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Model configuration\n",
    "LLM_MODEL = \"gemini-2.0-flash-exp\"  # LLM for text generation\n",
    "EMBEDDING_MODEL = \"text-embedding-004\"  # Model for embeddings\n",
    "\n",
    "# Initialize the LLM model\n",
    "genai_model = genai.GenerativeModel(LLM_MODEL)\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logger = logging.getLogger(\"case_generation\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "print(f\"Models initialized: LLM={LLM_MODEL}, Embeddings={EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports to enable interim storage options\n",
    "import re\n",
    "import uuid\n",
    "from qdrant_client.http import models\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Qdrant Cloud. Available collections:\n",
      "- case_generation_references: 46 points\n",
      "- logistics_datapoints: 1345 points\n"
     ]
    }
   ],
   "source": [
    "# Set collection names\n",
    "DATAPOINTS_COLLECTION = \"maritime_logistics_kb\"  # Your main collection with 1,346 datapoints\n",
    "REFERENCES_COLLECTION = \"case_generation_references\"  # The collection with guidelines & examples\n",
    "\n",
    "# Test connection\n",
    "client = get_qdrant_client()\n",
    "print(f\"Connected to Qdrant Cloud. Available collections:\")\n",
    "collections = client.get_collections().collections\n",
    "for collection in collections:\n",
    "    count = client.count(collection_name=collection.name).count\n",
    "    print(f\"- {collection.name}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous embedding model format: text-embedding-004\n",
      "Updated embedding model format: models/text-embedding-004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:08:23,824 - INFO - Created embeddings with models/text-embedding-004 in 0.35s\n",
      "2025-03-28 17:08:23,824 - INFO - Created embeddings with models/text-embedding-004 in 0.35s\n",
      "2025-03-28 17:08:23,824 - INFO - Created embeddings with models/text-embedding-004 in 0.35s\n",
      "2025-03-28 17:08:23,824 - INFO - Created embeddings with models/text-embedding-004 in 0.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding creation works with updated model format\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update embedding model format for Google Vertex AI compatibility\n",
    "print(f\"Previous embedding model format: {EMBEDDING_MODEL}\")\n",
    "EMBEDDING_MODEL = f\"models/{EMBEDDING_MODEL}\" if not EMBEDDING_MODEL.startswith(\"models/\") else EMBEDDING_MODEL\n",
    "print(f\"Updated embedding model format: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Test that the embedding works with updated format\n",
    "def test_embedding():\n",
    "    try:\n",
    "        test_result = create_embeddings(\"test query\")\n",
    "        if test_result:\n",
    "            print(\"✅ Embedding creation works with updated model format\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Embedding created but returned empty result\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing embeddings: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run the test to verify our configuration works\n",
    "test_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Checkpoint Storage Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_serializable(obj):\n",
    "    \"\"\"Recursively convert objects to be JSON serializable\n",
    "    \n",
    "    This function handles:\n",
    "    - Dictionaries (recursively processed)\n",
    "    - Lists and tuples (recursively processed)\n",
    "    - Custom objects with __dict__ attributes\n",
    "    - Removes logger objects and callable items\n",
    "    - Returns primitive types directly\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    elif isinstance(obj, (str, int, float, bool)):\n",
    "        return obj\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in obj.items() \n",
    "                if k != \"logger\" and not callable(v)}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_json_serializable(i) for i in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        # Convert custom objects to dictionaries\n",
    "        try:\n",
    "            return make_json_serializable(obj.__dict__)\n",
    "        except:\n",
    "            # If that fails, try string representation\n",
    "            return str(obj)\n",
    "    else:\n",
    "        # Default to string representation for other types\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except:\n",
    "            return \"Non-serializable object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_checkpoint(case=None):\n",
    "    \"\"\"Initialize a new case with checkpoint metadata\"\"\"\n",
    "    checkpoint_dir = Path(\"../Data/Checkpoints\")\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if case is None:\n",
    "        case = {}\n",
    "        \n",
    "    # Generate a unique ID if not present\n",
    "    if \"case_id\" not in case:\n",
    "        case[\"case_id\"] = str(uuid.uuid4())\n",
    "    \n",
    "    # Set up checkpoint information\n",
    "    case[\"checkpoint_file\"] = str(checkpoint_dir / f\"case_{case['case_id']}.json\")\n",
    "    case[\"checkpoint_history\"] = []\n",
    "    case[\"last_checkpoint\"] = None\n",
    "    case[\"creation_timestamp\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Initial save\n",
    "    save_checkpoint(case, \"initialized\")\n",
    "    \n",
    "    return case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(case, stage):\n",
    "    \"\"\"Save a checkpoint of the case generation process\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    case_copy = case.copy()\n",
    "    \n",
    "    # Update checkpoint metadata\n",
    "    case_copy[\"last_checkpoint\"] = stage\n",
    "    case_copy[\"checkpoint_time\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Make JSON serializable\n",
    "    case_copy = make_json_serializable(case_copy)\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(os.path.dirname(case_copy[\"checkpoint_file\"]), exist_ok=True)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✓ Checkpoint saved: {stage}\")\n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(case_id=None):\n",
    "    \"\"\"Load a case from a checkpoint file\"\"\"\n",
    "    checkpoint_dir = Path(\"../Data/Checkpoints\")\n",
    "    \n",
    "    if case_id:\n",
    "        # Load specific case\n",
    "        checkpoint_file = checkpoint_dir / f\"case_{case_id}.json\"\n",
    "        if checkpoint_file.exists():\n",
    "            with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            print(f\"No checkpoint found for case ID: {case_id}\")\n",
    "            return None\n",
    "    else:\n",
    "        # Find most recent checkpoint\n",
    "        checkpoint_files = list(checkpoint_dir.glob(\"case_*.json\"))\n",
    "        if not checkpoint_files:\n",
    "            print(\"No checkpoints found\")\n",
    "            return None\n",
    "        \n",
    "        # Sort by modification time (most recent first)\n",
    "        checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        # Load most recent checkpoint\n",
    "        with open(checkpoint_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "            case = json.load(f)\n",
    "            print(f\"Loaded checkpoint from {checkpoint_files[0].name}\")\n",
    "            print(f\"Last stage completed: {case.get('last_checkpoint', 'unknown')}\")\n",
    "            return case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_checkpoints(limit=10, include_completed=False):\n",
    "    \"\"\"List available checkpoints with their status\"\"\"\n",
    "    checkpoint_dir = Path(\"../Data/Checkpoints\")\n",
    "    checkpoint_files = list(checkpoint_dir.glob(\"case_*.json\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"No checkpoints found\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by modification time (most recent first)\n",
    "    checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for file in checkpoint_files[:limit]:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                case = json.load(f)\n",
    "                \n",
    "                # Skip completed cases if requested\n",
    "                if not include_completed and case.get('last_checkpoint') == 'completed':\n",
    "                    continue\n",
    "                    \n",
    "                results.append({\n",
    "                    \"case_id\": case.get(\"case_id\", \"unknown\"),\n",
    "                    \"last_stage\": case.get(\"last_checkpoint\", \"unknown\"),\n",
    "                    \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "                    \"created\": case.get(\"creation_timestamp\", \"unknown\"),\n",
    "                    \"modified\": datetime.fromtimestamp(file.stat().st_mtime).isoformat(),\n",
    "                    \"checkpoint_file\": str(file)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"Found {len(results)} checkpoints:\")\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"{i+1}. [{r['last_stage']}] {r['title']} ({r['modified']})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Logging Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class CaseGenerationLogger:\n",
    "    \"\"\"Logger for case generation pipeline with JSON file output\"\"\"\n",
    "    \n",
    "    def __init__(self, case_id=None, log_dir=\"../Data/Logs\"):\n",
    "        self.case_id = case_id or str(uuid.uuid4())[:8]\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create log filename with case ID\n",
    "        self.log_file = self.log_dir / f\"case_{self.case_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.start_time = datetime.now()\n",
    "        self.stage_timings = {}\n",
    "        self.stage_start_time = None\n",
    "        self.current_stage = None\n",
    "        \n",
    "        # Initialize the log file with header\n",
    "        self.info(\"LOGGING_INITIALIZED\", \"Case generation logging started\", {\n",
    "            \"case_id\": self.case_id,\n",
    "            \"log_file\": str(self.log_file),\n",
    "            \"timestamp\": self.start_time.isoformat()\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ Logging initialized for case {self.case_id}\")\n",
    "    \n",
    "    def _log(self, level, event_type, message, data=None):\n",
    "        \"\"\"Write a log entry to the log file\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"level\": level,\n",
    "            \"case_id\": self.case_id,\n",
    "            \"event\": event_type,\n",
    "            \"message\": message,\n",
    "            \"stage\": self.current_stage\n",
    "        }\n",
    "        \n",
    "        # Add additional data if provided\n",
    "        if data:\n",
    "            log_entry[\"data\"] = data\n",
    "        \n",
    "        # Write to log file\n",
    "        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(log_entry) + \"\\n\")\n",
    "    \n",
    "    def start_stage(self, stage_name):\n",
    "        \"\"\"Start timing a new stage\"\"\"\n",
    "        self.current_stage = stage_name\n",
    "        self.stage_start_time = datetime.now()\n",
    "        self.info(f\"STAGE_START\", f\"Starting stage: {stage_name}\")\n",
    "    \n",
    "    def end_stage(self, success=True, result_summary=None):\n",
    "        \"\"\"End timing for the current stage and log results\"\"\"\n",
    "        if not self.current_stage or not self.stage_start_time:\n",
    "            return\n",
    "        \n",
    "        duration = (datetime.now() - self.stage_start_time).total_seconds()\n",
    "        self.stage_timings[self.current_stage] = duration\n",
    "        \n",
    "        log_data = {\n",
    "            \"duration_seconds\": duration,\n",
    "            \"success\": success\n",
    "        }\n",
    "        \n",
    "        if result_summary:\n",
    "            log_data[\"result_summary\"] = result_summary\n",
    "            \n",
    "        event_type = \"STAGE_COMPLETE\" if success else \"STAGE_FAILED\"\n",
    "        self.info(\n",
    "            event_type, \n",
    "            f\"Stage {self.current_stage} completed in {duration:.2f}s\",\n",
    "            log_data\n",
    "        )\n",
    "    \n",
    "    def debug(self, event_type, message, data=None):\n",
    "        \"\"\"Log debug information\"\"\"\n",
    "        self._log(\"DEBUG\", event_type, message, data)\n",
    "    \n",
    "    def info(self, event_type, message, data=None):\n",
    "        \"\"\"Log informational message\"\"\"\n",
    "        self._log(\"INFO\", event_type, message, data)\n",
    "    \n",
    "    def warning(self, event_type, message, data=None):\n",
    "        \"\"\"Log warning\"\"\"\n",
    "        self._log(\"WARNING\", event_type, message, data)\n",
    "    \n",
    "    def error(self, event_type, message, data=None):\n",
    "        \"\"\"Log error\"\"\"\n",
    "        self._log(\"ERROR\", event_type, message, data)\n",
    "    \n",
    "    def critical(self, event_type, message, data=None):\n",
    "        \"\"\"Log critical error\"\"\"\n",
    "        self._log(\"CRITICAL\", event_type, message, data)\n",
    "    \n",
    "    def log_llm_request(self, model, prompt_length, temperature=None, max_tokens=None):\n",
    "        \"\"\"Log an LLM request being made\"\"\"\n",
    "        self.debug(\"LLM_REQUEST\", f\"Request to {model}\", {\n",
    "            \"model\": model,\n",
    "            \"prompt_length\": prompt_length,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        })\n",
    "    \n",
    "    def log_llm_response(self, model, response_length, duration):\n",
    "        \"\"\"Log an LLM response received\"\"\"\n",
    "        self.debug(\"LLM_RESPONSE\", f\"Response from {model}\", {\n",
    "            \"model\": model,\n",
    "            \"response_length\": response_length,\n",
    "            \"duration_seconds\": duration\n",
    "        })\n",
    "    \n",
    "    def log_data_retrieval(self, query, results_count, duration=None):\n",
    "        \"\"\"Log data retrieval from vector DB\"\"\"\n",
    "        self.debug(\"DATA_RETRIEVAL\", f\"Retrieved {results_count} results\", {\n",
    "            \"query\": query[:100] + \"...\" if len(query) > 100 else query,\n",
    "            \"results_count\": results_count,\n",
    "            \"duration_seconds\": duration\n",
    "        })\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of the logging activity\"\"\"\n",
    "        if self.stage_timings:\n",
    "            total_duration = sum(self.stage_timings.values())\n",
    "            stages = len(self.stage_timings)\n",
    "        else:\n",
    "            total_duration = 0\n",
    "            stages = 0\n",
    "            \n",
    "        return {\n",
    "            \"case_id\": self.case_id,\n",
    "            \"log_file\": str(self.log_file),\n",
    "            \"total_duration\": total_duration,\n",
    "            \"stages_completed\": stages,\n",
    "            \"stage_timings\": self.stage_timings\n",
    "        }\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Log final summary and completion\"\"\"\n",
    "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        self.info(\"GENERATION_COMPLETE\", f\"Case generation completed in {total_duration:.2f}s\", {\n",
    "            \"total_duration_seconds\": total_duration,\n",
    "            \"stage_timings\": self.stage_timings\n",
    "        })\n",
    "        \n",
    "        return self.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Arize Phoenix Tracing Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arize Phoenix setup with VertexAI instrumentation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Phoenix configuration\n",
    "USE_PHOENIX_TRACING = True\n",
    "PHOENIX_PROJECT_ID = \"maritime_logistics_case_generator\"\n",
    "PHOENIX_ENVIRONMENT = \"development\"  # We'll use this in span attributes\n",
    "\n",
    "def setup_phoenix_tracing():\n",
    "    \"\"\"Set up Arize Phoenix tracing with VertexAI instrumentation\"\"\"\n",
    "    try:\n",
    "        # Import required packages\n",
    "        from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
    "        from arize.otel import register\n",
    "        import google.generativeai as genai\n",
    "        \n",
    "        # Get API credentials\n",
    "        SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\") \n",
    "        API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "        \n",
    "        if not SPACE_ID or not API_KEY:\n",
    "            print(\"⚠️ Missing ARIZE_SPACE_ID or ARIZE_API_KEY in .env file\")\n",
    "            return False\n",
    "            \n",
    "        # Initialize tracing - removing the environment parameter\n",
    "        tracer_provider = register(\n",
    "            space_id=SPACE_ID,\n",
    "            api_key=API_KEY,\n",
    "            project_name=PHOENIX_PROJECT_ID  # No environment parameter\n",
    "        )\n",
    "        \n",
    "        # Instrument with VertexAI\n",
    "        VertexAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "        \n",
    "        # Add custom instrumentation for Google Generative AI\n",
    "        from opentelemetry import trace\n",
    "        \n",
    "        # Get a tracer\n",
    "        tracer = trace.get_tracer(\"google-generativeai-custom\")\n",
    "        \n",
    "        # Store the original method\n",
    "        original_generate_content = genai.GenerativeModel.generate_content\n",
    "        \n",
    "        # Create a traced version - we'll include environment in the span attributes\n",
    "        def traced_generate_content(self, contents, **kwargs):\n",
    "            with tracer.start_as_current_span(\n",
    "                \"google.generativeai.generate_content\",\n",
    "                attributes={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"project_id\": PHOENIX_PROJECT_ID,\n",
    "                    \"environment\": PHOENIX_ENVIRONMENT,  # Include environment as an attribute\n",
    "                    \"service.name\": \"google-generativeai\"\n",
    "                }\n",
    "            ) as span:\n",
    "                # Add input to span\n",
    "                span.set_attribute(\"input.content\", str(contents)[:1000])  # Truncate if too long\n",
    "                \n",
    "                # Add generation parameters to span if present\n",
    "                if kwargs.get(\"generation_config\"):\n",
    "                    config = kwargs[\"generation_config\"]\n",
    "                    if hasattr(config, \"temperature\"):\n",
    "                        span.set_attribute(\"parameter.temperature\", config.temperature)\n",
    "                    if hasattr(config, \"top_p\"):\n",
    "                        span.set_attribute(\"parameter.top_p\", config.top_p)\n",
    "                    if hasattr(config, \"top_k\"):\n",
    "                        span.set_attribute(\"parameter.top_k\", config.top_k)\n",
    "                \n",
    "                # Call the original method\n",
    "                result = original_generate_content(self, contents, **kwargs)\n",
    "                \n",
    "                # Add output to span\n",
    "                if hasattr(result, \"text\"):\n",
    "                    span.set_attribute(\"output.text\", result.text[:1000])  # Truncate if too long\n",
    "                \n",
    "                return result\n",
    "        \n",
    "        # Replace the original method with our traced version\n",
    "        genai.GenerativeModel.generate_content = traced_generate_content\n",
    "        \n",
    "        print(\"✅ Arize Phoenix tracing initialized\")\n",
    "        print(f\"   Project: {PHOENIX_PROJECT_ID}\")\n",
    "        print(f\"   Environment: {PHOENIX_ENVIRONMENT} (set as span attribute)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️ Import error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing Phoenix: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_arize_setup():\n",
    "    \"\"\"Skip Arize Phoenix setup and continue with the notebook\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    # Add placeholder global variables so the rest of the notebook works\n",
    "    global USE_PHOENIX_TRACING, arize_tracing_enabled\n",
    "    global PHOENIX_PROJECT_ID, PHOENIX_ENVIRONMENT\n",
    "    \n",
    "    USE_PHOENIX_TRACING = False\n",
    "    arize_tracing_enabled = False\n",
    "    PHOENIX_PROJECT_ID = \"not_configured\"\n",
    "    PHOENIX_ENVIRONMENT = \"development\"\n",
    "    \n",
    "    display(HTML(\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #e8f4f8; border-radius: 5px; margin: 20px 0;\">\n",
    "        <h4 style=\"margin-top: 0;\">✓ Arize Phoenix Setup Skipped</h4>\n",
    "        <p>You can continue using the notebook without Phoenix tracing.</p>\n",
    "        <p style=\"font-size: 0.9em; color: #555;\">\n",
    "            The case generation system will work normally, but you won't see traces in the Phoenix dashboard.\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔭 OpenTelemetry Tracing Details 🔭\n",
      "|  Arize Project: maritime_logistics_case_generator\n",
      "|  Span Processor: BatchSpanProcessor\n",
      "|  Collector Endpoint: otlp.arize.com\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'space_id': '****', 'api_key': '****', 'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n",
      "✅ Arize Phoenix tracing initialized\n",
      "   Project: maritime_logistics_case_generator\n",
      "   Environment: development (set as span attribute)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phoenix tracing if enabled\n",
    "arize_tracing_enabled = True\n",
    "if USE_PHOENIX_TRACING:\n",
    "    arize_tracing_enabled = setup_phoenix_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation successful!\n",
      "\n",
      "Generated text:\n",
      "--------------------------------------------------\n",
      "Maritime logistics encompasses the planning, execution, and control of the movement and storage of goods via sea, ensuring efficient and timely delivery across global supply chains.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Check the Phoenix dashboard to see the trace.\n"
     ]
    }
   ],
   "source": [
    "# Test Phoenix tracing with a simple generation\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Initialize model with our global variable\n",
    "LLM_MODEL = \"gemini-2.0-flash-exp\"\n",
    "genai_model = genai.GenerativeModel(LLM_MODEL)\n",
    "\n",
    "def test_phoenix_tracing():\n",
    "    \"\"\"Test function to verify Phoenix tracing\"\"\"\n",
    "    try:\n",
    "        # Simple test prompt\n",
    "        test_prompt = \"Write a one-sentence description of maritime logistics.\"\n",
    "        \n",
    "        # Generate with tracing\n",
    "        response = genai_model.generate_content(\n",
    "            test_prompt,\n",
    "            generation_config={\"temperature\": 0.7}\n",
    "        )\n",
    "        \n",
    "        if response.text:\n",
    "            print(\"✅ Generation successful!\")\n",
    "            print(\"\\nGenerated text:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(response.text)\n",
    "            print(\"-\" * 50)\n",
    "            print(\"\\nCheck the Phoenix dashboard to see the trace.\")\n",
    "        else:\n",
    "            print(\"❌ No text generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during test: {str(e)}\")\n",
    "\n",
    "# Run the test\n",
    "test_phoenix_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Stage Pipeline Implementation\n",
    "\n",
    "The case generation process follows five distinct stages, each building on the previous to create increasingly refined and accurate content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Case Draft Generation\n",
    "\n",
    "**Process**: Selects a random example case from our reference collection and uses it as inspiration to generate a new, unique case draft focused on container shipping logistics between Asia and Northern Europe/Baltic.\n",
    "\n",
    "**Expected Output**: Initial case scenario with realistic operational challenges, fictional but plausible entities, and a clear problem to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "logger = logging.getLogger(\"case_generation\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_example():\n",
    "    \"\"\"Select a random example case from the database\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    try:\n",
    "        # Get all examples\n",
    "        examples, _ = client.scroll(\n",
    "            collection_name=REFERENCES_COLLECTION,\n",
    "            scroll_filter=models.Filter(  # Ensure this is scroll_filter, not filter\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"content_type\",\n",
    "                        match=models.MatchValue(value=\"example\")\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit=100,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        if not examples:\n",
    "            print(\"⚠️ No example cases found in the database\")\n",
    "            # Return a default example\n",
    "            return {\n",
    "                \"title\": \"Default Example\",\n",
    "                \"summary\": \"This is a default example case for maritime logistics training. It involves a vessel carrying containers between ports, with delays and regulatory issues.\",\n",
    "                \"filename\": \"default_example.md\"\n",
    "            }\n",
    "        \n",
    "        # Select a random example\n",
    "        example = random.choice(examples)\n",
    "        print(f\"✅ Selected example: {example.payload.get('title', 'Untitled')}\")\n",
    "        \n",
    "        return example.payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error retrieving examples: {str(e)}\")\n",
    "        print(\"Using default example instead.\")\n",
    "        # Return a default example as fallback\n",
    "        return {\n",
    "            \"title\": \"Default Example\",\n",
    "            \"summary\": \"This is a default example case for maritime logistics training. It involves a vessel carrying containers between ports, with delays and regulatory issues.\",\n",
    "            \"filename\": \"default_example.md\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_llm(prompt, temperature=0.7):\n",
    "    \"\"\"Generate text with LLM, with Phoenix tracing happening automatically\"\"\"\n",
    "    try:\n",
    "        # Start measuring time for our own logging\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use the model (tracing happens automatically through instrumentation)\n",
    "        response = genai_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"temperature\": temperature}\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            response_text = response.candidates[0].content.parts[0].text\n",
    "            logger.info(f\"Generated content with {LLM_MODEL}: {len(response_text)} chars in {duration:.2f}s\")\n",
    "            return response_text\n",
    "        else:\n",
    "            logger.warning(\"No response generated\")\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Error generating with LLM {LLM_MODEL}: {error_msg}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_case_draft(example_case):\n",
    "    \"\"\"Generate initial case draft based on an example case\"\"\"\n",
    "    # Create prompt for case generation\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with creating a new case study for maritime logistics training.\n",
    "    \n",
    "    I'll provide you with an EXAMPLE CASE for inspiration. Your task is to create a NEW CASE that:\n",
    "    1. Is in a similar domain but with entirely different details\n",
    "    2. Focuses on container shipping logistics between Asia and Northern Europe/Baltic\n",
    "    3. Involves realistic operational challenges\n",
    "    4. References specific ports, companies, and vessels (use realistic but fictional names)\n",
    "    5. Presents a clear problem that needs resolution\n",
    "    \n",
    "    DO NOT copy the example directly - create something new that tests similar knowledge.\n",
    "    \n",
    "    EXAMPLE CASE SUMMARY:\n",
    "    {example_case.get('summary', 'No summary available')}\n",
    "    \n",
    "    NEW CASE (write only the case description, not the solution):\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating initial case draft...\")\n",
    "    case_draft = generate_with_llm(\n",
    "        prompt,\n",
    "        temperature=0.7  # Using default temperature, model is set in generate_with_llm\n",
    "    )\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    return {\n",
    "        \"example_inspiration\": example_case.get('title', 'Unknown example'),\n",
    "        \"example_filename\": example_case.get('filename', 'unknown.md'),\n",
    "        \"draft_case\": case_draft,\n",
    "        \"creation_date\": time.strftime(\"%Y-%m-%d\"),\n",
    "        \"stage\": \"draft\",\n",
    "        \"model\": LLM_MODEL  # Store the model used for reference\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    \"\"\"Create embeddings using the specified embedding model\"\"\"\n",
    "    try:\n",
    "        # Start measuring time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embedding = genai.embed_content(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            content=text,\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"Created embeddings with {EMBEDDING_MODEL} in {duration:.2f}s\")\n",
    "        \n",
    "        # Return the embedding values\n",
    "        return embedding[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating embeddings with {EMBEDDING_MODEL}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 14:17:44,667 - INFO - Generated content with gemini-2.0-flash-exp: 572 chars in 1.35s\n",
      "2025-03-28 14:17:44,667 - INFO - Generated content with gemini-2.0-flash-exp: 572 chars in 1.35s\n",
      "2025-03-28 14:17:44,667 - INFO - Generated content with gemini-2.0-flash-exp: 572 chars in 1.35s\n",
      "2025-03-28 14:17:44,667 - INFO - Generated content with gemini-2.0-flash-exp: 572 chars in 1.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maritime logistics is the backbone of global trade, encompassing the intricate processes of moving goods across oceans and waterways. It involves a complex network of ports, ships, terminals, and various transportation modes, all coordinated to ensure the efficient and cost-effective delivery of cargo. From managing customs clearance and documentation to optimizing vessel routes and container handling, maritime logistics plays a crucial role in connecting manufacturers, suppliers, and consumers worldwide, facilitating the flow of goods that fuel the global economy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test generation with tracing\n",
    "test_result = generate_with_llm(\"Write a short paragraph about maritime logistics.\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Critical Analysis & Query Generation\n",
    "\n",
    "**Process**: Analyzes the draft case against guidelines, identifies knowledge gaps, and generates search queries to find relevant regulations and information.\n",
    "\n",
    "**Expected Output**: Structured analysis with specific search queries and keywords for retrieving relevant datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_generate_queries(case_draft):\n",
    "    \"\"\"Analyze the case draft and generate queries for datapoint retrieval\"\"\"\n",
    "    \n",
    "    # Load general guidelines\n",
    "    guidelines = get_general_guidelines()\n",
    "    \n",
    "    # Create prompt for analysis\n",
    "    prompt = f\"\"\"\n",
    "    You are a maritime logistics expert analyzing a draft case study.\n",
    "    \n",
    "    CASE DRAFT:\n",
    "    {case_draft['draft_case']}\n",
    "    \n",
    "    GUIDELINES FOR CASE QUALITY:\n",
    "    {guidelines[:2000]}\n",
    "    \n",
    "    Your task:\n",
    "    1. Identify key topics, entities, and regulations mentioned in the case\n",
    "    2. Generate 5-8 specific search queries that would help find relevant datapoints \n",
    "    3. List 3-5 areas where the case could be improved with more specific regulatory details\n",
    "    \n",
    "    Format your response as follows:\n",
    "    \n",
    "    CASE ANALYSIS:\n",
    "    [Brief analysis of the current case draft]\n",
    "    \n",
    "    KEY TOPICS:\n",
    "    - Topic 1\n",
    "    - Topic 2\n",
    "    [etc.]\n",
    "    \n",
    "    SEARCH QUERIES:\n",
    "    1. [Specific query 1]\n",
    "    2. [Specific query 2]\n",
    "    [etc.]\n",
    "    \n",
    "    AREAS FOR IMPROVEMENT:\n",
    "    1. [Area 1 - what regulatory or factual detail is needed]\n",
    "    2. [Area 2 - what regulatory or factual detail is needed]\n",
    "    [etc.]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing case draft and generating queries...\")\n",
    "    analysis = generate_with_llm(prompt, temperature=0.2)  # Lower temperature for analysis\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Update case information\n",
    "    case_draft.update({\n",
    "        \"analysis\": analysis,\n",
    "        \"stage\": \"analysis\",\n",
    "        \"model\": LLM_MODEL  # Store the model used\n",
    "    })\n",
    "    \n",
    "    return case_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_guidelines():\n",
    "    \"\"\"Retrieve general case generation guidelines\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    # Get guidelines\n",
    "    guidelines, _ = client.scroll(\n",
    "        collection_name=REFERENCES_COLLECTION,\n",
    "        scroll_filter=models.Filter(  # FIXED: 'filter' → 'scroll_filter'\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"guideline_type\",\n",
    "                    match=models.MatchValue(value=\"general\")\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=100,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    if not guidelines:\n",
    "        return \"No guidelines available\"\n",
    "    \n",
    "    # Combine all guideline chunks\n",
    "    combined = \"\"\n",
    "    for guideline in guidelines:\n",
    "        combined += guideline.payload.get('content', '') + \"\\n\\n\"\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Datapoint Retrieval\n",
    "\n",
    "**Process**: Uses the generated queries and keywords to search the vector database for relevant maritime regulations, requirements, and procedures.\n",
    "\n",
    "**Expected Output**: Collection of contextually relevant datapoints that can inform case enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_datapoints(case_draft, max_points=15):\n",
    "    \"\"\"Find datapoints relevant to the case based on analysis\"\"\"\n",
    "    # Extract queries from analysis\n",
    "    analysis = case_draft.get('analysis', '')\n",
    "    \n",
    "    # Simple extraction of queries (in production, use more robust parsing)\n",
    "    queries = []\n",
    "    in_queries_section = False\n",
    "    for line in analysis.split('\\n'):\n",
    "        if 'SEARCH QUERIES:' in line:\n",
    "            in_queries_section = True\n",
    "            continue\n",
    "        if in_queries_section and line.strip() and not line.startswith('AREAS FOR'):\n",
    "            # Strip numbers and punctuation\n",
    "            query = line.strip()\n",
    "            for prefix in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '- ']:\n",
    "                if query.startswith(prefix):\n",
    "                    query = query[len(prefix):].strip()\n",
    "            queries.append(query)\n",
    "        if 'AREAS FOR' in line:\n",
    "            in_queries_section = False\n",
    "    \n",
    "    print(f\"Extracted {len(queries)} queries from analysis:\")\n",
    "    for q in queries:\n",
    "        print(f\"- {q}\")\n",
    "        \n",
    "    # Get relevant datapoints using each query\n",
    "    all_datapoints = []\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    for query in queries:\n",
    "        query_embedding = get_embedding(query)\n",
    "        if not query_embedding:\n",
    "            print(f\"Warning: Failed to generate embedding for query '{query}'\")\n",
    "            continue\n",
    "            \n",
    "        results = client.search(\n",
    "            collection_name=DATAPOINTS_COLLECTION,\n",
    "            query_vector=query_embedding,\n",
    "            limit=5  # Get top 5 per query\n",
    "        )\n",
    "        \n",
    "        # Add results to our list, avoiding duplicates\n",
    "        for result in results:\n",
    "            datapoint_id = result.id\n",
    "            if not any(dp['id'] == datapoint_id for dp in all_datapoints):\n",
    "                all_datapoints.append({\n",
    "                    'id': datapoint_id,\n",
    "                    'score': result.score,\n",
    "                    'payload': result.payload,\n",
    "                    'query': query\n",
    "                })\n",
    "    \n",
    "    # Sort by relevance and limit\n",
    "    all_datapoints = sorted(all_datapoints, key=lambda x: x['score'], reverse=True)\n",
    "    all_datapoints = all_datapoints[:max_points]\n",
    "    \n",
    "    print(f\"Found {len(all_datapoints)} relevant datapoints\")\n",
    "    \n",
    "    return all_datapoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_guideline(case_draft):\n",
    "    \"\"\"Find the most relevant domain-specific guideline\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    # Get embedding for the case draft\n",
    "    case_text = case_draft.get('draft_case', '')\n",
    "    case_embedding = get_embedding(case_text)\n",
    "    \n",
    "    if not case_embedding:\n",
    "        print(\"Warning: Failed to generate embedding for case draft\")\n",
    "        return \"No relevant guidelines found\"\n",
    "    \n",
    "    # Search for guidelines (excluding general guidelines)\n",
    "    guidelines = client.search(\n",
    "        collection_name=REFERENCES_COLLECTION,\n",
    "        query_vector=case_embedding,\n",
    "        filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"content_type\",\n",
    "                    match=models.MatchValue(value=\"guideline\")\n",
    "                )\n",
    "            ],\n",
    "            must_not=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"guideline_type\",\n",
    "                    match=models.MatchValue(value=\"general\")\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    if not guidelines:\n",
    "        return \"No relevant guidelines found\"\n",
    "    \n",
    "    # Get most relevant guideline content\n",
    "    top_guideline = guidelines[0].payload\n",
    "    guideline_type = top_guideline.get('guideline_type', 'unknown')\n",
    "    content = top_guideline.get('content', 'No content available')\n",
    "    \n",
    "    print(f\"Found relevant guideline type: {guideline_type}\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Contextual Enhancement\n",
    "\n",
    "**Process**: Identifies the most relevant domain-specific guideline based on case content, then enhances the case with specific regulatory details from retrieved datapoints.\n",
    "\n",
    "**Expected Output**: An improved case that incorporates specific regulations, requirements, and realistic logistics processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_case_with_context(case_draft, datapoints, guideline):\n",
    "    \"\"\"Enhance the case draft with datapoints and guidelines\"\"\"\n",
    "    \n",
    "    # Prepare datapoints in a readable format\n",
    "    datapoint_text = \"\"\n",
    "    for i, dp in enumerate(datapoints, 1):\n",
    "        payload = dp['payload']\n",
    "        datapoint_text += f\"DATAPOINT {i}:\\n\"\n",
    "        datapoint_text += f\"Type: {payload.get('datapoint_type', 'Unknown')}\\n\"\n",
    "        datapoint_text += f\"Entity: {payload.get('relevant_entity', 'Unknown')}\\n\"\n",
    "        datapoint_text += f\"Content: {payload.get('content', 'No content')}\\n\\n\"\n",
    "    \n",
    "    # Create prompt for enhancement\n",
    "    prompt = f\"\"\"\n",
    "    You are a maritime logistics expert improving a case study with specific factual and regulatory details.\n",
    "    \n",
    "    ORIGINAL CASE DRAFT:\n",
    "    {case_draft['draft_case']}\n",
    "    \n",
    "    ANALYSIS AND SUGGESTIONS:\n",
    "    {case_draft['analysis']}\n",
    "    \n",
    "    RELEVANT DOMAIN GUIDELINE:\n",
    "    {guideline[:2000]}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoint_text[:3000]}\n",
    "    \n",
    "    Your task is to enhance the original case draft by:\n",
    "    1. Adding specific regulatory references from the datapoints\n",
    "    2. Including more detailed logistics processes mentioned in the guidelines\n",
    "    3. Making the scenario more realistic and detailed\n",
    "    4. Ensuring the case aligns with industry best practices\n",
    "    5. Developing the case narrative with attention to the guidelines\n",
    "    \n",
    "    ENHANCED CASE:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Enhancing case with contextual information...\")\n",
    "    # Using higher temperature for creative enhancement\n",
    "    enhanced_case = generate_with_llm(prompt, temperature=0.7)\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Update case information\n",
    "    case_draft.update({\n",
    "        \"enhanced_case\": enhanced_case,\n",
    "        \"datapoints_used\": [dp['id'] for dp in datapoints],\n",
    "        \"relevant_guideline\": guideline[:500] + \"...\",\n",
    "        \"stage\": \"enhanced\",\n",
    "        \"model\": LLM_MODEL  # Store the model used\n",
    "    })\n",
    "    \n",
    "    return case_draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5: Solution Development\n",
    "\n",
    "**Process**: Generates a comprehensive solution addressing all aspects of the enhanced case, referencing specific regulations and providing clear guidance.\n",
    "\n",
    "**Expected Output**: Professional consulting-style solution with step-by-step recommendations and regulatory justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(case_draft, datapoints):\n",
    "    \"\"\"Generate a solution for the enhanced case\"\"\"\n",
    "    \n",
    "    # Prepare datapoints in a readable format\n",
    "    datapoint_text = \"\"\n",
    "    for i, dp in enumerate(datapoints, 1):\n",
    "        payload = dp['payload']\n",
    "        datapoint_text += f\"DATAPOINT {i}:\\n\"\n",
    "        datapoint_text += f\"Type: {payload.get('datapoint_type', 'Unknown')}\\n\"\n",
    "        datapoint_text += f\"Entity: {payload.get('relevant_entity', 'Unknown')}\\n\"\n",
    "        datapoint_text += f\"Content: {payload.get('content', 'No content')}\\n\\n\"\n",
    "    \n",
    "    # Create prompt for solution\n",
    "    prompt = f\"\"\"\n",
    "    You are a maritime logistics expert providing a solution to a case study.\n",
    "    \n",
    "    CASE:\n",
    "    {case_draft.get('enhanced_case', case_draft['draft_case'])}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoint_text[:3000]}\n",
    "    \n",
    "    Your task is to provide a comprehensive solution that:\n",
    "    1. Addresses the core issues in the case\n",
    "    2. References specific regulations and requirements from the datapoints\n",
    "    3. Provides step-by-step guidance on how to resolve the situation\n",
    "    4. Recommends best practices for similar situations in the future\n",
    "    5. Includes any documentation or stakeholder communication needed\n",
    "    \n",
    "    Format your response as a professional consulting solution.\n",
    "    \n",
    "    SOLUTION:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating solution for the case...\")\n",
    "    # Medium temperature (0.5) for professional yet creative solution\n",
    "    solution = generate_with_llm(prompt, temperature=0.5)\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Format as final case-solution pair\n",
    "    final_case = {\n",
    "        \"title\": generate_case_title(case_draft),\n",
    "        \"case\": case_draft.get('enhanced_case', case_draft['draft_case']),\n",
    "        \"solution\": solution,\n",
    "        \"metadata\": {\n",
    "            \"creation_date\": case_draft['creation_date'],\n",
    "            \"inspiration\": case_draft['example_inspiration'],\n",
    "            \"datapoints_used\": case_draft.get('datapoints_used', []),\n",
    "            \"process\": \"Four-stage guided generation with context enhancement\",\n",
    "            \"model\": LLM_MODEL  # Add model information to metadata\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return final_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_case_title(case_draft):\n",
    "    \"\"\"Generate a title for the case\"\"\"\n",
    "    \n",
    "    # Use the enhanced case if available, otherwise use the draft\n",
    "    case_text = case_draft.get('enhanced_case', case_draft['draft_case'])\n",
    "    \n",
    "    # Create prompt for title generation\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following case study, create a concise, professional title that:\n",
    "    1. Captures the core challenge or situation\n",
    "    2. Mentions the key company or organization\n",
    "    3. Is under 10 words in length\n",
    "    \n",
    "    CASE:\n",
    "    {case_text[:1000]}\n",
    "    \n",
    "    TITLE:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate title with very low temperature for precision\n",
    "    title = generate_with_llm(prompt, temperature=0.1)\n",
    "    \n",
    "    # Clean up the title (remove quotes or extra formatting)\n",
    "    title = title.strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "def get_general_guideline():\n",
    "    \"\"\"Retrieve the general case generation guideline using configured embedding model\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    # Query for general guidelines\n",
    "    results = client.search(\n",
    "        collection_name=REFERENCES_COLLECTION,\n",
    "        query_vector=create_embeddings(\"general case study guidelines maritime logistics\"),\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"guideline_type\",\n",
    "                    match=models.MatchValue(value=\"general\")\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=10,\n",
    "        with_payload=True,\n",
    "        with_vectors=False  # No need to return vectors\n",
    "    )\n",
    "    \n",
    "    # Combine all general guideline chunks\n",
    "    guideline_text = \"\"\n",
    "    for result in results:\n",
    "        if \"content\" in result.payload:\n",
    "            guideline_text += result.payload[\"content\"] + \"\\n\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": guideline_text,\n",
    "        \"model\": EMBEDDING_MODEL,  # Track which embedding model was used\n",
    "        \"chunks_found\": len(results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_case_draft(case_draft):\n",
    "    \"\"\"Analyze the case draft and generate search queries\"\"\"\n",
    "    # Get general guidelines\n",
    "    guideline = get_general_guideline()\n",
    "    \n",
    "    # Create prompt for analysis\n",
    "    prompt = f\"\"\"\n",
    "    I'll provide you with a DRAFT CASE and GUIDELINES for case creation. Your task is to:\n",
    "    1. Critically analyze the case against the guidelines\n",
    "    2. Identify topics and concepts that need to be researched in our datapoints\n",
    "    3. Generate search queries to find relevant regulations and requirements\n",
    "    \n",
    "    GUIDELINES:\n",
    "    {guideline[:3000]}  # Truncate if too long\n",
    "    \n",
    "    DRAFT CASE:\n",
    "    {case_draft[\"draft_case\"]}\n",
    "    \n",
    "    Please respond with:\n",
    "    \n",
    "    ## Case Analysis\n",
    "    [Provide a critical assessment of the draft case against the guidelines]\n",
    "    \n",
    "    ## Knowledge Gaps\n",
    "    [List specific areas where more regulatory/requirement information is needed]\n",
    "    \n",
    "    ## Search Queries\n",
    "    [Provide 10 specific search queries that would help find relevant datapoints in our database]\n",
    "    \n",
    "    ## Keywords\n",
    "    [List 10-15 specific keywords that are most relevant to this case]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing case draft and generating search queries...\")\n",
    "    # Low temperature for analytical task\n",
    "    analysis = generate_with_llm(prompt, temperature=0.3)\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Add analysis to the case\n",
    "    case_draft[\"analysis\"] = analysis\n",
    "    case_draft[\"stage\"] = \"analyzed\"\n",
    "    case_draft[\"model\"] = LLM_MODEL  # Add model information\n",
    "    \n",
    "    return case_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries_and_keywords(case):\n",
    "    \"\"\"Extract search queries and keywords from the analysis\"\"\"\n",
    "    analysis = case.get(\"analysis\", \"\")\n",
    "    \n",
    "    # Try to extract queries\n",
    "    queries = []\n",
    "    if \"## Search Queries\" in analysis:\n",
    "        queries_section = analysis.split(\"## Search Queries\")[1].split(\"##\")[0]\n",
    "        for line in queries_section.strip().split(\"\\n\"):\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and not clean_line.startswith(\"#\"):\n",
    "                # Remove leading numbers or bullet points\n",
    "                clean_line = re.sub(r\"^\\d+\\.\\s*|\\-\\s*\", \"\", clean_line)\n",
    "                if clean_line:\n",
    "                    queries.append(clean_line)\n",
    "    \n",
    "    # Try to extract keywords\n",
    "    keywords = []\n",
    "    if \"## Keywords\" in analysis:\n",
    "        keywords_section = analysis.split(\"## Keywords\")[1].split(\"##\")[0] if \"##\" in analysis.split(\"## Keywords\")[1] else analysis.split(\"## Keywords\")[1]\n",
    "        for line in keywords_section.strip().split(\"\\n\"):\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and not clean_line.startswith(\"#\"):\n",
    "                # Remove leading numbers or bullet points\n",
    "                clean_line = re.sub(r\"^\\d+\\.\\s*|\\-\\s*\", \"\", clean_line)\n",
    "                if clean_line:\n",
    "                    for word in clean_line.split(\",\"):\n",
    "                        word = word.strip()\n",
    "                        if word:\n",
    "                            keywords.append(word)\n",
    "    \n",
    "    # If no queries or keywords found, generate some default ones\n",
    "    if not queries:\n",
    "        print(\"No search queries found in analysis, using defaults\")\n",
    "        queries = [\"maritime logistics\", \"container shipping\", \"customs requirements\"]\n",
    "    \n",
    "    if not keywords:\n",
    "        print(\"No keywords found in analysis, using defaults\")\n",
    "        keywords = [\"logistics\", \"shipping\", \"maritime\", \"container\"]\n",
    "    \n",
    "    return queries, keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_datapoints(case):\n",
    "    \"\"\"Retrieve relevant datapoints using the queries and keywords with configured embedding model\"\"\"\n",
    "    # Extract queries and keywords\n",
    "    queries, keywords = extract_queries_and_keywords(case)\n",
    "    print(f\"Using {len(queries)} queries and {len(keywords)} keywords to find relevant datapoints\")\n",
    "    print(f\"Using embedding model: {EMBEDDING_MODEL}\")\n",
    "    \n",
    "    client = get_qdrant_client()\n",
    "    all_datapoints = []\n",
    "    query_stats = []  # Track performance of each query\n",
    "    \n",
    "    # Search using each query\n",
    "    for query in tqdm(queries, desc=\"Searching datapoints\"):\n",
    "        query_embedding = create_embeddings(query)  # Use our standard embedding function\n",
    "        if query_embedding:\n",
    "            results = client.search(\n",
    "                collection_name=REFERENCES_COLLECTION,  # Use global constant\n",
    "                query_vector=query_embedding,\n",
    "                limit=5,\n",
    "                with_payload=True,\n",
    "                with_vectors=False  # Save bandwidth by not retrieving vectors\n",
    "            )\n",
    "            \n",
    "            # Track query performance\n",
    "            query_stats.append({\n",
    "                \"query\": query,\n",
    "                \"results_found\": len(results),\n",
    "                \"top_score\": results[0].score if results else 0\n",
    "            })\n",
    "            \n",
    "            for result in results:\n",
    "                # Add score, query info, and embedding model\n",
    "                result.payload[\"search_score\"] = result.score\n",
    "                result.payload[\"matched_query\"] = query\n",
    "                result.payload[\"embedding_model\"] = EMBEDDING_MODEL\n",
    "                all_datapoints.append(result.payload)\n",
    "    \n",
    "    # Deduplicate datapoints\n",
    "    unique_datapoints = []\n",
    "    seen_ids = set()\n",
    "    for datapoint in all_datapoints:\n",
    "        dp_id = datapoint.get(\"id\", str(datapoint))\n",
    "        if dp_id not in seen_ids:\n",
    "            seen_ids.add(dp_id)\n",
    "            unique_datapoints.append(datapoint)\n",
    "    \n",
    "    print(f\"Retrieved {len(unique_datapoints)} unique datapoints\")\n",
    "    \n",
    "    # Add to case with enhanced metadata\n",
    "    case.update({\n",
    "        \"relevant_datapoints\": unique_datapoints,\n",
    "        \"search_queries\": queries,\n",
    "        \"keywords\": keywords,\n",
    "        \"stage\": \"datapoints_retrieved\",\n",
    "        \"search_metadata\": {\n",
    "            \"embedding_model\": EMBEDDING_MODEL,\n",
    "            \"total_queries\": len(queries),\n",
    "            \"unique_datapoints\": len(unique_datapoints),\n",
    "            \"query_stats\": query_stats,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_guideline(case):\n",
    "    \"\"\"Find the most relevant domain-specific guideline using configured embedding model\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    # Create a query from the case draft\n",
    "    query_text = case.get(\"draft_case\", \"\")[:5000]  # Limit length\n",
    "    print(f\"Creating embedding using model: {EMBEDDING_MODEL}\")\n",
    "    query_embedding = create_embeddings(query_text)\n",
    "    \n",
    "    if not query_embedding:\n",
    "        print(f\"❌ Failed to create embedding for case draft using {EMBEDDING_MODEL}\")\n",
    "        return None\n",
    "    \n",
    "    # Search for domain-specific guidelines\n",
    "    results = client.search(\n",
    "        collection_name=REFERENCES_COLLECTION,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"content_type\", \n",
    "                    match=models.MatchValue(value=\"guideline\")\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"guideline_type\",\n",
    "                    match=models.MatchAny(any=[\"bishou\", \"maritime\", \"ocean\"])\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=3,\n",
    "        with_payload=True,\n",
    "        with_vectors=False  # Optimize by not retrieving vectors\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ No relevant guidelines found\")\n",
    "        return None\n",
    "    \n",
    "    # Get the top guideline\n",
    "    top_guideline = results[0].payload\n",
    "    guideline_type = top_guideline.get(\"guideline_type\", \"unknown\")\n",
    "    print(f\"✅ Found relevant guideline: {guideline_type} (score: {results[0].score:.4f})\")\n",
    "    \n",
    "    # Add all relevant guideline chunks\n",
    "    guideline_content = \"\"\n",
    "    for result in results:\n",
    "        if \"content\" in result.payload:\n",
    "            guideline_content += result.payload[\"content\"] + \"\\n\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"guideline_type\": guideline_type,\n",
    "        \"content\": guideline_content,\n",
    "        \"score\": results[0].score,\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"chunks_found\": len(results),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"query_length\": len(query_text)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_case_with_context(case):\n",
    "    \"\"\"Enhance the case with datapoints and domain-specific guidelines\"\"\"\n",
    "    # Find relevant guideline\n",
    "    guideline = find_relevant_guideline(case)\n",
    "    \n",
    "    # Prepare datapoints for prompt\n",
    "    datapoints_text = \"\"\n",
    "    for i, dp in enumerate(case.get(\"relevant_datapoints\", [])[:15]):  # Limit to top 15\n",
    "        datapoints_text += f\"\\nDATAPOINT {i+1}:\\n\"\n",
    "        datapoints_text += f\"Title: {dp.get('title', 'Untitled')}\\n\"\n",
    "        datapoints_text += f\"Content: {dp.get('content', 'No content')}\\n\"\n",
    "        if \"port_area\" in dp:\n",
    "            datapoints_text += f\"Port Area: {dp.get('port_area', 'Unknown')}\\n\"\n",
    "        if \"relevant_entity\" in dp:\n",
    "            datapoints_text += f\"Entity: {dp.get('relevant_entity', 'Unknown')}\\n\"\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"\n",
    "    I'll provide you with a DRAFT CASE, DATAPOINTS, and DOMAIN GUIDELINES. Your task is to:\n",
    "    1. Enhance the case with specific regulatory details from the datapoints\n",
    "    2. Ensure it follows domain-specific guidelines \n",
    "    3. Make the scenario more realistic and educational\n",
    "    4. Add a clear title for the case\n",
    "    \n",
    "    DRAFT CASE:\n",
    "    {case.get(\"draft_case\", \"\")}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoints_text}\n",
    "    \n",
    "    DOMAIN GUIDELINES:\n",
    "    {guideline.get(\"content\", \"\") if guideline else \"No specific guidelines available\"}\n",
    "    \n",
    "    Please respond with:\n",
    "    \n",
    "    ## Case Title\n",
    "    [Provide a concise, descriptive title for this case]\n",
    "    \n",
    "    ## Enhanced Case\n",
    "    [Provide the improved case with specific regulations and requirements from the datapoints]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Enhancing case with contextual information using {LLM_MODEL}...\")\n",
    "    enhanced_content = generate_with_llm(\n",
    "        prompt,\n",
    "        temperature=0.7  # Higher temperature for creative enhancement\n",
    "    )\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Extract title and enhanced case\n",
    "    title = \"Untitled Case\"\n",
    "    enhanced_case = enhanced_content\n",
    "    \n",
    "    if \"## Case Title\" in enhanced_content:\n",
    "        parts = enhanced_content.split(\"## Case Title\")\n",
    "        title_section = parts[1].split(\"##\")[0]\n",
    "        title = title_section.strip()\n",
    "        \n",
    "    if \"## Enhanced Case\" in enhanced_content:\n",
    "        parts = enhanced_content.split(\"## Enhanced Case\")\n",
    "        enhanced_case = parts[1].strip()\n",
    "    \n",
    "    # Update case with enhanced metadata\n",
    "    case.update({\n",
    "        \"title\": title,\n",
    "        \"enhanced_case\": enhanced_case,\n",
    "        \"domain_guideline\": guideline.get(\"guideline_type\") if guideline else \"none\",\n",
    "        \"stage\": \"enhanced\",\n",
    "        \"enhancement_metadata\": {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"datapoints_used\": len(case.get(\"relevant_datapoints\", [])),\n",
    "            \"guideline_type\": guideline.get(\"guideline_type\") if guideline else \"none\",\n",
    "            \"enhancement_version\": \"2.0\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_solution(case):\n",
    "    \"\"\"Develop a solution for the case using configured LLM model\"\"\"\n",
    "    # Create prompt for solution development\n",
    "    datapoints_text = \"\"\n",
    "    for i, dp in enumerate(case.get(\"relevant_datapoints\", [])[:10]):  # Limit to top 10\n",
    "        datapoints_text += f\"\\nDATAPOINT {i+1}:\\n\"\n",
    "        datapoints_text += f\"Title: {dp.get('title', 'Untitled')}\\n\"\n",
    "        datapoints_text += f\"Content: {dp.get('content', 'No content')}\\n\"\n",
    "        if \"relevant_entity\" in dp:\n",
    "            datapoints_text += f\"Entity: {dp.get('relevant_entity', 'Unknown')}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    I'll provide you with an ENHANCED CASE. Your task is to:\n",
    "    1. Develop a comprehensive solution that addresses all aspects of the case\n",
    "    2. Reference specific regulations and requirements that apply\n",
    "    3. Explain the reasoning behind the solution\n",
    "    4. Structure the solution clearly with steps/recommendations\n",
    "    \n",
    "    CASE TITLE:\n",
    "    {case.get(\"title\", \"Untitled Case\")}\n",
    "    \n",
    "    CASE SCENARIO:\n",
    "    {case.get(\"enhanced_case\", case.get(\"draft_case\", \"\"))}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoints_text}\n",
    "    \n",
    "    Please provide a detailed solution that demonstrates understanding of maritime logistics regulations and requirements.\n",
    "    Structure your response as follows:\n",
    "    \n",
    "    ## Executive Summary\n",
    "    [Brief overview of the solution]\n",
    "    \n",
    "    ## Detailed Solution Steps\n",
    "    [Step-by-step solution with regulatory references]\n",
    "    \n",
    "    ## Recommendations\n",
    "    [Key recommendations and best practices]\n",
    "    \n",
    "    ## Risk Mitigation\n",
    "    [Potential risks and mitigation strategies]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Developing solution using {LLM_MODEL}...\")\n",
    "    solution = generate_with_llm(\n",
    "        prompt,\n",
    "        temperature=0.4  # Lower temperature for more focused solution\n",
    "    )\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Update case with solution and metadata\n",
    "    case.update({\n",
    "        \"solution\": solution,\n",
    "        \"stage\": \"completed\",\n",
    "        \"solution_metadata\": {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"datapoints_referenced\": len(case.get(\"relevant_datapoints\", [])),\n",
    "            \"solution_version\": \"2.0\",\n",
    "            \"prompt_tokens\": len(prompt),  # Approximate token count\n",
    "            \"structure_version\": \"four_part\"  # Track solution structure version\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stage_error(case, stage_name, error):\n",
    "    \"\"\"Log a stage error and print details\"\"\"\n",
    "    error_msg = str(error)\n",
    "    print(f\"\\n❌ ERROR IN {stage_name.upper()}: {error_msg}\")\n",
    "    \n",
    "    if 'logger' in case:\n",
    "        case['logger'].error(f\"{stage_name.upper()}_ERROR\", f\"Error in {stage_name}: {error_msg}\", {\n",
    "            \"error\": error_msg,\n",
    "            \"stage\": stage_name\n",
    "        })\n",
    "    \n",
    "    print(\"\\nStack trace:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    return error_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_checkpoint():\n",
    "    \"\"\"Initialize a new case checkpoint with basic metadata and unique ID\"\"\"\n",
    "    # Generate unique case ID\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
    "    case_id = f\"case-{timestamp}-{random_suffix}\"\n",
    "    \n",
    "    # Create initial case structure\n",
    "    case = {\n",
    "        \"case_id\": case_id,\n",
    "        \"creation_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"last_checkpoint\": \"initialized\",\n",
    "        \"pipeline_version\": \"2.0\",\n",
    "        \"draft_complete\": False,\n",
    "        \"analysis_complete\": False,\n",
    "        \"datapoints_complete\": False,\n",
    "        \"enhancement_complete\": False,\n",
    "        \"solution_complete\": False\n",
    "    }\n",
    "    \n",
    "    return case\n",
    "\n",
    "def save_final_case(case):\n",
    "    \"\"\"Save the final case to output directory\"\"\"\n",
    "    # Create a sanitized file name for the case\n",
    "    title = case.get(\"title\", \"Untitled_Case\")\n",
    "    sanitized_title = \"\".join(c if c.isalnum() or c in \" _-\" else \"_\" for c in title).replace(\" \", \"_\")\n",
    "    \n",
    "    # Get the case ID\n",
    "    case_id = case[\"case_id\"]\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_dir = Path(\"../Output/Generated_Cases\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare the case copy for saving (remove logger)\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    \n",
    "    # Save the file\n",
    "    filepath = output_dir / f\"{sanitized_title}.json\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Final case saved to {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def log_stage_error(case, stage_name, error):\n",
    "    \"\"\"Log an error that occurred during a pipeline stage\"\"\"\n",
    "    error_msg = str(error)\n",
    "    print(f\"❌ Error in {stage_name} stage: {error_msg}\")\n",
    "    \n",
    "    if 'logger' in case and case['logger']:\n",
    "        case['logger'].error(f\"ERROR_{stage_name.upper()}\", f\"Error in {stage_name} stage: {error_msg}\", {\n",
    "            \"error\": error_msg,\n",
    "            \"stage\": stage_name,\n",
    "            \"llm_model\": LLM_MODEL,\n",
    "            \"embedding_model\": EMBEDDING_MODEL\n",
    "        })\n",
    "        case['logger'].end_stage(success=False, result_summary={\"error\": error_msg})\n",
    "\n",
    "def run_case_generation_pipeline(resume_from=None, save_checkpoints=True, debug=True):\n",
    "    \"\"\"Run the full case generation pipeline with checkpoint saving and logging\"\"\"\n",
    "    case = None\n",
    "    \n",
    "    # Debug mode indication\n",
    "    if debug:\n",
    "        print(\"🔍 Running pipeline in DEBUG mode\")\n",
    "    \n",
    "    # Resume from existing checkpoint if requested\n",
    "    if resume_from:\n",
    "        case = load_checkpoint(resume_from)\n",
    "        if not case:\n",
    "            print(f\"Could not load checkpoint {resume_from}, starting new case\")\n",
    "    \n",
    "    # Initialize new case if not resuming\n",
    "    if not case:\n",
    "        print(\"STAGE 0: INITIALIZATION\")\n",
    "        case = initialize_checkpoint()\n",
    "        print(f\"Created new case with ID: {case['case_id']}\")\n",
    "    \n",
    "    # Initialize logger\n",
    "    logger = CaseGenerationLogger(case_id=case['case_id'])\n",
    "    case['logger'] = logger  # Store reference to logger\n",
    "    \n",
    "    # Track the current stage\n",
    "    current_stage = case.get(\"last_checkpoint\", \"initialized\")\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Case Draft Generation\n",
    "        if current_stage in [\"initialized\"]:\n",
    "            logger.start_stage(\"draft_generation\")\n",
    "            print(f\"\\nSTAGE 1: CASE DRAFT GENERATION (using {LLM_MODEL})\")\n",
    "            \n",
    "            try:\n",
    "                if debug: print(\"  1.1 Selecting random example case...\")\n",
    "                example_case = select_random_example()\n",
    "                logger.info(\"EXAMPLE_SELECTED\", f\"Selected example case\", {\n",
    "                    \"example_length\": len(example_case) if example_case else 0\n",
    "                })\n",
    "                \n",
    "                if debug: print(\"  1.2 Generating case draft...\")\n",
    "                # Log LLM request\n",
    "                prompt_length = len(example_case.get(\"summary\", \"\"))[:2000] + 500  # Approximate\n",
    "                logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "                \n",
    "                # Time the LLM call\n",
    "                start_time = datetime.now()\n",
    "                case.update(generate_case_draft(example_case))\n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                # Log LLM response\n",
    "                logger.log_llm_response(\n",
    "                    LLM_MODEL, \n",
    "                    len(case.get(\"draft_case\", \"\")),\n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                display(Markdown(\"### Initial Case Draft\"))\n",
    "                display(Markdown(case[\"draft_case\"]))\n",
    "                \n",
    "                logger.info(\"DRAFT_GENERATED\", \"Case draft generated\", {\n",
    "                    \"draft_length\": len(case.get(\"draft_case\", \"\")),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "                \n",
    "                if save_checkpoints:\n",
    "                    if debug: print(\"  1.3 Saving checkpoint...\")\n",
    "                    save_checkpoint(case, \"draft\")\n",
    "                    \n",
    "                logger.end_stage(result_summary={\n",
    "                    \"draft_length\": len(case.get(\"draft_case\", \"\")),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log_stage_error(case, \"draft_generation\", e)\n",
    "                if not debug:  # If not in debug mode, re-raise to exit\n",
    "                    raise\n",
    "        \n",
    "        # Stage 2: Critical Analysis & Query Generation\n",
    "        if current_stage in [\"initialized\", \"draft\"]:\n",
    "            logger.start_stage(\"analysis\")\n",
    "            print(f\"\\nSTAGE 2: CRITICAL ANALYSIS & QUERY GENERATION (using {LLM_MODEL})\")\n",
    "            \n",
    "            try:\n",
    "                if debug: print(\"  2.1 Analyzing case draft...\")\n",
    "                # Log LLM request details \n",
    "                prompt_length = len(case.get(\"draft_case\", \"\")) + 1000  # Approximate\n",
    "                logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "                \n",
    "                # Time the LLM call\n",
    "                start_time = datetime.now()\n",
    "                case = analyze_case_draft(case)\n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                # Log LLM response\n",
    "                logger.log_llm_response(\n",
    "                    LLM_MODEL, \n",
    "                    len(case.get(\"analysis\", \"\")),\n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                display(Markdown(\"### Case Analysis\"))\n",
    "                display(Markdown(case[\"analysis\"]))\n",
    "                \n",
    "                if debug: print(\"  2.2 Extracting queries and keywords...\")\n",
    "                # Extract and log queries/keywords\n",
    "                queries, keywords = extract_queries_and_keywords(case)\n",
    "                logger.info(\"ANALYSIS_COMPLETE\", \"Case analysis complete\", {\n",
    "                    \"analysis_length\": len(case.get(\"analysis\", \"\")),\n",
    "                    \"query_count\": len(queries),\n",
    "                    \"keyword_count\": len(keywords),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "                \n",
    "                if save_checkpoints:\n",
    "                    if debug: print(\"  2.3 Saving checkpoint...\")\n",
    "                    save_checkpoint(case, \"analyzed\")\n",
    "                    \n",
    "                logger.end_stage(result_summary={\n",
    "                    \"query_count\": len(queries),\n",
    "                    \"keyword_count\": len(keywords),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log_stage_error(case, \"analysis\", e)\n",
    "                if not debug:  # If not in debug mode, re-raise to exit\n",
    "                    raise\n",
    "        \n",
    "        # Stage 3: Retrieve Relevant Datapoints\n",
    "        if current_stage in [\"initialized\", \"draft\", \"analyzed\"]:\n",
    "            logger.start_stage(\"datapoint_retrieval\")\n",
    "            print(f\"\\nSTAGE 3: RETRIEVE RELEVANT DATAPOINTS (using {EMBEDDING_MODEL})\")\n",
    "            \n",
    "            try:\n",
    "                if debug: print(\"  3.1 Retrieving datapoints...\")\n",
    "                # Time datapoint retrieval\n",
    "                start_time = datetime.now()\n",
    "                case = retrieve_relevant_datapoints(case)\n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                datapoint_count = len(case.get(\"relevant_datapoints\", []))\n",
    "                print(f\"Retrieved {datapoint_count} datapoints\")\n",
    "                \n",
    "                logger.info(\"DATAPOINTS_RETRIEVED\", f\"Retrieved {datapoint_count} datapoints\", {\n",
    "                    \"datapoint_count\": datapoint_count,\n",
    "                    \"retrieval_duration\": duration,\n",
    "                    \"embedding_model\": EMBEDDING_MODEL\n",
    "                })\n",
    "                \n",
    "                if save_checkpoints:\n",
    "                    if debug: print(\"  3.2 Saving checkpoint...\")\n",
    "                    save_checkpoint(case, \"datapoints_retrieved\")\n",
    "                    \n",
    "                logger.end_stage(result_summary={\n",
    "                    \"datapoint_count\": datapoint_count,\n",
    "                    \"embedding_model\": EMBEDDING_MODEL\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log_stage_error(case, \"datapoint_retrieval\", e)\n",
    "                if not debug:  # If not in debug mode, re-raise to exit\n",
    "                    raise\n",
    "        \n",
    "        # Stage 4: Contextual Enhancement\n",
    "        if current_stage in [\"initialized\", \"draft\", \"analyzed\", \"datapoints_retrieved\"]:\n",
    "            logger.start_stage(\"enhancement\")\n",
    "            print(f\"\\nSTAGE 4: CONTEXTUAL ENHANCEMENT (using {LLM_MODEL})\")\n",
    "            \n",
    "            try:\n",
    "                if debug: print(\"  4.1 Finding relevant domain-specific guideline...\")\n",
    "                guideline = find_relevant_guideline(case)\n",
    "                \n",
    "                if guideline:\n",
    "                    print(f\"✅ Found guideline: {guideline.get('guideline_type', 'unknown')} with {guideline.get('chunks_found', 0)} chunks\")\n",
    "                else:\n",
    "                    print(\"⚠️ No relevant guidelines found, using default\")\n",
    "                    guideline = {\n",
    "                        \"guideline_type\": \"default_maritime\",\n",
    "                        \"content\": \"Maritime logistics cases should demonstrate realistic challenges in container shipping.\"\n",
    "                    }\n",
    "                \n",
    "                if debug: print(\"  4.2 Enhancing case with context...\")\n",
    "                # Log LLM request\n",
    "                datapoints_text = json.dumps(case.get(\"relevant_datapoints\", [])[:15])\n",
    "                prompt_length = len(case.get(\"draft_case\", \"\")) + len(datapoints_text)\n",
    "                logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "                \n",
    "                # Time the enhancement\n",
    "                start_time = datetime.now()\n",
    "                case = enhance_case_with_context(case)\n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                # Log LLM response\n",
    "                logger.log_llm_response(\n",
    "                    LLM_MODEL, \n",
    "                    len(case.get(\"enhanced_case\", \"\")),\n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                display(Markdown(f\"### Enhanced Case: {case.get('title', 'Untitled Case')}\"))\n",
    "                display(Markdown(case[\"enhanced_case\"]))\n",
    "                \n",
    "                logger.info(\"CASE_ENHANCED\", \"Case enhanced with context\", {\n",
    "                    \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "                    \"enhanced_length\": len(case.get(\"enhanced_case\", \"\")),\n",
    "                    \"domain_guideline\": case.get(\"domain_guideline\", \"none\"),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "                \n",
    "                if save_checkpoints:\n",
    "                    if debug: print(\"  4.3 Saving checkpoint...\")\n",
    "                    save_checkpoint(case, \"enhanced\")\n",
    "                    \n",
    "                logger.end_stage(result_summary={\n",
    "                    \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "                    \"domain_guideline\": case.get(\"domain_guideline\", \"none\"),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log_stage_error(case, \"enhancement\", e)\n",
    "                if not debug:  # If not in debug mode, re-raise to exit\n",
    "                    raise\n",
    "        \n",
    "        # Stage 5: Solution Development\n",
    "        if current_stage in [\"initialized\", \"draft\", \"analyzed\", \"datapoints_retrieved\", \"enhanced\"]:\n",
    "            logger.start_stage(\"solution\")\n",
    "            print(f\"\\nSTAGE 5: SOLUTION DEVELOPMENT (using {LLM_MODEL})\")\n",
    "            \n",
    "            try:\n",
    "                if debug: print(\"  5.1 Developing solution...\")\n",
    "                # Log LLM request\n",
    "                prompt_length = len(case.get(\"enhanced_case\", \"\")) + 1000  # Approximate\n",
    "                logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "                \n",
    "                # Time solution development\n",
    "                start_time = datetime.now()\n",
    "                case = develop_solution(case)\n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                # Log LLM response\n",
    "                logger.log_llm_response(\n",
    "                    LLM_MODEL, \n",
    "                    len(case.get(\"solution\", \"\")),\n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                display(Markdown(\"### Solution\"))\n",
    "                display(Markdown(case[\"solution\"]))\n",
    "                \n",
    "                logger.info(\"SOLUTION_DEVELOPED\", \"Solution developed\", {\n",
    "                    \"solution_length\": len(case.get(\"solution\", \"\")),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "                \n",
    "                if save_checkpoints:\n",
    "                    if debug: print(\"  5.2 Saving checkpoint...\")\n",
    "                    save_checkpoint(case, \"completed\")\n",
    "                    \n",
    "                logger.end_stage(result_summary={\n",
    "                    \"solution_length\": len(case.get(\"solution\", \"\")),\n",
    "                    \"model\": LLM_MODEL\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log_stage_error(case, \"solution\", e)\n",
    "                if not debug:  # If not in debug mode, re-raise to exit\n",
    "                    raise\n",
    "        \n",
    "        # Finalization - Fixed indentation and structure\n",
    "        current_stage = \"finalization\"\n",
    "        try:\n",
    "            if debug: print(\"\\nFINALIZING CASE GENERATION\")\n",
    "            # Add generation metadata\n",
    "            case[\"generation_metadata\"] = {\n",
    "                \"llm_model\": LLM_MODEL,\n",
    "                \"embedding_model\": EMBEDDING_MODEL,\n",
    "                \"completion_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"pipeline_version\": \"2.0\"\n",
    "            }\n",
    "            \n",
    "            # Save the final case to output directory\n",
    "            if debug: print(\"  6.1 Saving final case...\")\n",
    "            filepath = save_final_case(case)  # Using the proper function name\n",
    "            case[\"final_path\"] = str(filepath)\n",
    "            \n",
    "            # Log completion\n",
    "            logger.info(\"CASE_SAVED\", f\"Case saved to {filepath}\", {\n",
    "                \"filepath\": str(filepath),\n",
    "                \"llm_model\": LLM_MODEL,\n",
    "                \"embedding_model\": EMBEDDING_MODEL\n",
    "            })\n",
    "            \n",
    "            if save_checkpoints:\n",
    "                if debug: print(\"  6.2 Saving final checkpoint...\")\n",
    "                save_checkpoint(case, \"completed\")\n",
    "            \n",
    "            # Finalize logging\n",
    "            if debug: print(\"  6.3 Finalizing logs...\")\n",
    "            log_summary = logger.finalize()\n",
    "            case[\"log_summary\"] = log_summary\n",
    "            \n",
    "            print(\"\\n✨ CASE GENERATION COMPLETE ✨\")\n",
    "            return case, filepath\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_stage_error(case, \"finalization\", e)\n",
    "            return case, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n❌ PIPELINE FAILURE: {error_msg}\")\n",
    "        \n",
    "        # Log the error\n",
    "        if 'logger' in case and case['logger']:\n",
    "            case['logger'].error(\"PIPELINE_ERROR\", f\"Error in pipeline: {error_msg}\", {\n",
    "                \"error\": error_msg,\n",
    "                \"stage\": current_stage,\n",
    "                \"llm_model\": LLM_MODEL,\n",
    "                \"embedding_model\": EMBEDDING_MODEL\n",
    "            })\n",
    "            case['logger'].end_stage(success=False, result_summary={\"error\": error_msg})\n",
    "            \n",
    "        if save_checkpoints and case:\n",
    "            # Save checkpoint at point of failure\n",
    "            case[\"error\"] = error_msg\n",
    "            case[\"error_metadata\"] = {\n",
    "                \"llm_model\": LLM_MODEL,\n",
    "                \"embedding_model\": EMBEDDING_MODEL,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            save_checkpoint(case, f\"failed_at_{current_stage}\")\n",
    "            print(f\"✓ Failure checkpoint saved: failed_at_{current_stage}\")\n",
    "            \n",
    "        # Print stack trace in debug mode\n",
    "        if debug:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        return case, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution\n",
    "\n",
    "This section executes the full pipeline and displays the resulting case-solution pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pipeline Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize case and logging\n",
    "def initialize_debug():\n",
    "    \"\"\"Initialize a new case for debugging\"\"\"\n",
    "    print(\"🔍 Starting interactive debugging\")\n",
    "    \n",
    "    # Create a new case\n",
    "    case_id = str(uuid.uuid4())\n",
    "    checkpoint_dir = Path(\"../Data/Checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    case = {\n",
    "        \"case_id\": case_id,\n",
    "        \"checkpoint_file\": str(checkpoint_dir / f\"case_{case_id}.json\"),\n",
    "        \"checkpoint_history\": [],\n",
    "        \"last_checkpoint\": \"initialized\",  # FIXED: Set this explicitly to \"initialized\"\n",
    "        \"creation_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"Created new case with ID: {case['case_id']}\")\n",
    "    print(f\"Current stage: {case['last_checkpoint']}\")\n",
    "    \n",
    "    # Initialize logger\n",
    "    logger = CaseGenerationLogger(case_id=case['case_id'])\n",
    "    case['logger'] = logger\n",
    "    \n",
    "    # Save initial checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]  # Remove logger before serializing\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(os.path.dirname(case_copy[\"checkpoint_file\"]), exist_ok=True)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✓ Checkpoint saved: initialized\")\n",
    "    \n",
    "    return case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting interactive debugging\n",
      "Created new case with ID: 33b0c355-cb29-4e8b-af4f-683c248ce179\n",
      "Current stage: initialized\n",
      "✓ Logging initialized for case 33b0c355-cb29-4e8b-af4f-683c248ce179\n",
      "✓ Checkpoint saved: initialized\n"
     ]
    }
   ],
   "source": [
    "# Run the initialization\n",
    "debug_case = initialize_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Stage 1: Case Draft Generation\n",
    "def debug_stage_1(case):\n",
    "    \"\"\"Run Stage 1: Case Draft Generation\"\"\"\n",
    "    print(f\"\\nSTAGE 1: CASE DRAFT GENERATION (using {LLM_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"draft_generation\")\n",
    "    \n",
    "    print(\"  1.1 Selecting random example case...\")\n",
    "    # Manually specify a simple example case to avoid database issues\n",
    "    example_case = {\n",
    "        \"title\": \"Test Example Case\",\n",
    "        \"summary\": \"A container ship experiences delays at Rotterdam port due to documentation issues and must navigate customs regulations.\",\n",
    "        \"filename\": \"test_example.md\"\n",
    "    }\n",
    "    logger.info(\"EXAMPLE_SELECTED\", f\"Selected example case\", {\n",
    "        \"example_length\": len(example_case[\"summary\"])\n",
    "    })\n",
    "    \n",
    "    print(\"  1.2 Generating case draft...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with creating a new case study for maritime logistics training.\n",
    "    \n",
    "    I'll provide you with an EXAMPLE CASE for inspiration. Your task is to create a NEW CASE that:\n",
    "    1. Is in a similar domain but with entirely different details\n",
    "    2. Focuses on container shipping logistics between Asia and Northern Europe/Baltic\n",
    "    3. Involves realistic operational challenges\n",
    "    4. References specific ports, companies, and vessels (use realistic but fictional names)\n",
    "    5. Presents a clear problem that needs resolution\n",
    "    \n",
    "    DO NOT copy the example directly - create something new that tests similar knowledge.\n",
    "    \n",
    "    EXAMPLE CASE SUMMARY:\n",
    "    {example_case[\"summary\"]}\n",
    "    \n",
    "    NEW CASE (write only the case description, not the solution):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log LLM request\n",
    "    prompt_length = len(prompt)\n",
    "    logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "    \n",
    "    # Time the LLM call\n",
    "    start_time = datetime.now()\n",
    "    draft_case = generate_with_llm(prompt, temperature=0.7)\n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Log LLM response\n",
    "    logger.log_llm_response(\n",
    "        LLM_MODEL, \n",
    "        len(draft_case),\n",
    "        duration\n",
    "    )\n",
    "    \n",
    "    case.update({\n",
    "        \"draft_case\": draft_case,\n",
    "        \"example_inspiration\": example_case[\"title\"],\n",
    "        \"example_filename\": example_case[\"filename\"],\n",
    "        \"creation_date\": time.strftime(\"%Y-%m-%d\"),\n",
    "        \"stage\": \"draft\",\n",
    "        \"last_checkpoint\": \"draft\"  # Update the checkpoint stage\n",
    "    })\n",
    "    \n",
    "    display(Markdown(\"### Initial Case Draft\"))\n",
    "    display(Markdown(case[\"draft_case\"]))\n",
    "    \n",
    "    logger.info(\"DRAFT_GENERATED\", \"Case draft generated\", {\n",
    "        \"draft_length\": len(case.get(\"draft_case\", \"\")),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: draft\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"draft_length\": len(case.get(\"draft_case\", \"\")),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    return case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 1: CASE DRAFT GENERATION (using gemini-2.0-flash-exp)\n",
      "  1.1 Selecting random example case...\n",
      "  1.2 Generating case draft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:58:17,852 - INFO - Generated content with gemini-2.0-flash-exp: 3419 chars in 4.82s\n",
      "2025-03-28 16:58:17,852 - INFO - Generated content with gemini-2.0-flash-exp: 3419 chars in 4.82s\n",
      "2025-03-28 16:58:17,852 - INFO - Generated content with gemini-2.0-flash-exp: 3419 chars in 4.82s\n",
      "2025-03-28 16:58:17,852 - INFO - Generated content with gemini-2.0-flash-exp: 3419 chars in 4.82s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Initial Case Draft"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Case Study: The \"Baltic Mariner\" Bottleneck\n",
       "\n",
       "**Scenario:**\n",
       "\n",
       "Oceanic Shipping Solutions (OSS), a medium-sized container shipping company headquartered in Hamburg, Germany, specializes in trade routes between East Asia and the Baltic Sea region. OSS operates a fleet of ten container vessels, including the *Baltic Mariner*, a 6,800 TEU container ship.\n",
       "\n",
       "The *Baltic Mariner* is currently en route from Shanghai, China, to St. Petersburg, Russia, a key destination for OSS's Baltic operations. Its cargo manifest includes a diverse range of goods, from electronics and textiles to automotive components and consumer goods, destined for various clients in Russia, Finland, and the Baltic states. The vessel's scheduled route includes calls at Busan (South Korea), Singapore, Colombo (Sri Lanka), and Rotterdam (Netherlands) before proceeding to St. Petersburg via the Kiel Canal.\n",
       "\n",
       "Everything proceeded smoothly until the vessel’s arrival in Rotterdam. While initially scheduled for a 24-hour turnaround for unloading and loading operations, the *Baltic Mariner* encountered an unexpected delay. A new, stringent inspection regime, implemented by Dutch Customs and Port Authority due to recent concerns about undeclared hazardous materials, flagged several containers for mandatory inspection. These containers were identified based on a risk assessment algorithm that considered factors such as the declared cargo type, the shipper's history, and the origin of the goods.\n",
       "\n",
       "Complicating matters further, the Rotterdam terminal, managed by ECT Delta Terminal, is experiencing peak season congestion due to a surge in import volumes from Asia. This has resulted in limited availability of inspection slots and longer waiting times for container handling.\n",
       "\n",
       "Adding to the pressure, one of the refrigerated containers flagged for inspection is carrying temperature-sensitive pharmaceuticals destined for a hospital in St. Petersburg. Any significant delay could compromise the integrity of the cargo and lead to substantial financial losses and reputational damage for OSS.\n",
       "\n",
       "The *Baltic Mariner* is now facing a potential 72-hour delay in Rotterdam. This delay will impact the vessel's arrival time in St. Petersburg, causing ripple effects on downstream logistics operations, including trucking and rail connections for the cargo destined for other Baltic countries. OSS is also contractually obligated to meet specific delivery deadlines with its clients, facing potential penalty clauses for late deliveries.\n",
       "\n",
       "**The Problem:**\n",
       "\n",
       "OSS needs to develop a strategy to minimize the impact of the delay in Rotterdam and ensure the timely delivery of its cargo, particularly the temperature-sensitive pharmaceuticals, to St. Petersburg and other Baltic destinations. This strategy must consider the following factors:\n",
       "\n",
       "*   The limited availability of inspection slots and container handling capacity at the Rotterdam terminal.\n",
       "*   The potential risks associated with delaying temperature-sensitive cargo.\n",
       "*   The contractual obligations and potential penalty clauses for late deliveries.\n",
       "*   The overall impact on OSS's reputation and customer relationships.\n",
       "*   The cost implications of alternative solutions, such as expediting cargo handling or rerouting shipments.\n",
       "\n",
       "How should OSS mitigate the impact of the Rotterdam delay and ensure the timely delivery of its cargo while minimizing financial losses and reputational damage?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: draft\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 1\n",
    "debug_case = debug_stage_1(debug_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Stage 2: Critical Analysis & Query Generation\n",
    "def debug_stage_2(case):\n",
    "    \"\"\"Run Stage 2: Critical Analysis & Query Generation\"\"\"\n",
    "    print(f\"\\nSTAGE 2: CRITICAL ANALYSIS & QUERY GENERATION (using {LLM_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"analysis\")\n",
    "    \n",
    "    print(\"  2.1 Using simplified guidelines...\")\n",
    "    # Hard-code simple guidelines for testing\n",
    "    guidelines = \"\"\"\n",
    "    CASE STUDY GUIDELINES:\n",
    "    1. Cases must include specific real-world regulations\n",
    "    2. Include details about documentation requirements\n",
    "    3. Reference actual port procedures and maritime regulations\n",
    "    4. Make sure customs clearance processes are accurate\n",
    "    5. Include realistic timelines for shipping operations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"  2.2 Analyzing case draft...\")\n",
    "    prompt = f\"\"\"\n",
    "    You are a maritime logistics expert analyzing a draft case study.\n",
    "    \n",
    "    CASE DRAFT:\n",
    "    {case['draft_case']}\n",
    "    \n",
    "    GUIDELINES FOR CASE QUALITY:\n",
    "    {guidelines}\n",
    "    \n",
    "    Your task:\n",
    "    1. Identify key topics, entities, and regulations mentioned in the case\n",
    "    2. Generate 5-8 specific search queries that would help find relevant datapoints \n",
    "    3. List 3-5 areas where the case could be improved with more specific regulatory details\n",
    "    \n",
    "    Format your response as follows:\n",
    "    \n",
    "    ## Case Analysis\n",
    "    [Brief analysis of the current case draft]\n",
    "    \n",
    "    ## Key Topics\n",
    "    - Topic 1\n",
    "    - Topic 2\n",
    "    [etc.]\n",
    "    \n",
    "    ## Search Queries\n",
    "    1. [Specific query 1]\n",
    "    2. [Specific query 2]\n",
    "    [etc.]\n",
    "    \n",
    "    ## Areas for Improvement\n",
    "    1. [Area 1 - what regulatory or factual detail is needed]\n",
    "    2. [Area 2 - what regulatory or factual detail is needed]\n",
    "    [etc.]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log LLM request\n",
    "    prompt_length = len(prompt)\n",
    "    logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "    \n",
    "    # Time the LLM call\n",
    "    start_time = datetime.now()\n",
    "    analysis = generate_with_llm(prompt, temperature=0.3)\n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Log LLM response\n",
    "    logger.log_llm_response(\n",
    "        LLM_MODEL, \n",
    "        len(analysis),\n",
    "        duration\n",
    "    )\n",
    "    \n",
    "    case.update({\n",
    "        \"analysis\": analysis,\n",
    "        \"stage\": \"analysis\",\n",
    "        \"last_checkpoint\": \"analyzed\",  # Update the checkpoint stage\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    display(Markdown(\"### Case Analysis\"))\n",
    "    display(Markdown(case[\"analysis\"]))\n",
    "    \n",
    "    print(\"  2.3 Extracting queries and keywords...\")\n",
    "    # Simplified extraction for debug purposes\n",
    "    queries = []\n",
    "    keywords = []\n",
    "    \n",
    "    for line in analysis.split('\\n'):\n",
    "        if line.strip().startswith('1.') and 'Search Queries' in analysis.split('\\n')[analysis.split('\\n').index(line)-5:analysis.split('\\n').index(line)]:\n",
    "            queries.append(line.strip()[3:])\n",
    "        if line.strip().startswith('- ') and 'Key Topics' in analysis.split('\\n')[analysis.split('\\n').index(line)-5:analysis.split('\\n').index(line)]:\n",
    "            keywords.append(line.strip()[2:])\n",
    "    \n",
    "    # Ensure we have at least some queries and keywords\n",
    "    if not queries:\n",
    "        queries = [\"maritime regulations Baltic Sea\", \"container shipping documentation requirements\"]\n",
    "    if not keywords:\n",
    "        keywords = [\"maritime logistics\", \"container shipping\", \"documentation\"]\n",
    "        \n",
    "    case[\"search_queries\"] = queries\n",
    "    case[\"keywords\"] = keywords\n",
    "    \n",
    "    print(f\"  Extracted {len(queries)} queries and {len(keywords)} keywords\")\n",
    "    \n",
    "    logger.info(\"ANALYSIS_COMPLETE\", \"Case analysis complete\", {\n",
    "        \"analysis_length\": len(case.get(\"analysis\", \"\")),\n",
    "        \"query_count\": len(queries),\n",
    "        \"keyword_count\": len(keywords),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: analyzed\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"query_count\": len(queries),\n",
    "        \"keyword_count\": len(keywords),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    return case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 2: CRITICAL ANALYSIS & QUERY GENERATION (using gemini-2.0-flash-exp)\n",
      "  2.1 Using simplified guidelines...\n",
      "  2.2 Analyzing case draft...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:59:43,882 - INFO - Generated content with gemini-2.0-flash-exp: 3722 chars in 4.87s\n",
      "2025-03-28 16:59:43,882 - INFO - Generated content with gemini-2.0-flash-exp: 3722 chars in 4.87s\n",
      "2025-03-28 16:59:43,882 - INFO - Generated content with gemini-2.0-flash-exp: 3722 chars in 4.87s\n",
      "2025-03-28 16:59:43,882 - INFO - Generated content with gemini-2.0-flash-exp: 3722 chars in 4.87s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Case Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Case Analysis\n",
       "\n",
       "The case study presents a realistic scenario of a container ship facing delays due to port congestion and increased customs scrutiny in Rotterdam. It highlights the complexities of maritime logistics, particularly the challenges of managing time-sensitive cargo and adhering to contractual obligations. The core problem revolves around mitigating the impact of the delay while minimizing financial and reputational damage. However, the case could benefit from more specific details regarding regulations, documentation, and port procedures to enhance its realism and analytical depth.\n",
       "\n",
       "## Key Topics\n",
       "\n",
       "- Port Congestion (Rotterdam)\n",
       "- Customs Inspections (Netherlands)\n",
       "- Temperature-Sensitive Pharmaceuticals\n",
       "- Container Shipping\n",
       "- Contractual Obligations (Shipping)\n",
       "- Risk Assessment Algorithms (Customs)\n",
       "- Reefer Container Management\n",
       "- Maritime Logistics\n",
       "- Kiel Canal Transit\n",
       "\n",
       "## Search Queries\n",
       "\n",
       "1. \"Rotterdam port congestion peak season statistics\"\n",
       "2. \"Dutch Customs inspection procedures containerized cargo\"\n",
       "3. \"EU regulations temperature controlled pharmaceuticals transport\"\n",
       "4. \"ECT Delta Terminal Rotterdam container handling capacity\"\n",
       "5. \"Kiel Canal transit regulations container ships\"\n",
       "6. \"INCOTERMS 2020 liability late delivery\"\n",
       "7. \"EU customs risk assessment algorithm container cargo\"\n",
       "8. \"Rotterdam Port Authority hazardous materials regulations\"\n",
       "\n",
       "## Areas for Improvement\n",
       "\n",
       "1. **Specific Dutch Customs Regulations:** The case mentions a \"new, stringent inspection regime.\" To improve the case, specify the exact regulation(s) being implemented (e.g., referencing specific articles within the Dutch Customs Act or EU regulations on customs controls). Include details on the documentation required for high-risk cargo and the specific criteria used to flag containers for inspection beyond the general \"risk assessment algorithm.\"\n",
       "\n",
       "2. **Reefer Container Monitoring and Documentation:** Elaborate on the specific temperature monitoring requirements for pharmaceutical shipments under EU regulations (e.g., GDP - Good Distribution Practice guidelines). Include details on the documentation required to prove temperature integrity (e.g., temperature logs, calibration certificates for monitoring equipment) and the procedures for reporting temperature excursions to relevant authorities. Mention specific standards like EN 12830 for temperature recorders.\n",
       "\n",
       "3. **Contractual Obligations and Liability:** The case mentions \"contractual obligations and potential penalty clauses.\" Specify the relevant INCOTERM used in the contract between OSS and its clients. This will define the point at which risk and responsibility for the cargo transfer. Detail the typical penalty clauses for late delivery in shipping contracts, including the calculation of demurrage and detention charges.\n",
       "\n",
       "4. **Port Procedure Specifics:** Include details on the communication protocols between the shipping line (OSS), the terminal operator (ECT Delta), and Dutch Customs. What specific forms or electronic messages are required to request inspection slots, report hazardous materials, or appeal inspection decisions? What are the standard procedures for handling refrigerated containers flagged for inspection, including power supply and temperature monitoring during the inspection process?\n",
       "\n",
       "5. **Hazardous Material Declaration Details:** Expand on the \"undeclared hazardous materials\" concern. What specific types of hazardous materials are Dutch Customs particularly concerned about? What are the consequences of misdeclaration or non-declaration of hazardous materials, including fines and potential criminal charges? What specific documentation (e.g., IMO declarations) is required for hazardous materials shipments?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.3 Extracting queries and keywords...\n",
      "  Extracted 2 queries and 3 keywords\n",
      "✓ Checkpoint saved: analyzed\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 2\n",
    "debug_case = debug_stage_2(debug_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Stage 3: Datapoint Retrieval\n",
    "def debug_stage_3(case):\n",
    "    \"\"\"Run Stage 3: Datapoint Retrieval\"\"\"\n",
    "    print(f\"\\nSTAGE 3: DATAPOINT RETRIEVAL (using {EMBEDDING_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"datapoint_retrieval\")\n",
    "    \n",
    "    print(\"  3.1 Creating mock datapoints (avoiding database calls)...\")\n",
    "    \n",
    "    # Create mock datapoints for debugging\n",
    "    mock_datapoints = [\n",
    "        {\n",
    "            \"id\": \"dp001\",\n",
    "            \"title\": \"Baltic Sea Shipping Regulations\",\n",
    "            \"content\": \"Vessels entering the Baltic Sea must comply with HELCOM regulations for environmental protection. Ships must use low-sulfur fuel (0.1% or less) and follow strict waste management procedures.\",\n",
    "            \"datapoint_type\": \"regulation\",\n",
    "            \"relevant_entity\": \"HELCOM\",\n",
    "            \"search_score\": 0.92\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"dp002\",\n",
    "            \"title\": \"Container Documentation Requirements\",\n",
    "            \"content\": \"All containers must have a properly completed bill of lading, packing list, commercial invoice, and dangerous goods declaration if applicable. For EU ports, an Entry Summary Declaration (ENS) must be submitted electronically at least 24 hours before loading.\",\n",
    "            \"datapoint_type\": \"requirement\",\n",
    "            \"relevant_entity\": \"EU Customs\",\n",
    "            \"search_score\": 0.89\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"dp003\",\n",
    "            \"title\": \"Port of Hamburg Container Handling Procedures\",\n",
    "            \"content\": \"The Port of Hamburg requires advance notification for all container vessels at least 48 hours before arrival. Container release requires proper customs clearance and port fees payment. The terminal operates 24/7 with special procedures for oversized or refrigerated containers.\",\n",
    "            \"datapoint_type\": \"procedure\",\n",
    "            \"relevant_entity\": \"Port of Hamburg\",\n",
    "            \"search_score\": 0.85\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"dp004\",\n",
    "            \"title\": \"Customs Clearance in Northern Europe\",\n",
    "            \"content\": \"Customs clearance in Northern European ports requires digital submission of the Single Administrative Document (SAD), valid certificates of origin, and compliance with EU product safety regulations. AEO-certified companies benefit from expedited procedures.\",\n",
    "            \"datapoint_type\": \"process\",\n",
    "            \"relevant_entity\": \"EU Customs Union\",\n",
    "            \"search_score\": 0.82\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"dp005\",\n",
    "            \"title\": \"Container Shipping Transit Times Asia-Europe\",\n",
    "            \"content\": \"Standard transit times from major Asian ports to Northern Europe: Shanghai to Hamburg: 30-35 days, Singapore to Rotterdam: 24-28 days, Hong Kong to Gdańsk: 35-40 days. Weather conditions in the North Sea can add 1-3 days delay during winter months.\",\n",
    "            \"datapoint_type\": \"reference\",\n",
    "            \"relevant_entity\": \"Shipping Lines\",\n",
    "            \"search_score\": 0.79\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add to case\n",
    "    case[\"relevant_datapoints\"] = mock_datapoints\n",
    "    case[\"stage\"] = \"datapoints_retrieved\"\n",
    "    case[\"last_checkpoint\"] = \"datapoints_retrieved\"\n",
    "    \n",
    "    print(f\"  Mock datapoints created: {len(mock_datapoints)}\")\n",
    "    \n",
    "    # Add search metadata\n",
    "    case[\"search_metadata\"] = {\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"total_queries\": len(case.get(\"search_queries\", [])),\n",
    "        \"unique_datapoints\": len(mock_datapoints),\n",
    "        \"query_stats\": [\n",
    "            {\"query\": q, \"results_found\": random.randint(3, 8), \"top_score\": round(0.75 + random.random()*0.2, 2)}\n",
    "            for q in case.get(\"search_queries\", [\"default query\"])\n",
    "        ],\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    logger.info(\"DATAPOINTS_RETRIEVED\", f\"Retrieved {len(mock_datapoints)} datapoints\", {\n",
    "        \"datapoint_count\": len(mock_datapoints),\n",
    "        \"embedding_model\": EMBEDDING_MODEL\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: datapoints_retrieved\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"datapoint_count\": len(mock_datapoints),\n",
    "        \"embedding_model\": EMBEDDING_MODEL\n",
    "    })\n",
    "    \n",
    "    return case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Stage 3: Real Datapoint Retrieval\n",
    "def debug_stage_3_real(case):\n",
    "    \"\"\"Run Stage 3: Datapoint Retrieval with real database\"\"\"\n",
    "    print(f\"\\nSTAGE 3: DATAPOINT RETRIEVAL (using {EMBEDDING_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"datapoint_retrieval\")\n",
    "    \n",
    "    print(\"  3.1 Retrieving datapoints using search queries...\")\n",
    "    \n",
    "    # Time datapoint retrieval\n",
    "    start_time = datetime.now()\n",
    "    try:\n",
    "        # Use the actual retrieve_relevant_datapoints function\n",
    "        case = retrieve_relevant_datapoints(case)\n",
    "        print(\"✅ Successfully retrieved datapoints from database\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error retrieving datapoints: {str(e)}\")\n",
    "        print(\"Falling back to mock datapoints...\")\n",
    "        # Create mock datapoints as fallback\n",
    "        mock_datapoints = [\n",
    "            {\n",
    "                \"id\": \"dp001\",\n",
    "                \"title\": \"Baltic Sea Shipping Regulations\",\n",
    "                \"content\": \"Vessels entering the Baltic Sea must comply with HELCOM regulations...\",\n",
    "                \"datapoint_type\": \"regulation\",\n",
    "                \"relevant_entity\": \"HELCOM\",\n",
    "                \"search_score\": 0.92\n",
    "            },\n",
    "            # Include the other mock datapoints\n",
    "        ]\n",
    "        case[\"relevant_datapoints\"] = mock_datapoints\n",
    "        case[\"using_mock_data\"] = True\n",
    "    \n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    datapoint_count = len(case.get(\"relevant_datapoints\", []))\n",
    "    print(f\"Retrieved {datapoint_count} datapoints\")\n",
    "    \n",
    "    logger.info(\"DATAPOINTS_RETRIEVED\", f\"Retrieved {datapoint_count} datapoints\", {\n",
    "        \"datapoint_count\": datapoint_count,\n",
    "        \"retrieval_duration\": duration,\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"using_mock_data\": case.get(\"using_mock_data\", False)\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: datapoints_retrieved\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"datapoint_count\": datapoint_count,\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"using_mock_data\": case.get(\"using_mock_data\", False)\n",
    "    })\n",
    "    \n",
    "    return case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 3: DATAPOINT RETRIEVAL (using models/text-embedding-004)\n",
      "  3.1 Retrieving datapoints using search queries...\n",
      "No keywords found in analysis, using defaults\n",
      "Using 8 queries and 4 keywords to find relevant datapoints\n",
      "Using embedding model: models/text-embedding-004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff862420b8b4bc2b67d4eb69ff89905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching datapoints:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:09:54,788 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:54,788 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:54,788 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:54,788 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_74043/4038469405.py:16: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n",
      "2025-03-28 17:09:55,195 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:55,195 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:55,195 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:55,195 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:55,606 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:55,606 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:55,606 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:55,606 - INFO - Created embeddings with models/text-embedding-004 in 0.37s\n",
      "2025-03-28 17:09:56,012 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:09:56,012 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:09:56,012 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:09:56,012 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:09:56,356 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,356 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,356 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,356 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,658 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,658 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,658 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,658 - INFO - Created embeddings with models/text-embedding-004 in 0.26s\n",
      "2025-03-28 17:09:56,960 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:56,960 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:56,960 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:56,960 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:57,263 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:57,263 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:57,263 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n",
      "2025-03-28 17:09:57,263 - INFO - Created embeddings with models/text-embedding-004 in 0.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 40 unique datapoints\n",
      "✅ Successfully retrieved datapoints from database\n",
      "Retrieved 40 datapoints\n",
      "✓ Checkpoint saved: datapoints_retrieved\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 3 with real data\n",
    "debug_case = debug_stage_3_real(debug_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Stage 4: Contextual Enhancement with REAL guidelines\n",
    "def debug_stage_4(case):\n",
    "    \"\"\"Run Stage 4: Contextual Enhancement using real guidelines from Qdrant\"\"\"\n",
    "    print(f\"\\nSTAGE 4: CONTEXTUAL ENHANCEMENT (using {LLM_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"enhancement\")\n",
    "    \n",
    "    print(\"  4.1 Finding relevant domain-specific guideline...\")\n",
    "    try:\n",
    "        # Use the real guideline retrieval function\n",
    "        guideline = find_relevant_guideline(case)\n",
    "        \n",
    "        if guideline:\n",
    "            print(f\"✅ Found guideline: {guideline.get('guideline_type', 'unknown')} with {guideline.get('chunks_found', 0)} chunks\")\n",
    "        else:\n",
    "            print(\"⚠️ No relevant guidelines found, using default\")\n",
    "            guideline = {\n",
    "                \"guideline_type\": \"default_maritime\",\n",
    "                \"content\": \"Maritime logistics cases should demonstrate realistic challenges in container shipping.\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error finding guideline: {str(e)}\")\n",
    "        print(\"Using default guideline instead\")\n",
    "        guideline = {\n",
    "            \"guideline_type\": \"default_maritime\",\n",
    "            \"content\": \"Maritime logistics cases should demonstrate realistic challenges in container shipping.\"\n",
    "        }\n",
    "    \n",
    "    print(\"  4.2 Preparing datapoints for prompt...\")\n",
    "    # Format datapoints for the prompt\n",
    "    datapoints_text = \"\"\n",
    "    for i, dp in enumerate(case.get(\"relevant_datapoints\", [])[:15]):\n",
    "        datapoints_text += f\"\\nDATAPOINT {i+1}:\\n\"\n",
    "        datapoints_text += f\"Title: {dp.get('title', 'Untitled')}\\n\"\n",
    "        datapoints_text += f\"Content: {dp.get('content', 'No content')}\\n\"\n",
    "        if \"relevant_entity\" in dp:\n",
    "            datapoints_text += f\"Entity: {dp.get('relevant_entity', 'Unknown')}\\n\"\n",
    "    \n",
    "    print(\"  4.3 Enhancing case with context...\")\n",
    "    prompt = f\"\"\"\n",
    "    I'll provide you with a DRAFT CASE, DATAPOINTS, and DOMAIN GUIDELINES. Your task is to:\n",
    "    1. Enhance the case with specific regulatory details from the datapoints\n",
    "    2. Ensure it follows domain-specific guidelines \n",
    "    3. Make the scenario more realistic and educational\n",
    "    4. Add a clear title for the case\n",
    "    \n",
    "    DRAFT CASE:\n",
    "    {case.get(\"draft_case\", \"\")}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoints_text}\n",
    "    \n",
    "    DOMAIN GUIDELINES:\n",
    "    {guideline.get(\"content\", \"No specific guidelines available\")}\n",
    "    \n",
    "    Please respond with:\n",
    "    \n",
    "    ## Case Title\n",
    "    [Provide a concise, descriptive title for this case]\n",
    "    \n",
    "    ## Enhanced Case\n",
    "    [Provide the improved case with specific regulations and requirements from the datapoints]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log LLM request\n",
    "    prompt_length = len(prompt)\n",
    "    logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "    \n",
    "    # Time the LLM call\n",
    "    start_time = datetime.now()\n",
    "    enhanced_content = generate_with_llm(prompt, temperature=0.7)\n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Log LLM response\n",
    "    logger.log_llm_response(\n",
    "        LLM_MODEL, \n",
    "        len(enhanced_content),\n",
    "        duration\n",
    "    )\n",
    "    \n",
    "    # Extract title and enhanced case\n",
    "    title = \"Untitled Case\"\n",
    "    enhanced_case = enhanced_content\n",
    "    \n",
    "    if \"## Case Title\" in enhanced_content:\n",
    "        parts = enhanced_content.split(\"## Case Title\")\n",
    "        title_section = parts[1].split(\"##\")[0]\n",
    "        title = title_section.strip()\n",
    "        \n",
    "    if \"## Enhanced Case\" in enhanced_content:\n",
    "        parts = enhanced_content.split(\"## Enhanced Case\")\n",
    "        enhanced_case = parts[1].strip()\n",
    "    \n",
    "    # Update case\n",
    "    case.update({\n",
    "        \"title\": title,\n",
    "        \"enhanced_case\": enhanced_case,\n",
    "        \"domain_guideline\": guideline.get(\"guideline_type\", \"default\"),\n",
    "        \"stage\": \"enhanced\",\n",
    "        \"last_checkpoint\": \"enhanced\",\n",
    "        \"enhancement_metadata\": {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"datapoints_used\": len(case.get(\"relevant_datapoints\", [])),\n",
    "            \"guideline_type\": guideline.get(\"guideline_type\", \"default\"),\n",
    "            \"guideline_chunks\": guideline.get(\"chunks_found\", 0) if guideline else 0\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    display(Markdown(f\"### Enhanced Case: {case.get('title', 'Untitled Case')}\"))\n",
    "    display(Markdown(case[\"enhanced_case\"]))\n",
    "    \n",
    "    logger.info(\"CASE_ENHANCED\", \"Case enhanced with context\", {\n",
    "        \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "        \"enhanced_length\": len(case.get(\"enhanced_case\", \"\")),\n",
    "        \"domain_guideline\": case.get(\"domain_guideline\", \"none\"),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: enhanced\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "        \"domain_guideline\": case.get(\"domain_guideline\", \"default\"),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    return case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 4: CONTEXTUAL ENHANCEMENT (using gemini-2.0-flash-exp)\n",
      "  4.1 Finding relevant domain-specific guideline...\n",
      "Creating embedding using model: models/text-embedding-004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:16:24,815 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:16:24,815 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:16:24,815 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "2025-03-28 17:16:24,815 - INFO - Created embeddings with models/text-embedding-004 in 0.32s\n",
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_74043/2998382696.py:15: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found relevant guideline: maritime (score: 0.6963)\n",
      "✅ Found guideline: maritime with 3 chunks\n",
      "  4.2 Preparing datapoints for prompt...\n",
      "  4.3 Enhancing case with context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:16:32,911 - INFO - Generated content with gemini-2.0-flash-exp: 7149 chars in 7.95s\n",
      "2025-03-28 17:16:32,911 - INFO - Generated content with gemini-2.0-flash-exp: 7149 chars in 7.95s\n",
      "2025-03-28 17:16:32,911 - INFO - Generated content with gemini-2.0-flash-exp: 7149 chars in 7.95s\n",
      "2025-03-28 17:16:32,911 - INFO - Generated content with gemini-2.0-flash-exp: 7149 chars in 7.95s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Enhanced Case: **The \"Baltic Mariner\" Crisis: Navigating Rotterdam's Regulatory Bottleneck**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Scenario:**\n",
       "\n",
       "Oceanic Shipping Solutions (OSS), a medium-sized container shipping company headquartered in Hamburg, Germany, specializes in trade routes between East Asia and the Baltic Sea region. OSS operates a fleet of ten container vessels, including the *Baltic Mariner*, a 6,800 TEU container ship.\n",
       "\n",
       "The *Baltic Mariner* is currently en route from Shanghai, China, to St. Petersburg, Russia, a key destination for OSS's Baltic operations. Its cargo manifest includes a diverse range of goods, from electronics and textiles to automotive components and consumer goods, destined for various clients in Russia, Finland, and the Baltic states. The vessel's scheduled route includes calls at Busan (South Korea), Singapore, Colombo (Sri Lanka), and Rotterdam (Netherlands) before proceeding to St. Petersburg via the Kiel Canal. The shipment operates under DAP Incoterms, placing the responsibility for import clearance on OSS.\n",
       "\n",
       "Everything proceeded smoothly until the vessel’s arrival in Rotterdam. While initially scheduled for a 24-hour turnaround for unloading and loading operations, the *Baltic Mariner* encountered an unexpected delay. A new, stringent inspection regime, implemented by Dutch Customs and the Port of Rotterdam Authority, was in effect. This regime, driven by heightened security concerns and stricter enforcement of EU Regulation No 952/2013 (Union Customs Code - UCC) Article 46, aims to combat the import of undeclared hazardous materials and counterfeit goods. Several containers were flagged for mandatory inspection based on a risk assessment algorithm. This algorithm considered factors such as the declared cargo type, the shipper's history, the origin of the goods (with specific attention to regions flagged for export control violations), and inconsistencies in the 24-hour manifest data submitted prior to arrival.\n",
       "\n",
       "The containers flagged include:\n",
       "\n",
       "*   One containing high-value electronics. The Bill of Lading description was deemed too vague (\"Electronic Components\") and requires a more detailed Commercial Invoice and Packing List according to customs requirements.\n",
       "*   One refrigerated container with pharmaceuticals.\n",
       "*   Several containers originating from a region known for intellectual property violations. Dutch Customs requires verification of the authenticity and proper documentation of the goods, increasing inspection time.\n",
       "\n",
       "Complicating matters further, the Rotterdam terminal, managed by ECT Delta Terminal, is experiencing peak season congestion due to a surge in import volumes from Asia. This has resulted in limited availability of inspection slots and longer waiting times for container handling. The terminal is also experiencing a backlog in processing Entry Summary Declarations (ENS), a pre-loading notification required under EU customs regulations, further delaying the inspection process.\n",
       "\n",
       "Adding to the pressure, one of the refrigerated containers flagged for inspection is carrying temperature-sensitive pharmaceuticals destined for a hospital in St. Petersburg. The pharmaceuticals require a constant temperature between 2°C and 8°C. Any significant delay could compromise the integrity of the cargo, violating Good Distribution Practice (GDP) guidelines for pharmaceuticals and leading to substantial financial losses, potential health risks, and reputational damage for OSS. The Bill of Lading must accurately reflect the temperature requirements and a temperature monitoring log is essential.\n",
       "\n",
       "The *Baltic Mariner* is now facing a potential 72-hour delay in Rotterdam. This delay will impact the vessel's arrival time in St. Petersburg, causing ripple effects on downstream logistics operations, including trucking and rail connections for the cargo destined for other Baltic countries. OSS is also contractually obligated to meet specific delivery deadlines with its clients, facing potential penalty clauses for late deliveries. Furthermore, the delay increases the risk of demurrage and detention charges at the Rotterdam terminal.\n",
       "\n",
       "**The Problem:**\n",
       "\n",
       "OSS needs to develop a strategy to minimize the impact of the delay in Rotterdam and ensure the timely delivery of its cargo, particularly the temperature-sensitive pharmaceuticals, to St. Petersburg and other Baltic destinations. This strategy must consider the following factors:\n",
       "\n",
       "*   The limited availability of inspection slots and container handling capacity at the Rotterdam terminal.\n",
       "*   The potential risks associated with delaying temperature-sensitive cargo, including GDP compliance and potential health risks.\n",
       "*   The contractual obligations and potential penalty clauses for late deliveries, as well as potential demurrage and detention charges.\n",
       "*   The overall impact on OSS's reputation and customer relationships.\n",
       "*   The cost implications of alternative solutions, such as expediting cargo handling, rerouting shipments, or using alternative transportation modes.\n",
       "*   Compliance with EU customs regulations, including the Union Customs Code (UCC) and ENS requirements. The strategy must consider the need for accurate and complete documentation, including the Bill of Lading, Commercial Invoice, Packing List, and potentially a Certificate of Origin.\n",
       "*   Adherence to ISPS Code compliance and security documentation requirements, given the heightened security measures in place. Ensure the Ship Security Certificate (ISSC) is valid and the Ship Security Plan (SSP) has been followed.\n",
       "\n",
       "**Specifically, OSS must address:**\n",
       "\n",
       "1.  **Expediting the Inspection Process:** How can OSS proactively engage with Dutch Customs and the Port of Rotterdam Authority to expedite the inspection process for the flagged containers? This includes providing all necessary documentation promptly and accurately. This may include pre-arrival submission of documents via the Port Community System (PCS).\n",
       "2.  **Protecting the Pharmaceuticals:** What measures can OSS take to ensure the integrity of the temperature-sensitive pharmaceuticals during the delay, including potentially arranging for cold storage at the terminal or using a temperature-controlled transportation solution?\n",
       "3.  **Mitigating Downstream Impacts:** How can OSS communicate effectively with its clients in Russia, Finland, and the Baltic states to manage expectations and minimize disruptions to their supply chains? Can alternative transportation arrangements be made for cargo destined for these other countries?\n",
       "4.  **Minimizing Costs:** How can OSS optimize its strategy to minimize financial losses, including penalty clauses, demurrage and detention charges, and the cost of alternative solutions?\n",
       "\n",
       "How should OSS mitigate the impact of the Rotterdam delay and ensure the timely delivery of its cargo while minimizing financial losses and reputational damage? Furthermore, how can OSS improve its import workflows and documentation procedures to prevent similar delays in the future, taking into account Incoterms, data quality issues, and security regulations? Should OSS implement EDI for Bill of Lading exchange to prevent future delays?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: enhanced\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 4\n",
    "debug_case = debug_stage_4(debug_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Stage 5: Solution Development\n",
    "def debug_stage_5(case):\n",
    "    \"\"\"Run Stage 5: Solution Development\"\"\"\n",
    "    print(f\"\\nSTAGE 5: SOLUTION DEVELOPMENT (using {LLM_MODEL})\")\n",
    "    logger = case['logger']\n",
    "    logger.start_stage(\"solution\")\n",
    "    \n",
    "    print(\"  5.1 Preparing solution prompt...\")\n",
    "    # Format datapoints for the solution prompt\n",
    "    datapoints_text = \"\"\n",
    "    for i, dp in enumerate(case.get(\"relevant_datapoints\", [])[:5]):\n",
    "        datapoints_text += f\"\\nDATAPOINT {i+1}:\\n\"\n",
    "        datapoints_text += f\"Title: {dp.get('title', 'Untitled')}\\n\"\n",
    "        datapoints_text += f\"Content: {dp.get('content', 'No content')}\\n\"\n",
    "        if \"relevant_entity\" in dp:\n",
    "            datapoints_text += f\"Entity: {dp.get('relevant_entity', 'Unknown')}\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    I'll provide you with an ENHANCED CASE. Your task is to:\n",
    "    1. Develop a comprehensive solution that addresses all aspects of the case\n",
    "    2. Reference specific regulations and requirements that apply\n",
    "    3. Explain the reasoning behind the solution\n",
    "    4. Structure the solution clearly with steps/recommendations\n",
    "    \n",
    "    CASE TITLE:\n",
    "    {case.get(\"title\", \"Untitled Case\")}\n",
    "    \n",
    "    CASE SCENARIO:\n",
    "    {case.get(\"enhanced_case\", case.get(\"draft_case\", \"\"))}\n",
    "    \n",
    "    RELEVANT DATAPOINTS:\n",
    "    {datapoints_text}\n",
    "    \n",
    "    Please provide a detailed solution that demonstrates understanding of maritime logistics regulations and requirements.\n",
    "    Structure your response as follows:\n",
    "    \n",
    "    ## Executive Summary\n",
    "    [Brief overview of the solution]\n",
    "    \n",
    "    ## Detailed Solution Steps\n",
    "    [Step-by-step solution with regulatory references]\n",
    "    \n",
    "    ## Recommendations\n",
    "    [Key recommendations and best practices]\n",
    "    \n",
    "    ## Risk Mitigation\n",
    "    [Potential risks and mitigation strategies]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"  5.2 Generating solution...\")\n",
    "    # Log LLM request\n",
    "    prompt_length = len(prompt)\n",
    "    logger.log_llm_request(LLM_MODEL, prompt_length)\n",
    "    \n",
    "    # Time the LLM call\n",
    "    start_time = datetime.now()\n",
    "    solution = generate_with_llm(prompt, temperature=0.4)\n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Log LLM response\n",
    "    logger.log_llm_response(\n",
    "        LLM_MODEL, \n",
    "        len(solution),\n",
    "        duration\n",
    "    )\n",
    "    \n",
    "    # Update case\n",
    "    case.update({\n",
    "        \"solution\": solution,\n",
    "        \"stage\": \"completed\",\n",
    "        \"last_checkpoint\": \"completed\",\n",
    "        \"solution_metadata\": {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"datapoints_referenced\": len(case.get(\"relevant_datapoints\", [])),\n",
    "            \"solution_version\": \"2.0\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    display(Markdown(\"### Solution\"))\n",
    "    display(Markdown(case[\"solution\"]))\n",
    "    \n",
    "    logger.info(\"SOLUTION_DEVELOPED\", \"Solution developed\", {\n",
    "        \"solution_length\": len(case.get(\"solution\", \"\")),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    # Save checkpoint\n",
    "    case_copy = case.copy()\n",
    "    if \"logger\" in case_copy:\n",
    "        del case_copy[\"logger\"]\n",
    "    with open(case_copy[\"checkpoint_file\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(case_copy, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Checkpoint saved: completed\")\n",
    "    \n",
    "    logger.end_stage(result_summary={\n",
    "        \"solution_length\": len(case.get(\"solution\", \"\")),\n",
    "        \"model\": LLM_MODEL\n",
    "    })\n",
    "    \n",
    "    return case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 5: SOLUTION DEVELOPMENT (using gemini-2.0-flash-exp)\n",
      "  5.1 Preparing solution prompt...\n",
      "  5.2 Generating solution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:17:59,649 - INFO - Generated content with gemini-2.0-flash-exp: 10133 chars in 11.45s\n",
      "2025-03-28 17:17:59,649 - INFO - Generated content with gemini-2.0-flash-exp: 10133 chars in 11.45s\n",
      "2025-03-28 17:17:59,649 - INFO - Generated content with gemini-2.0-flash-exp: 10133 chars in 11.45s\n",
      "2025-03-28 17:17:59,649 - INFO - Generated content with gemini-2.0-flash-exp: 10133 chars in 11.45s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Solution"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Executive Summary\n",
       "\n",
       "The \"Baltic Mariner\" crisis requires a multi-pronged approach focusing on immediate action to expedite inspections, protect temperature-sensitive cargo, manage stakeholder communication, and minimize costs. This involves proactive engagement with Dutch Customs and the Port of Rotterdam Authority, securing appropriate cold storage for pharmaceuticals, transparent communication with clients, and exploring alternative transportation options. Long-term, OSS needs to improve its documentation procedures, enhance data quality, and leverage technology to prevent future delays. This solution addresses the immediate crisis while also laying the groundwork for a more resilient and efficient supply chain.\n",
       "\n",
       "## Detailed Solution Steps\n",
       "\n",
       "**Step 1: Immediate Engagement and Expediting Inspections**\n",
       "\n",
       "*   **Action:** Immediately contact Dutch Customs and the Port of Rotterdam Authority to understand the specific reasons for the flagged containers and the required corrective actions. Designate a dedicated liaison to handle all communication.\n",
       "*   **Regulatory Reference:** EU Regulation No 952/2013 (Union Customs Code - UCC) Article 46 outlines the basis for customs controls and risk assessment. The Port of Rotterdam Authority operates under Dutch customs regulations, which are aligned with the UCC.\n",
       "*   **Reasoning:** Proactive communication can demonstrate OSS's commitment to compliance and potentially expedite the inspection process. Understanding the specific concerns allows for targeted responses.\n",
       "*   **Deliverables:**\n",
       "    *   Establish direct communication channels with Dutch Customs and the Port of Rotterdam Authority.\n",
       "    *   Obtain a detailed explanation of the reasons for each flagged container.\n",
       "    *   Provide all requested documentation immediately (Commercial Invoice, Packing List, Certificate of Origin if applicable, Bill of Lading).\n",
       "*   **Specific Actions for Flagged Containers:**\n",
       "    *   **Electronics Container:** Immediately provide a detailed Commercial Invoice and Packing List specifying the exact components, their value, and their intended use. If possible, provide product datasheets.\n",
       "    *   **Pharmaceuticals Container:** Emphasize the time-sensitive nature of the cargo and the GDP requirements. Provide the temperature monitoring log, Bill of Lading, and any relevant certifications. Request priority inspection due to the critical nature of the cargo.\n",
       "    *   **IP Violation Region Containers:** Provide evidence of the authenticity of the goods, such as licenses, trademarks, or certificates of origin. Prepare for a thorough inspection and be ready to answer any questions regarding the origin and legitimacy of the goods.\n",
       "*   **Leverage Port Community System (PCS):** Ensure all documentation is submitted electronically via the Port Community System (PCS) to facilitate faster processing.\n",
       "*   **ISPS Code Compliance:** Re-verify that all ISPS Code requirements have been met, including ensuring the Ship Security Certificate (ISSC) is valid and the Ship Security Plan (SSP) has been followed.\n",
       "\n",
       "**Step 2: Protecting the Pharmaceuticals**\n",
       "\n",
       "*   **Action:** Immediately contact the Rotterdam terminal (ECT Delta Terminal) to arrange for cold storage for the pharmaceuticals container. If cold storage is unavailable, explore alternative temperature-controlled transportation solutions.\n",
       "*   **Regulatory Reference:** Good Distribution Practice (GDP) guidelines for pharmaceuticals mandate the maintenance of a consistent temperature range (2°C to 8°C in this case). Failure to comply can result in regulatory penalties and product recalls.\n",
       "*   **Reasoning:** Maintaining the integrity of the pharmaceuticals is paramount. Failure to do so can have severe consequences.\n",
       "*   **Deliverables:**\n",
       "    *   Secure cold storage at the Rotterdam terminal or arrange for temperature-controlled transportation.\n",
       "    *   Continuously monitor the temperature of the container.\n",
       "    *   Document all temperature readings and actions taken.\n",
       "*   **Contingency Plan:** If cold storage is unavailable, consider transferring the pharmaceuticals to a temperature-controlled truck and transporting them directly to St. Petersburg via road. This would be a costly option but may be necessary to preserve the integrity of the cargo.\n",
       "\n",
       "**Step 3: Mitigating Downstream Impacts**\n",
       "\n",
       "*   **Action:** Communicate proactively and transparently with clients in Russia, Finland, and the Baltic states to inform them of the delay and its potential impact on their deliveries.\n",
       "*   **Regulatory Reference:** While no specific regulation mandates communication, maintaining good customer relationships is crucial for business continuity and reputation management.\n",
       "*   **Reasoning:** Managing expectations and providing timely updates can minimize customer dissatisfaction and potential disputes.\n",
       "*   **Deliverables:**\n",
       "    *   Prepare a communication template explaining the situation and the expected delay.\n",
       "    *   Contact each client individually to discuss their specific needs and concerns.\n",
       "    *   Provide regular updates on the progress of the inspection and the revised delivery schedule.\n",
       "*   **Alternative Transportation Options:**\n",
       "    *   **Air Freight:** For time-critical cargo, consider air freighting the goods from Rotterdam to St. Petersburg or other Baltic destinations. This is an expensive option but may be necessary to meet contractual obligations.\n",
       "    *   **Trucking/Rail:** Explore alternative trucking or rail routes to bypass the congestion in Rotterdam. This may involve transloading the cargo to another vessel or mode of transport.\n",
       "\n",
       "**Step 4: Minimizing Costs**\n",
       "\n",
       "*   **Action:** Negotiate with the Rotterdam terminal (ECT Delta Terminal) to minimize demurrage and detention charges. Explore options for expedited container handling.\n",
       "*   **Regulatory Reference:** Demurrage and detention charges are governed by the terminal's tariff and the terms of the contract between OSS and the terminal operator.\n",
       "*   **Reasoning:** Minimizing financial losses is a key objective.\n",
       "*   **Deliverables:**\n",
       "    *   Negotiate a waiver or reduction of demurrage and detention charges.\n",
       "    *   Secure expedited container handling services.\n",
       "*   **Cost-Benefit Analysis:** Conduct a thorough cost-benefit analysis of all alternative solutions, including air freight, trucking, and expedited container handling. Factor in the potential costs of penalty clauses, demurrage and detention charges, and reputational damage.\n",
       "*   **Insurance Claim:** Assess the possibility of filing an insurance claim to cover the costs associated with the delay.\n",
       "\n",
       "**Step 5: Improving Import Workflows and Documentation Procedures**\n",
       "\n",
       "*   **Action:** Conduct a thorough review of OSS's import workflows and documentation procedures to identify areas for improvement.\n",
       "*   **Regulatory Reference:** EU Regulation No 952/2013 (Union Customs Code - UCC) emphasizes the importance of accurate and complete documentation for customs clearance.\n",
       "*   **Reasoning:** Preventing future delays requires addressing the root causes of the current problem.\n",
       "*   **Deliverables:**\n",
       "    *   Identify and correct any data quality issues in the Bill of Lading and other import documents.\n",
       "    *   Implement a system for verifying the accuracy and completeness of all import documents before shipment.\n",
       "    *   Develop a checklist of required documents for each type of cargo and destination.\n",
       "*   **Specific Improvements:**\n",
       "    *   **Bill of Lading Accuracy:** Implement a system for verifying the accuracy of the Bill of Lading before it is issued. This should include cross-checking the information against the Commercial Invoice, Packing List, and other relevant documents. Consider implementing EDI for Bill of Lading exchange to improve data accuracy and efficiency.\n",
       "    *   **Data Quality:** Implement data validation rules to ensure that all required fields are completed accurately and consistently.\n",
       "    *   **Incoterms:** Ensure that all parties understand their responsibilities under the applicable Incoterms.\n",
       "    *   **Security Regulations:** Stay up-to-date on the latest security regulations and ensure that all shipments comply with these regulations.\n",
       "*   **Training:** Provide training to all employees involved in the import process on the importance of accurate and complete documentation.\n",
       "\n",
       "## Recommendations\n",
       "\n",
       "*   **Proactive Communication:** Establish and maintain strong relationships with Dutch Customs and the Port of Rotterdam Authority.\n",
       "*   **Data Quality:** Implement robust data quality controls to ensure the accuracy and completeness of all import documents.\n",
       "*   **Technology:** Leverage technology, such as EDI and Port Community Systems, to improve efficiency and reduce errors.\n",
       "*   **Contingency Planning:** Develop contingency plans for dealing with unexpected delays and disruptions.\n",
       "*   **Training:** Provide ongoing training to employees on import regulations and best practices.\n",
       "\n",
       "## Risk Mitigation\n",
       "\n",
       "*   **Risk:** Potential for further delays due to unforeseen circumstances.\n",
       "    *   **Mitigation:** Maintain close communication with Dutch Customs and the Port of Rotterdam Authority. Have alternative transportation options readily available.\n",
       "*   **Risk:** Damage to the pharmaceuticals due to temperature fluctuations.\n",
       "    *   **Mitigation:** Continuously monitor the temperature of the container. Have a backup cold storage solution available.\n",
       "*   **Risk:** Customer dissatisfaction due to late deliveries.\n",
       "    *   **Mitigation:** Communicate proactively and transparently with clients. Offer alternative solutions, such as air freight, to mitigate the impact of the delay.\n",
       "*   **Risk:** Financial losses due to penalty clauses, demurrage and detention charges, and the cost of alternative solutions.\n",
       "    *   **Mitigation:** Negotiate with the Rotterdam terminal to minimize demurrage and detention charges. Conduct a thorough cost-benefit analysis of all alternative solutions. Explore the possibility of filing an insurance claim.\n",
       "*   **Risk:** Security breach or non-compliance with ISPS Code.\n",
       "    *   **Mitigation:** Re-verify ISPS Code compliance. Ensure all security documentation is up-to-date and accurate.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: completed\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 5\n",
    "debug_case = debug_stage_5(debug_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Final Case Package\n",
    "def save_final_case(case):\n",
    "    \"\"\"Save the final case to output directory\"\"\"\n",
    "    print(\"\\nFINALIZING CASE GENERATION\")\n",
    "    logger = case['logger']\n",
    "    \n",
    "    # Add generation metadata\n",
    "    case[\"generation_metadata\"] = {\n",
    "        \"llm_model\": LLM_MODEL,\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"completion_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"pipeline_version\": \"2.0\"\n",
    "    }\n",
    "    \n",
    "    # Format as final case-solution pair\n",
    "    final_case = {\n",
    "        \"title\": case.get(\"title\", \"Untitled Case\"),\n",
    "        \"case\": case.get(\"enhanced_case\", case.get(\"draft_case\", \"\")),\n",
    "        \"solution\": case.get(\"solution\", \"\"),\n",
    "        \"metadata\": {\n",
    "            \"creation_date\": case.get(\"creation_date\", time.strftime(\"%Y-%m-%d\")),\n",
    "            \"case_id\": case.get(\"case_id\", \"unknown\"),\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"generation_pipeline\": \"Interactive debugging\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"../Output/Generated_Cases\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create filename from title\n",
    "    if \"title\" in final_case and final_case[\"title\"]:\n",
    "        # Clean title for filename\n",
    "        title = re.sub(r'[^\\w\\s-]', '', final_case[\"title\"]).strip()\n",
    "        title = re.sub(r'[-\\s]+', '_', title)\n",
    "        filename = f\"{title}.json\"\n",
    "    else:\n",
    "        filename = f\"case_{case.get('case_id', 'unknown')}.json\"\n",
    "    \n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_case, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Final case saved to {filepath}\")\n",
    "    \n",
    "    # Finalize logging\n",
    "    logger.info(\"CASE_SAVED\", f\"Case saved to {filepath}\", {\n",
    "        \"filepath\": str(filepath),\n",
    "        \"llm_model\": LLM_MODEL,\n",
    "        \"embedding_model\": EMBEDDING_MODEL\n",
    "    })\n",
    "    \n",
    "    log_summary = logger.finalize()\n",
    "    \n",
    "    print(\"\\n✨ CASE GENERATION COMPLETE ✨\")\n",
    "    \n",
    "    return filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINALIZING CASE GENERATION\n",
      "✅ Final case saved to ../Output/Generated_Cases/The_Baltic_Mariner_Crisis_Navigating_Rotterdams_Regulatory_Bottleneck.json\n",
      "\n",
      "✨ CASE GENERATION COMPLETE ✨\n"
     ]
    }
   ],
   "source": [
    "# Save the final case\n",
    "final_filepath = save_final_case(debug_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running pipeline in DEBUG mode\n",
      "STAGE 0: INITIALIZATION\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initialize_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the full pipeline with debugging enabled\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m case, filepath \u001b[38;5;241m=\u001b[39m \u001b[43mrun_case_generation_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m, in \u001b[0;36mrun_case_generation_pipeline\u001b[0;34m(resume_from, save_checkpoints, debug)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m case:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTAGE 0: INITIALIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     case \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_checkpoint\u001b[49m()\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new case with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize logger\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the full pipeline with debugging enabled\n",
    "case, filepath = run_case_generation_pipeline(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Case Visualization\n",
    "\n",
    "Formatted display of the final case and solution, showing how domain knowledge has been integrated into a realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_case(case):\n",
    "    \"\"\"Display the case in a formatted way\"\"\"\n",
    "    if not case:\n",
    "        display(Markdown(\"### No case available to display\"))\n",
    "        return\n",
    "    \n",
    "    # Create a formatted HTML view\n",
    "    html = f\"\"\"\n",
    "    <div style=\"max-width: 800px; margin: auto; padding: 20px; border: 1px solid #ddd; border-radius: 8px;\">\n",
    "        <h1 style=\"text-align: center; margin-bottom: 30px;\">{case.get('title', 'Untitled Case')}</h1>\n",
    "        \n",
    "        <h2>Case Scenario</h2>\n",
    "        <div style=\"background: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px;\">\n",
    "            {case.get('enhanced_case', case.get('draft_case', '')).replace('\\n', '<br>')}</div>\n",
    "        \n",
    "        <h2>Solution</h2>\n",
    "        <div style=\"background: #f0f7ff; padding: 15px; border-radius: 5px;\">\n",
    "            {case.get('solution', 'No solution available').replace('\\n', '<br>')}</div>\n",
    "        \n",
    "        <div style=\"margin-top: 30px; font-size: 0.9em; color: #777;\">\n",
    "            <p>Created: {case.get('creation_date', 'Unknown date')}</p>\n",
    "            <p>Based on example: {case.get('example_inspiration', 'Original')}</p>\n",
    "            <p>Domain guideline: {case.get('domain_guideline', 'None')}</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "# Display the completed case\n",
    "if case:\n",
    "    display_case(case)\n",
    "else:\n",
    "    print(\"No case was generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Generation (Optional)\n",
    "\n",
    "For generating multiple cases with different focus areas or regions, this section provides batch processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_case_with_focus(example_case, focus_area, region):\n",
    "    \"\"\"Generate a case draft with specific focus area and region\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a new maritime logistics case study scenario inspired by but not copying the example below.\n",
    "    \n",
    "    The case should focus specifically on: {focus_area}\n",
    "    The region/route context should be: {region}\n",
    "    \n",
    "    EXAMPLE CASE FOR INSPIRATION:\n",
    "    {example_case[:2000]}\n",
    "    \n",
    "    IMPORTANT GUIDELINES:\n",
    "    1. Create a realistic container shipping/logistics scenario with clear problems to solve\n",
    "    2. Include fictional but plausible company names and stakeholders\n",
    "    3. Focus on international regulatory compliance and documentation issues\n",
    "    4. Present specific logistics challenges that require expertise to resolve\n",
    "    5. The scenario should prompt the reader to consider relevant regulations\n",
    "    6. Make the case educational while remaining engaging\n",
    "    7. Include enough specific details to make the case realistic\n",
    "    8. Focus on the {focus_area} aspects of maritime logistics\n",
    "    9. Set the scenario in the {region} context\n",
    "    \n",
    "    CASE:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating case draft with focus on {focus_area} in {region}...\")\n",
    "    draft_case = generate_with_llm(prompt, model=\"gemini-1.5-pro-latest\")\n",
    "    time.sleep(6)  # Rate limit\n",
    "    \n",
    "    # Create case object\n",
    "    return {\n",
    "        \"draft_case\": draft_case,\n",
    "        \"example_inspiration\": example_case[:100] + \"...\",\n",
    "        \"focus_area\": focus_area,\n",
    "        \"region\": region,\n",
    "        \"creation_date\": time.strftime(\"%Y-%m-%d\"),\n",
    "        \"stage\": \"draft\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_cases(num_cases=3, focus_areas=None, regions=None, max_retries=2):\n",
    "    \"\"\"\n",
    "    Generate multiple cases in batch with different focus areas or regions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_cases : int\n",
    "        Number of cases to generate\n",
    "    focus_areas : list\n",
    "        List of focus areas to distribute across cases (e.g., ['customs', 'documentation', 'compliance'])\n",
    "    regions : list\n",
    "        List of regions to focus on (e.g., ['Baltic', 'North Sea', 'Asia-Europe'])\n",
    "    max_retries : int\n",
    "        Maximum number of retries per case if generation fails\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary of generation results with paths to saved cases\n",
    "    \"\"\"\n",
    "        # Create batch record\n",
    "    batch_id = str(uuid.uuid4())\n",
    "    batch_dir = Path(\"../Data/Checkpoints/batches\")\n",
    "    batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    batch_record = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"requested_cases\": num_cases,\n",
    "        \"focus_areas\": focus_areas,\n",
    "        \"regions\": regions,\n",
    "        \"case_records\": [],\n",
    "        \"completed_cases\": []\n",
    "    }\n",
    "    \n",
    "    # Save initial batch record\n",
    "    with open(batch_dir / f\"batch_{batch_id}.json\", \"w\") as f:\n",
    "        json.dump(batch_record, f, indent=2)\n",
    "    \n",
    "    # Default focus areas if none provided\n",
    "    if not focus_areas:\n",
    "        focus_areas = [\n",
    "            \"customs documentation\", \n",
    "            \"container compliance\", \n",
    "            \"port operations\", \n",
    "            \"multimodal transfer\", \n",
    "            \"regulatory requirements\"\n",
    "        ]\n",
    "    \n",
    "    # Default regions if none provided\n",
    "    if not regions:\n",
    "        regions = [\n",
    "            \"Baltic Sea ports\", \n",
    "            \"North European gateways\", \n",
    "            \"China-Europe route\", \n",
    "            \"Southeast Asia logistics\", \n",
    "            \"Nordic regional distribution\"\n",
    "        ]\n",
    "    \n",
    "    # Ensure we have enough combinations for requested cases\n",
    "    if num_cases > len(focus_areas) * len(regions):\n",
    "        print(f\"Warning: Requested {num_cases} cases but only {len(focus_areas) * len(regions)} unique combinations. Some combinations will repeat.\")\n",
    "    \n",
    "    print(f\"Starting batch generation of {num_cases} cases...\")\n",
    "    generated_cases = []\n",
    "    failed_attempts = 0\n",
    "    results = {\n",
    "        \"success\": [],\n",
    "        \"failed\": [],\n",
    "        \"total_requested\": num_cases,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(total=num_cases, desc=\"Generating cases\")\n",
    "    \n",
    "    # Try to generate the requested number of cases\n",
    "    case_index = 0\n",
    "    while len(generated_cases) < num_cases and failed_attempts <= num_cases * max_retries:\n",
    "        # Select focus area and region for this case\n",
    "        focus_area = focus_areas[case_index % len(focus_areas)]\n",
    "        region = regions[(case_index // len(focus_areas)) % len(regions)]\n",
    "        \n",
    "        print(f\"\\n\\n{'='*80}\\nGenerating case {len(generated_cases)+1}/{num_cases}\")\n",
    "        print(f\"Focus Area: {focus_area} | Region: {region}\\n{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Select random example\n",
    "            example_case = select_random_example()\n",
    "            \n",
    "            # Generate case draft with focus area and region guidance\n",
    "            case = generate_case_with_focus(example_case, focus_area, region)\n",
    "            \n",
    "            # Continue with the rest of the pipeline\n",
    "            case = analyze_case_draft(case)\n",
    "            case = retrieve_relevant_datapoints(case)\n",
    "            case = enhance_case_with_context(case)\n",
    "            case = develop_solution(case)\n",
    "            \n",
    "            # Add metadata about the batch generation\n",
    "            case[\"batch_metadata\"] = {\n",
    "                \"focus_area\": focus_area,\n",
    "                \"region\": region,\n",
    "                \"batch_index\": len(generated_cases),\n",
    "                \"generation_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            # Save the case\n",
    "            filepath = save_final_case(case)\n",
    "            \n",
    "            # Add to results\n",
    "            generated_cases.append(case)\n",
    "            results[\"success\"].append({\n",
    "                \"title\": case.get(\"title\", \"Untitled\"),\n",
    "                \"filepath\": str(filepath),\n",
    "                \"focus_area\": focus_area,\n",
    "                \"region\": region\n",
    "            })\n",
    "            \n",
    "            # Update progress\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_attempts += 1\n",
    "            print(f\"Error generating case: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            results[\"failed\"].append({\n",
    "                \"focus_area\": focus_area,\n",
    "                \"region\": region,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            # Wait before retrying\n",
    "            time.sleep(10)\n",
    "        \n",
    "        # Move to next case configuration\n",
    "        case_index += 1\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Add summary to results\n",
    "    results[\"total_generated\"] = len(generated_cases)\n",
    "    results[\"success_rate\"] = len(generated_cases) / num_cases * 100 if num_cases > 0 else 0\n",
    "    \n",
    "    # Save summary report\n",
    "    report_file = Path(\"../Data/GeneratedCases/batch_report.json\")\n",
    "    with open(report_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nBatch generation complete!\")\n",
    "    print(f\"Successfully generated: {len(results['success'])}/{num_cases} cases\")\n",
    "    print(f\"Failed attempts: {len(results['failed'])}\")\n",
    "    print(f\"Report saved to: {report_file}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the batch generator\n",
    "def run_batch_generation_demo():\n",
    "    \"\"\"Run a small batch generation demo\"\"\"\n",
    "    # Define focus areas and regions for diverse case generation\n",
    "    focus_areas = [\n",
    "        \"customs documentation requirements\",\n",
    "        \"container loading compliance\",\n",
    "        \"hazardous materials handling\",\n",
    "        \"import/export regulations\",\n",
    "        \"port security procedures\"\n",
    "    ]\n",
    "    \n",
    "    regions = [\n",
    "        \"Baltic Sea to East Asian ports\",\n",
    "        \"Northern Europe hub distribution\",\n",
    "        \"China-Hamburg express route\",\n",
    "        \"Scandinavian multimodal network\"\n",
    "    ]\n",
    "    \n",
    "    # Generate a small batch (2 cases) for demonstration\n",
    "    results = batch_generate_cases(\n",
    "        num_cases=2,  \n",
    "        focus_areas=focus_areas,\n",
    "        regions=regions\n",
    "    )\n",
    "    \n",
    "    # Show summary of generated cases\n",
    "    if results[\"success\"]:\n",
    "        print(\"\\nGenerated Case Summaries:\")\n",
    "        for i, case_info in enumerate(results[\"success\"]):\n",
    "            print(f\"\\nCase {i+1}: {case_info['title']}\")\n",
    "            print(f\"Focus: {case_info['focus_area']}\")\n",
    "            print(f\"Region: {case_info['region']}\")\n",
    "            print(f\"Saved to: {case_info['filepath']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch generation demo\n",
    "# Uncomment to execute:\n",
    "# batch_results = run_batch_generation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Display Batch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch_results(results):\n",
    "    \"\"\"Display batch generation results in a formatted way\"\"\"\n",
    "    if not results or \"success\" not in results:\n",
    "        display(Markdown(\"### No batch results available to display\"))\n",
    "        return\n",
    "    \n",
    "    # Create summary table\n",
    "    html = f\"\"\"\n",
    "    <div style=\"max-width: 800px; margin: auto; padding: 20px; border: 1px solid #ddd; border-radius: 8px;\">\n",
    "        <h1 style=\"text-align: center; margin-bottom: 20px;\">Batch Generation Results</h1>\n",
    "        \n",
    "        <div style=\"margin-bottom: 20px;\">\n",
    "            <h3>Summary</h3>\n",
    "            <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                <tr style=\"background-color: #f2f2f2;\">\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Total Requested</th>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{results.get('total_requested', 'N/A')}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Successfully Generated</th>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{len(results.get('success', []))}</td>\n",
    "                </tr>\n",
    "                <tr style=\"background-color: #f2f2f2;\">\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Failed Attempts</th>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{len(results.get('failed', []))}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Success Rate</th>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{results.get('success_rate', 0):.1f}%</td>\n",
    "                </tr>\n",
    "                <tr style=\"background-color: #f2f2f2;\">\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Timestamp</th>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{results.get('timestamp', 'N/A')}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin-bottom: 20px;\">\n",
    "            <h3>Generated Cases</h3>\n",
    "            <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                <tr style=\"background-color: #f2f2f2;\">\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">#</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Title</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Focus Area</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Region</th>\n",
    "                </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add row for each successful case\n",
    "    for i, case in enumerate(results.get('success', [])):\n",
    "        bg_color = \"#f2f2f2\" if i % 2 == 0 else \"white\"\n",
    "        html += f\"\"\"\n",
    "                <tr style=\"background-color: {bg_color};\">\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{i+1}</td>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{case.get('title', 'Untitled')}</td>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{case.get('focus_area', 'N/A')}</td>\n",
    "                    <td style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">{case.get('region', 'N/A')}</td>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch results when available:\n",
    "# if 'batch_results' in globals():\n",
    "#     display_batch_results(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint Management\n",
    "\n",
    "This section provides tools for managing case generation checkpoints. The system saves interim results after each pipeline stage, enabling:\n",
    "\n",
    "- **Recovery from failures**: Resume generation from the last successful stage\n",
    "- **Process inspection**: Examine intermediate outputs for debugging\n",
    "- **Generation pausing**: Split the generation process across multiple sessions\n",
    "\n",
    "Each case is stored as a single JSON file with a unique ID, tracking the history of all processing stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_checkpoint_manager():\n",
    "    \"\"\"Display an interface for managing checkpoints\"\"\"\n",
    "    # List available checkpoints\n",
    "    checkpoints = list_checkpoints(include_completed=True)\n",
    "    \n",
    "    if not checkpoints:\n",
    "        display(HTML(\"<p style='color:red'>No checkpoints available</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Create HTML table for checkpoints\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "        .checkpoint-table {width:100%; border-collapse:collapse; margin:20px 0}\n",
    "        .checkpoint-table th, .checkpoint-table td {padding:8px; border:1px solid #ddd}\n",
    "        .checkpoint-table tr:nth-child(even) {background-color:#f2f2f2}\n",
    "        .checkpoint-table th {background-color:#4CAF50; color:white; text-align:left}\n",
    "    </style>\n",
    "    <h3>Available Checkpoints</h3>\n",
    "    <table class=\"checkpoint-table\">\n",
    "        <tr>\n",
    "            <th>#</th>\n",
    "            <th>Case ID</th>\n",
    "            <th>Title</th>\n",
    "            <th>Stage</th>\n",
    "            <th>Modified</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, cp in enumerate(checkpoints):\n",
    "        html += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{i+1}</td>\n",
    "            <td>{cp['case_id'][:8]}...</td>\n",
    "            <td>{cp['title']}</td>\n",
    "            <td>{cp['last_stage']}</td>\n",
    "            <td>{cp['modified']}</td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    \n",
    "    # Display instructions for resuming\n",
    "    html += \"\"\"\n",
    "    <p>To resume a checkpoint, use:</p>\n",
    "    <pre>case, filepath = run_case_generation_pipeline(resume_from=\"CASE_ID\")</pre>\n",
    "    <p>Where CASE_ID is the ID from the table above.</p>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the checkpoint manager\n",
    "display_checkpoint_manager()\n",
    "\n",
    "# Examples:\n",
    "# List recent checkpoints\n",
    "# list_checkpoints()\n",
    "\n",
    "# Resume a specific checkpoint\n",
    "# case, filepath = run_case_generation_pipeline(resume_from=\"the-case-id-here\")\n",
    "\n",
    "# Start new case with checkpointing\n",
    "# case, filepath = run_case_generation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Management & Analysis\n",
    "\n",
    "This section provides tools for managing, analyzing, and visualizing logs from the case generation process. The logging system captures detailed information about each stage of generation, including:\n",
    "\n",
    "- **Timing metrics**: How long each stage takes\n",
    "- **LLM interactions**: Requests and responses to language models\n",
    "- **Data retrievals**: Vector searches and results\n",
    "- **Errors and warnings**: Issues encountered during generation\n",
    "- **Overall performance**: Success rates and bottlenecks\n",
    "\n",
    "These tools help identify performance issues, track errors, and analyze trends across multiple case generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_logs(case_id=None, limit=100):\n",
    "    \"\"\"Display logs for a specific case or most recent logs\"\"\"\n",
    "    log_dir = Path(\"../Data/Logs\")\n",
    "    \n",
    "    if not log_dir.exists():\n",
    "        display(HTML(\"<p style='color:red'>No logs directory found</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Find log files\n",
    "    if case_id:\n",
    "        log_files = list(log_dir.glob(f\"case_{case_id}_*.jsonl\"))\n",
    "    else:\n",
    "        log_files = list(log_dir.glob(\"case_*.jsonl\"))\n",
    "    \n",
    "    if not log_files:\n",
    "        display(HTML(\"<p style='color:red'>No log files found</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Sort by modification time (most recent first)\n",
    "    log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    # Select most recent log file\n",
    "    log_file = log_files[0]\n",
    "    \n",
    "    # Read and parse log entries\n",
    "    log_entries = []\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    log_entries.append(entry)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        display(HTML(f\"<p style='color:red'>Error reading log file: {e}</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Limit number of entries\n",
    "    log# filepath: /Users/max/Documents/Code/magdeburg25/Notebooks/04_Case_Generation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_log_file(log_path):\n",
    "    \"\"\"Load and parse a JSONL log file\"\"\"\n",
    "    logs = []\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                logs.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse log line: {line[:50]}...\")\n",
    "    return logs\n",
    "\n",
    "def find_log_files(case_id=None):\n",
    "    \"\"\"Find log files, optionally filtered by case_id\"\"\"\n",
    "    log_dir = Path(\"../Data/Logs\")\n",
    "    if not log_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    if case_id:\n",
    "        pattern = f\"case_{case_id}_*.jsonl\"\n",
    "    else:\n",
    "        pattern = \"case_*.jsonl\"\n",
    "        \n",
    "    return sorted(log_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_log(log_path):\n",
    "    \"\"\"Analyze a log file and extract key metrics\"\"\"\n",
    "    logs = load_log_file(log_path)\n",
    "    if not logs:\n",
    "        return {\"error\": \"No logs found or could not parse log file\"}\n",
    "    \n",
    "    # Extract case ID\n",
    "    case_id = logs[0].get('case_id', 'unknown')\n",
    "    \n",
    "    # Find all stages\n",
    "    stages = []\n",
    "    stage_timings = {}\n",
    "    current_stage = None\n",
    "    stage_start_time = None\n",
    "    \n",
    "    errors = []\n",
    "    llm_calls = []\n",
    "    data_retrievals = []\n",
    "    \n",
    "    for entry in logs:\n",
    "        if entry.get('event') == 'STAGE_START':\n",
    "            current_stage = entry.get('stage')\n",
    "            stage_start_time = datetime.fromisoformat(entry.get('timestamp'))\n",
    "            stages.append(current_stage)\n",
    "        \n",
    "        elif entry.get('event') in ['STAGE_COMPLETE', 'STAGE_FAILED']:\n",
    "            if current_stage and 'data' in entry and 'duration_seconds' in entry['data']:\n",
    "                stage_timings[current_stage] = entry['data']['duration_seconds']\n",
    "                \n",
    "        # Track errors\n",
    "        if entry.get('level') in ['ERROR', 'CRITICAL']:\n",
    "            errors.append(entry)\n",
    "            \n",
    "        # Track LLM calls\n",
    "        if entry.get('event') == 'LLM_REQUEST':\n",
    "            llm_calls.append(entry)\n",
    "            \n",
    "        # Track data retrievals\n",
    "        if entry.get('event') == 'DATA_RETRIEVAL':\n",
    "            data_retrievals.append(entry)\n",
    "    \n",
    "    # Calculate overall timing if available\n",
    "    start_log = next((l for l in logs if l.get('event') == 'LOGGING_INITIALIZED'), None)\n",
    "    end_log = next((l for l in reversed(logs) if l.get('event') == 'GENERATION_COMPLETE'), None)\n",
    "    \n",
    "    total_duration = None\n",
    "    if start_log and end_log:\n",
    "        start_time = datetime.fromisoformat(start_log.get('timestamp'))\n",
    "        end_time = datetime.fromisoformat(end_log.get('timestamp'))\n",
    "        total_duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    return {\n",
    "        \"case_id\": case_id,\n",
    "        \"log_file\": str(log_path),\n",
    "        \"log_entries\": len(logs),\n",
    "        \"stages\": stages,\n",
    "        \"stage_timings\": stage_timings,\n",
    "        \"total_duration\": total_duration,\n",
    "        \"errors\": len(errors),\n",
    "        \"llm_calls\": len(llm_calls),\n",
    "        \"data_retrievals\": len(data_retrievals),\n",
    "        \"success\": any(l.get('event') == 'GENERATION_COMPLETE' for l in logs),\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_log_analysis(log_path):\n",
    "    \"\"\"Display an analysis of a log file with visualizations\"\"\"\n",
    "    analysis = analyze_log(log_path)\n",
    "    if 'error' in analysis:\n",
    "        display(HTML(f\"<p style='color:red'>{analysis['error']}</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Create summary HTML\n",
    "    html = f\"\"\"\n",
    "    <div style=\"max-width: 800px; margin: auto; padding: 20px; border: 1px solid #ddd; border-radius: 8px;\">\n",
    "        <h2>Log Analysis: Case {analysis['case_id']}</h2>\n",
    "        \n",
    "        <div style=\"display: flex; margin-bottom: 20px;\">\n",
    "            <div style=\"flex: 1; padding: 10px; background-color: #f8f9fa; border-radius: 5px; margin-right: 10px;\">\n",
    "                <h3 style=\"margin-top: 0;\">Summary</h3>\n",
    "                <p><b>Log entries:</b> {analysis['log_entries']}</p>\n",
    "                <p><b>Total duration:</b> {analysis['total_duration']:.2f}s ({analysis['total_duration']/60:.2f} min)</p>\n",
    "                <p><b>Success:</b> {\"✅\" if analysis['success'] else \"❌\"}</p>\n",
    "                <p><b>Errors:</b> {analysis['errors']}</p>\n",
    "                <p><b>LLM calls:</b> {analysis['llm_calls']}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div style=\"flex: 1; padding: 10px; background-color: #f8f9fa; border-radius: 5px;\">\n",
    "                <h3 style=\"margin-top: 0;\">Stage Timings</h3>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add stage timing bars\n",
    "    if analysis['stage_timings']:\n",
    "        max_time = max(analysis['stage_timings'].values())\n",
    "        for stage, time in analysis['stage_timings'].items():\n",
    "            percentage = (time / max_time) * 100\n",
    "            html += f\"\"\"\n",
    "                <div style=\"margin-bottom: 10px;\">\n",
    "                    <div>{stage}: {time:.2f}s</div>\n",
    "                    <div style=\"background-color: #ddd; border-radius: 3px; height: 20px; width: 100%;\">\n",
    "                        <div style=\"background-color: #4CAF50; height: 20px; width: {percentage}%; border-radius: 3px;\"></div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "    else:\n",
    "        html += \"<p>No stage timing data available</p>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_management_dashboard():\n",
    "    \"\"\"Display a dashboard for log management\"\"\"\n",
    "    log_files = find_log_files()\n",
    "    \n",
    "    if not log_files:\n",
    "        display(HTML(\"<p>No log files found</p>\"))\n",
    "        return\n",
    "    \n",
    "    # Create basic dashboard\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "    .log-table {width:100%; border-collapse:collapse; margin:20px 0}\n",
    "    .log-table th, .log-table td {padding:8px; border:1px solid #ddd; text-align:left;}\n",
    "    .log-table tr:nth-child(even) {background-color:#f2f2f2}\n",
    "    .log-table th {background-color:#4CAF50; color:white;}\n",
    "    </style>\n",
    "    <h2>Case Generation Logs</h2>\n",
    "    <table class=\"log-table\">\n",
    "        <tr>\n",
    "            <th>#</th>\n",
    "            <th>Case ID</th>\n",
    "            <th>Timestamp</th>\n",
    "            <th>File Size</th>\n",
    "            <th>Action</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, log_file in enumerate(log_files[:20]):  # Limit to 20 most recent\n",
    "        size_kb = log_file.stat().st_size / 1024\n",
    "        timestamp = datetime.fromtimestamp(log_file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        case_id = log_file.name.split('_')[1]  # Extract case ID from filename\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{i+1}</td>\n",
    "            <td>{case_id}</td>\n",
    "            <td>{timestamp}</td>\n",
    "            <td>{size_kb:.1f} KB</td>\n",
    "            <td>\n",
    "                <button onclick=\"IPython.notebook.kernel.execute('display_log_analysis(\\\"{log_file}\\\")')\">\n",
    "                    Analyze\n",
    "                </button>\n",
    "            </td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "    </table>\n",
    "    <p>To analyze a specific log file:</p>\n",
    "    <code>display_log_analysis(\"/path/to/log_file.jsonl\")</code>\n",
    "    \n",
    "    <h3>Log Management Options</h3>\n",
    "    <ul>\n",
    "        <li><b>Cleanup old logs:</b> <code>cleanup_logs(days=30)</code></li>\n",
    "        <li><b>Archive logs:</b> <code>archive_logs(\"../Data/LogArchive\")</code></li>\n",
    "        <li><b>Find logs for a case:</b> <code>find_log_files(\"case_id\")</code></li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_logs(days=30):\n",
    "    \"\"\"Remove logs older than the specified number of days\"\"\"\n",
    "    log_dir = Path(\"../Data/Logs\")\n",
    "    if not log_dir.exists():\n",
    "        print(\"Log directory not found\")\n",
    "        return\n",
    "    \n",
    "    cutoff_time = datetime.now() - timedelta(days=days)\n",
    "    old_logs = []\n",
    "    \n",
    "    for log_file in log_dir.glob(\"*.jsonl\"):\n",
    "        mod_time = datetime.fromtimestamp(log_file.stat().st_mtime)\n",
    "        if mod_time < cutoff_time:\n",
    "            old_logs.append(log_file)\n",
    "    \n",
    "    if not old_logs:\n",
    "        print(f\"No logs older than {days} days found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(old_logs)} logs older than {days} days:\")\n",
    "    for log in old_logs[:5]:\n",
    "        print(f\"- {log.name}\")\n",
    "    \n",
    "    if len(old_logs) > 5:\n",
    "        print(f\"... and {len(old_logs)-5} more\")\n",
    "        \n",
    "    confirm = input(f\"Delete these {len(old_logs)} log files? (yes/no): \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        for log in old_logs:\n",
    "            log.unlink()\n",
    "        print(f\"Deleted {len(old_logs)} log files\")\n",
    "    else:\n",
    "        print(\"Operation cancelled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archive_logs(archive_dir=\"../Data/LogArchive\", days=30):\n",
    "    \"\"\"Archive logs older than the specified number of days\"\"\"\n",
    "    import shutil\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    log_dir = Path(\"../Data/Logs\")\n",
    "    archive_path = Path(archive_dir)\n",
    "    archive_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cutoff_time = datetime.now() - timedelta(days=days)\n",
    "    old_logs = []\n",
    "    \n",
    "    for log_file in log_dir.glob(\"*.jsonl\"):\n",
    "        mod_time = datetime.fromtimestamp(log_file.stat().st_mtime)\n",
    "        if mod_time < cutoff_time:\n",
    "            old_logs.append(log_file)\n",
    "    \n",
    "    if not old_logs:\n",
    "        print(f\"No logs older than {days} days found\")\n",
    "        return\n",
    "    \n",
    "    # Create a ZIP archive\n",
    "    import zipfile\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_path = archive_path / f\"logs_archive_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for log in old_logs:\n",
    "            zipf.write(log, arcname=log.name)\n",
    "    \n",
    "    print(f\"Archived {len(old_logs)} logs to {zip_path}\")\n",
    "    \n",
    "    # Ask if we should delete the original files\n",
    "    confirm = input(\"Delete the archived log files? (yes/no): \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        for log in old_logs:\n",
    "            log.unlink()\n",
    "        print(f\"Deleted {len(old_logs)} log files\")\n",
    "    else:\n",
    "        print(\"Log files preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the log management dashboard (uncomment to use)\n",
    "# log_management_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Arize Phoenix Integration (Optional)\n",
    "\n",
    "This section adds optional LLM monitoring and evaluation capabilities using Arize Phoenix.\n",
    "\n",
    "### Options:\n",
    "\n",
    "1. **Enable Arize Phoenix** (requires additional setup)\n",
    "   - Run the dependency installation\n",
    "   - Restart the kernel\n",
    "   - Set up API keys\n",
    "   - Complete the setup process\n",
    "   \n",
    "2. **Skip Arize Phoenix** (simpler)\n",
    "   - Run `skip_arize_setup()`\n",
    "   - Continue with the notebook without tracing\n",
    "\n",
    "The case generation system works the same either way, but enabling Phoenix provides additional insights into your LLM's performance.\n",
    "\n",
    "### Benefits of Phoenix Tracing (if enabled):\n",
    "\n",
    "- Monitor token usage and costs\n",
    "- Track generation quality\n",
    "- Identify performance bottlenecks\n",
    "- Debug problematic prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_phoenix_dashboard():\n",
    "    \"\"\"Launch Arize Phoenix dashboard\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    # Check if Phoenix is enabled\n",
    "    phoenix_enabled = globals().get('USE_PHOENIX_TRACING', False) and globals().get('arize_tracing_enabled', False)\n",
    "    \n",
    "    if not phoenix_enabled:\n",
    "        display(HTML(\"\"\"\n",
    "        <div style=\"padding: 20px; background-color: #ffeeee; border-radius: 5px;\">\n",
    "            <h3>⚠️ Phoenix Tracing Not Active</h3>\n",
    "            <p>Tracing is not currently active. Please check:</p>\n",
    "            <ol>\n",
    "                <li>API credentials in .env file</li>\n",
    "                <li>Installation of required packages</li>\n",
    "                <li>Phoenix setup execution</li>\n",
    "            </ol>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "        return\n",
    "    \n",
    "    # Phoenix dashboard URLs\n",
    "    cloud_url = \"https://app.arize.com/openinference\"\n",
    "    \n",
    "    # Display links\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"padding: 20px; background-color: #f8f9fa; border-radius: 5px;\">\n",
    "        <h3>Arize Phoenix Dashboard</h3>\n",
    "        \n",
    "        <div style=\"padding: 15px; background-color: white; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "            <p>View all traces in the Arize Cloud dashboard:</p>\n",
    "            <a href=\"{cloud_url}\" target=\"_blank\" style=\"\n",
    "                display: inline-block;\n",
    "                padding: 10px 15px;\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                text-decoration: none;\n",
    "                border-radius: 4px;\n",
    "                font-weight: bold;\">\n",
    "                Open Phoenix Dashboard\n",
    "            </a>\n",
    "            <p style=\"margin-top: 10px; font-size: 0.9em;\">\n",
    "                Project: {PHOENIX_PROJECT_ID}<br>\n",
    "                Environment: {PHOENIX_ENVIRONMENT}\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"padding: 20px; background-color: #f8f9fa; border-radius: 5px;\">\n",
       "        <h3>Arize Phoenix Dashboard</h3>\n",
       "        \n",
       "        <div style=\"padding: 15px; background-color: white; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
       "            <p>View all traces in the Arize Cloud dashboard:</p>\n",
       "            <a href=\"https://app.arize.com/openinference\" target=\"_blank\" style=\"\n",
       "                display: inline-block;\n",
       "                padding: 10px 15px;\n",
       "                background-color: #4CAF50;\n",
       "                color: white;\n",
       "                text-decoration: none;\n",
       "                border-radius: 4px;\n",
       "                font-weight: bold;\">\n",
       "                Open Phoenix Dashboard\n",
       "            </a>\n",
       "            <p style=\"margin-top: 10px; font-size: 0.9em;\">\n",
       "                Project: maritime_logistics_case_generator<br>\n",
       "                Environment: development\n",
       "            </p>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this to show the dashboard link\n",
    "launch_phoenix_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
