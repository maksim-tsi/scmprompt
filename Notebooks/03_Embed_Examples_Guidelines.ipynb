{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidelines and Examples Embeddings Setup\n",
    "\n",
    "This notebook prepares guidelines and case examples for use in case generation by:\n",
    "1. Loading guidelines and splitting them into chunks\n",
    "2. Loading example cases and generating summaries\n",
    "3. Creating embeddings for all content\n",
    "4. Storing in a Qdrant collection for similarity search during case generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.qdrant_client import get_qdrant_client, get_embedding\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google Generative AI\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Connection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Qdrant Cloud\n",
      "Available collections: ['logistics_datapoints']\n",
      "Test embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Test connection to Qdrant\n",
    "client = get_qdrant_client()\n",
    "try:\n",
    "    collections = client.get_collections().collections\n",
    "    print(f\"✅ Successfully connected to Qdrant Cloud\")\n",
    "    print(f\"Available collections: {[c.name for c in collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to Qdrant Cloud: {e}\")\n",
    "    \n",
    "# Test embedding generation\n",
    "test_embedding = get_embedding(\"This is a test for embedding generation\")\n",
    "print(f\"Test embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Collection for Guidelines and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection 'case_generation_references'\n",
      "Created indices for fields: content_type, guideline_type, chunk_id, filename, title\n"
     ]
    }
   ],
   "source": [
    "# Create collection for guidelines and examples\n",
    "COLLECTION_NAME = \"case_generation_references\"\n",
    "\n",
    "# Check if collection already exists\n",
    "collections = client.get_collections().collections\n",
    "if any(c.name == COLLECTION_NAME for c in collections):\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists\")\n",
    "    recreate = input(\"Do you want to recreate the collection? (y/n): \")\n",
    "    if recreate.lower() == 'y':\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"Deleted existing collection '{COLLECTION_NAME}'\")\n",
    "    else:\n",
    "        print(\"Keeping existing collection\")\n",
    "\n",
    "# Create collection if it doesn't exist or was deleted\n",
    "collections = client.get_collections().collections\n",
    "if not any(c.name == COLLECTION_NAME for c in collections):\n",
    "    # Create collection with Google's embedding dimension (768)\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=768,  # Google text-embedding-004 dimension\n",
    "            distance=models.Distance.COSINE\n",
    "        ),\n",
    "        on_disk_payload=True,  # Store payload on disk for larger datasets\n",
    "    )\n",
    "    print(f\"Created collection '{COLLECTION_NAME}'\")\n",
    "    \n",
    "    # Create indices for efficient filtering\n",
    "    index_fields = [\n",
    "        \"content_type\",  # guideline or example\n",
    "        \"guideline_type\",  # general, bishou, maritime, container\n",
    "        \"chunk_id\",\n",
    "        \"filename\",\n",
    "        \"title\"\n",
    "    ]\n",
    "    \n",
    "    for field in index_fields:\n",
    "        client.create_payload_index(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            field_name=field,\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "    print(f\"Created indices for fields: {', '.join(index_fields)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test generation response: Maritime logistics is about efficiently and effectively managing the flow of goods, information, and resources across the sea to ensure timely, secure, and cost-effective delivery from origin to destination.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_with_llm(prompt, model=\"gemini-2.0-flash-exp\", max_retries=3):\n",
    "    \"\"\"Generate text using Gemini with rate limiting and retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model_instance = genai.GenerativeModel(model_name=model)\n",
    "            response = model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt + 1  # Exponential backoff\n",
    "                print(f\"Generation attempt {attempt+1} failed: {e}. Retrying after {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                return \"Failed to generate content\"\n",
    "\n",
    "# Test the generation function\n",
    "test_response = generate_with_llm(\"Summarize the key principles of maritime logistics in one sentence.\")\n",
    "print(f\"Test generation response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for markdown processing\n",
    "import re\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown(text):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_elements = markdown_splitter.split_text(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for element in md_elements:\n",
    "        element_text = element.page_content\n",
    "        element_len = len(element_text)\n",
    "\n",
    "        if current_chunk_len + element_len > 2000:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "                current_chunk_len = 0\n",
    "\n",
    "            # If the element itself is longer than 2000 characters, split it\n",
    "            while element_len > 2000:\n",
    "                chunks.append(element_text[:2000].strip())\n",
    "                element_text = element_text[2000:]\n",
    "                element_len -= 2000\n",
    "\n",
    "            current_chunk = element_text\n",
    "            current_chunk_len = element_len\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + element_text if current_chunk else element_text\n",
    "            current_chunk_len += element_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_markdown_with_metadata(text, guideline_type, file_path):\n",
    "    \"\"\"Process markdown with enhanced metadata\"\"\"\n",
    "    chunks = split_markdown(text)\n",
    "    results = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        title = extract_title(chunk)\n",
    "        # Calculate chunk overlap with next chunk if available\n",
    "        overlap = \"\"\n",
    "        if i < len(chunks) - 1:\n",
    "            next_chunk = chunks[i + 1]\n",
    "            next_lines = next_chunk.split(\"\\n\")\n",
    "            # Get header of next section for context\n",
    "            for line in next_lines:\n",
    "                if re.match(r'^#+\\s+', line):\n",
    "                    overlap = line\n",
    "                    break\n",
    "                    \n",
    "        # Get header level for better hierarchy understanding\n",
    "        header_level = 1\n",
    "        lines = chunk.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            header_match = re.match(r'^(#+)\\s+', line)\n",
    "            if header_match:\n",
    "                header_level = len(header_match.group(1))\n",
    "                break\n",
    "                \n",
    "        results.append({\n",
    "            \"content_type\": \"guideline\",\n",
    "            \"guideline_type\": guideline_type,\n",
    "            \"chunk_id\": f\"{guideline_type}_{i}\",\n",
    "            \"title\": title,\n",
    "            \"content\": chunk,\n",
    "            \"header_level\": header_level,\n",
    "            \"next_section\": overlap,\n",
    "            \"filename\": file_path.split(\"/\")[-1],\n",
    "            \"position\": i\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(chunk):\n",
    "    \"\"\"Extract title from a markdown chunk\"\"\"\n",
    "    lines = chunk.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Look for a markdown header\n",
    "        if re.match(r'^#+\\s+', line):\n",
    "            return re.sub(r'^#+\\s+', '', line).strip()\n",
    "    # If no header is found, use the first non-empty line\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            return line[:50].strip() + '...' if len(line) > 50 else line.strip()\n",
    "    return \"Untitled Section\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_guidelines(file_path=None, guideline_type=None):\n",
    "    \"\"\"\n",
    "    Load and chunk a single guideline file or all guidelines\n",
    "    \n",
    "    Args:\n",
    "        file_path (str, optional): Path to a specific guideline file. If None, processes all guideline files.\n",
    "        guideline_type (str, optional): Type of guideline. If None, tries to infer from filename.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of guideline chunks\n",
    "    \"\"\"\n",
    "    # Default paths for all guidelines\n",
    "    all_guideline_paths = {\n",
    "        \"general\": \"../Docs/Guidelines/3_Case_Generation_Guideline.md\",\n",
    "        \"bishou\": \"../Docs/Guidelines/3_Case_Generation_Bishou.md\", \n",
    "        \"maritime\": \"../Docs/Guidelines/3_Case_Generation_Maritime_Logistics.md\",\n",
    "        \"ocean\": \"../Docs/Guidelines/3_Case_Generation_Ocean_Container.md\"\n",
    "    }\n",
    "    \n",
    "    guideline_chunks = []\n",
    "    \n",
    "    # If a specific file is provided, process only that file\n",
    "    if file_path:\n",
    "        paths_to_process = {}\n",
    "        \n",
    "        # If guideline type is not provided, try to infer from filename\n",
    "        if guideline_type is None:\n",
    "            filename = os.path.basename(file_path)\n",
    "            if \"Guideline\" in filename:\n",
    "                guideline_type = \"general\"\n",
    "            elif \"Bishou\" in filename:\n",
    "                guideline_type = \"bishou\"\n",
    "            elif \"Maritime\" in filename:\n",
    "                guideline_type = \"maritime\"\n",
    "            elif \"Ocean\" in filename or \"Container\" in filename:\n",
    "                guideline_type = \"ocean\"\n",
    "            else:\n",
    "                guideline_type = \"custom\"\n",
    "                \n",
    "        paths_to_process[guideline_type] = file_path\n",
    "    else:\n",
    "        # Process all default guideline files\n",
    "        paths_to_process = all_guideline_paths\n",
    "    \n",
    "    # Process each file\n",
    "    for guideline_type, path in paths_to_process.items():\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                content_length = len(content)\n",
    "                \n",
    "                print(f\"Processing {path}: {content_length} characters\")\n",
    "                \n",
    "                # Try header-based splitting first\n",
    "                chunks = re.split(r'(?=#+\\s)', content)\n",
    "                chunk_method = \"header\"\n",
    "                print(f\"  - Header-based chunks for {guideline_type}: {len(chunks)}\")\n",
    "                \n",
    "                # If only one chunk, skip to size-based splitting with overlap\n",
    "                if len(chunks) <= 1:\n",
    "                    chunks = []\n",
    "                    chunk_size = 1800\n",
    "                    overlap = 100\n",
    "                    \n",
    "                    # Generate chunks with overlap\n",
    "                    for i in range(0, len(content), chunk_size - overlap):\n",
    "                        if i > 0:  # Not the first chunk\n",
    "                            start_pos = i\n",
    "                        else:\n",
    "                            start_pos = 0\n",
    "                            \n",
    "                        chunk = content[start_pos:start_pos + chunk_size]\n",
    "                        if len(chunk.strip()) > 50:  # Skip very small chunks\n",
    "                            chunks.append(chunk)\n",
    "                    \n",
    "                    chunk_method = \"size\"\n",
    "                    print(f\"  - Size-based chunks with {overlap} char overlap: {len(chunks)}\")\n",
    "                \n",
    "                file_chunks = 0\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if len(chunk.strip()) > 50:  # Skip very small chunks\n",
    "                        title = extract_title(chunk) or f\"{guideline_type} section {i+1}\"\n",
    "                        guideline_chunks.append({\n",
    "                            \"content_type\": \"guideline\",\n",
    "                            \"guideline_type\": guideline_type,\n",
    "                            \"chunk_id\": f\"{guideline_type}_{i}\",\n",
    "                            \"title\": title,\n",
    "                            \"content\": chunk,\n",
    "                            \"filename\": path.split(\"/\")[-1],\n",
    "                            \"chunk_method\": chunk_method,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"total_chunks\": len(chunks)\n",
    "                        })\n",
    "                        file_chunks += 1\n",
    "                \n",
    "                print(f\"  - Final chunks for {guideline_type}: {file_chunks}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading guideline {path}: {e}\")\n",
    "    \n",
    "    print(f\"Created {len(guideline_chunks)} chunks from {len(paths_to_process)} guideline files\")\n",
    "    return guideline_chunks\n",
    "\n",
    "# Example usage:\n",
    "# Process a single file\n",
    "# guideline_chunks = load_and_chunk_guidelines(\"../Docs/Guidelines/3_Case_Generation_Guideline.md\")\n",
    "\n",
    "# Process all default files\n",
    "# guideline_chunks = load_and_chunk_guidelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../Docs/Guidelines/3_Case_Generation_Guideline.md: 16838 characters\n",
      "  - Header-based chunks for general: 1\n",
      "  - Size-based chunks with 100 char overlap: 10\n",
      "  - Final chunks for general: 10\n",
      "Processing ../Docs/Guidelines/3_Case_Generation_Bishou.md: 12466 characters\n",
      "  - Header-based chunks for bishou: 1\n",
      "  - Size-based chunks with 100 char overlap: 8\n",
      "  - Final chunks for bishou: 8\n",
      "Processing ../Docs/Guidelines/3_Case_Generation_Maritime_Logistics.md: 16838 characters\n",
      "  - Header-based chunks for maritime: 1\n",
      "  - Size-based chunks with 100 char overlap: 10\n",
      "  - Final chunks for maritime: 10\n",
      "Processing ../Docs/Guidelines/3_Case_Generation_Ocean_Container.md: 14223 characters\n",
      "  - Header-based chunks for ocean: 1\n",
      "  - Size-based chunks with 100 char overlap: 9\n",
      "  - Final chunks for ocean: 9\n",
      "Created 37 chunks from 4 guideline files\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk guidelines\n",
    "guideline_chunks = load_and_chunk_guidelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(guideline_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Example Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_summarize_examples():\n",
    "    \"\"\"Load example cases and generate summaries using the same approach as guidelines\"\"\"\n",
    "    example_dir = \"../Data/Cases/\"\n",
    "    example_items = []\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(example_dir):\n",
    "        print(f\"Warning: Example directory {example_dir} not found\")\n",
    "        return []\n",
    "    \n",
    "    # Get list of markdown files first\n",
    "    md_files = [f for f in os.listdir(example_dir) if f.endswith(\".md\")]\n",
    "    print(f\"Found {len(md_files)} example case files\")\n",
    "    \n",
    "    # Process each file\n",
    "    for i, file in enumerate(tqdm(md_files, desc=\"Processing example cases\")):\n",
    "        try:\n",
    "            with open(os.path.join(example_dir, file), 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Extract title from content or filename\n",
    "                title = extract_title(content) or file.replace(\".md\", \"\").replace(\"_\", \" \")\n",
    "                \n",
    "                # Create a summary for embedding\n",
    "                summary_prompt = f\"\"\"\n",
    "                Read the following case example and create a concise summary that captures:\n",
    "                - The main scenario\n",
    "                - Key entities involved\n",
    "                - Core regulatory issues\n",
    "                - Problem to be solved\n",
    "                \n",
    "                EXAMPLE CASE:\n",
    "                {content[:7000]}\n",
    "                \n",
    "                SUMMARY (focus only on the key aspects, be concise):\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Generating summary for {file} ({i+1}/{len(md_files)})...\")\n",
    "                \n",
    "                # Generate the summary\n",
    "                try:\n",
    "                    summary = generate_with_llm(summary_prompt)\n",
    "                    print(f\"  ✓ Summary generated: {len(summary)} characters\")\n",
    "                    time.sleep(6)  # Rate limit\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error generating summary: {e}\")\n",
    "                    summary = None\n",
    "                \n",
    "                # Add the example to our list (similar structure to guidelines)\n",
    "                example_items.append({\n",
    "                    \"content_type\": \"example\",\n",
    "                    \"example_type\": \"case_study\",\n",
    "                    \"chunk_id\": f\"example_{file}\",\n",
    "                    \"title\": title,\n",
    "                    \"content\": content,\n",
    "                    \"summary\": summary,\n",
    "                    \"filename\": file,\n",
    "                    \"word_count\": len(content.split())\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(example_items)} example cases\")\n",
    "    return example_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 example case files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Global_Trust.md (1/9)...\n",
      "  ✓ Summary generated: 573 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  11%|█         | 1/9 [00:07<00:58,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for GPT_Case_Jamestown_Engine.md (2/9)...\n",
      "  ✓ Summary generated: 812 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  22%|██▏       | 2/9 [00:14<00:50,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for GPT_Case_Global_Semiconductor.md (3/9)...\n",
      "  ✓ Summary generated: 735 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  33%|███▎      | 3/9 [00:21<00:43,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Famine_Relief.md (4/9)...\n",
      "  ✓ Summary generated: 957 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  44%|████▍     | 4/9 [00:29<00:36,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Manwell_Toy.md (5/9)...\n",
      "  ✓ Summary generated: 809 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  56%|█████▌    | 5/9 [00:36<00:29,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for GPT_Case_Nicholas_Vroom.md (6/9)...\n",
      "  ✓ Summary generated: 827 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  67%|██████▋   | 6/9 [00:43<00:22,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Fedco_Fasteners.md (7/9)...\n",
      "  ✓ Summary generated: 657 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  78%|███████▊  | 7/9 [00:51<00:14,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Great_Bite.md (8/9)...\n",
      "  ✓ Summary generated: 839 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases:  89%|████████▉ | 8/9 [00:58<00:07,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Case_Barbara_Blouse.md (9/9)...\n",
      "  ✓ Summary generated: 578 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing example cases: 100%|██████████| 9/9 [01:06<00:00,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 9 example cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the examples\n",
    "example_items = load_and_summarize_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example summaries:\n",
      "\n",
      "Example 1: CASE: Global Trust Company\n",
      "Summary: *   **Scenario:** A commercial loan officer at an international bank is tasked with assessing the bank's potential investments in the transportation a...\n",
      "\n",
      "Example 2: CASE: Global Trust Company\n",
      "Summary: *   **Scenario:** Global Trust Company, an international bank, is evaluating loan and investment opportunities in the transportation and logistics sec...\n",
      "\n",
      "Saved examples to '../Data/examples_with_summaries.json'\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "if example_items:\n",
    "    print(\"\\nExample summaries:\")\n",
    "    for i, item in enumerate(example_items[:2]):  # Show first 2\n",
    "        print(f\"\\nExample {i+1}: {item['title']}\")\n",
    "        if item['summary']:\n",
    "            print(f\"Summary: {item['summary'][:150]}...\")\n",
    "        else:\n",
    "            print(\"Summary: [Failed to generate]\")\n",
    "            \n",
    "    # Save to JSON for backup\n",
    "    import json\n",
    "    with open(\"../Data/examples_with_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(example_items, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nSaved examples to '../Data/examples_with_summaries.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings and Add to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 46 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:12<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 46 items to collection 'case_generation_references'\n"
     ]
    }
   ],
   "source": [
    "def add_embeddings_to_qdrant(items, collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Generate embeddings and add to Qdrant collection\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    points = []\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(items)} items...\")\n",
    "    for item in tqdm(items):\n",
    "        # For guidelines, embed the content\n",
    "        # For examples, embed the summary\n",
    "        text_to_embed = item['summary'] if 'summary' in item else item['content']\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = get_embedding(text_to_embed)\n",
    "        if embedding is None:\n",
    "            print(f\"Warning: Failed to generate embedding for {item.get('chunk_id', item.get('title', 'unknown item'))}\")\n",
    "            continue\n",
    "                \n",
    "        # Generate a unique ID\n",
    "        unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, item['chunk_id']))\n",
    "                \n",
    "        # Add point to batch\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=unique_id,\n",
    "                vector=embedding,\n",
    "                payload=item\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Upload points\n",
    "    if points:\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"✅ Added {len(points)} items to collection '{collection_name}'\")\n",
    "    else:\n",
    "        print(\"No valid points to add\")\n",
    "\n",
    "# Combine all items and add to Qdrant\n",
    "all_items = guideline_chunks + example_items\n",
    "add_embeddings_to_qdrant(all_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_references(query, content_type=None, limit=3, collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Find similar guidelines or examples based on the query\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        print(\"Failed to generate embedding for query\")\n",
    "        return []\n",
    "    \n",
    "    # Build search filter\n",
    "    search_filter = None\n",
    "    if content_type:\n",
    "        search_filter = models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"content_type\",\n",
    "                    match=models.MatchValue(value=content_type)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # Execute search\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=search_filter,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_reference_results(results, query=None):\n",
    "    \"\"\"Display search results in a readable way\"\"\"\n",
    "    if query:\n",
    "        display(Markdown(f\"## Search results for: '{query}'\"))\n",
    "    \n",
    "    if not results:\n",
    "        display(Markdown(\"*No results found*\"))\n",
    "        return\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        content_type = result.payload.get('content_type', 'unknown')\n",
    "        guideline_type = result.payload.get('guideline_type', 'unknown')\n",
    "        title = result.payload.get('title', 'Untitled')\n",
    "        \n",
    "        display(Markdown(f\"### Result {i} - Score: {result.score:.4f}\"))\n",
    "        display(Markdown(f\"**Type:** {content_type} ({guideline_type})  \\n**Title:** {title}\"))\n",
    "        \n",
    "        # Show content preview\n",
    "        content = result.payload.get('content', '')\n",
    "        if content:\n",
    "            preview = content[:300] + '...' if len(content) > 300 else content\n",
    "            display(Markdown(f\"**Preview:**  \\n{preview}\"))\n",
    "        \n",
    "        # Show summary for examples\n",
    "        if 'summary' in result.payload:\n",
    "            display(Markdown(f\"**Summary:**  \\n{result.payload['summary']}\"))\n",
    "            \n",
    "        display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Some Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_58273/3338464716.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Search results for: 'Creating realistic logistics scenarios for container shipping'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 1 - Score: 0.7450"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** guideline (ocean)  \n",
       "**Title:** nue using the Refined Port Arrival Document Checkl..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "nue using the Refined Port Arrival Document Checklist Template (Version 3 - Security & Efficiency Focused) as the basis for creating \"ideal\" checklists for your scenarios.\n",
       "\n",
       "This **Handbook of Ocean Container Transport Logistics Research Report** provides a focused and actionable guide for generating..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 2 - Score: 0.7287"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** guideline (bishou)  \n",
       "**Title:** omplete or inaccurate data in the 24-hour manifest..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "omplete or inaccurate data in the 24-hour manifest, leading to penalties and inspection delays.\n",
       "        *   Scenario where a shipping line struggles to meet the 24-hour manifest cut-off time, requiring expedited documentation procedures and potentially incurring extra costs.\n",
       "        *   Scenario hig..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test query against guidelines\n",
    "guideline_query = \"Creating realistic logistics scenarios for container shipping\"\n",
    "guideline_results = find_similar_references(guideline_query, content_type=\"guideline\", limit=2)\n",
    "display_reference_results(guideline_results, guideline_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_58273/3338464716.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Search results for: 'Customs documentation requirements for imports'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 1 - Score: 0.5946"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** example (unknown)  \n",
       "**Title:** CASE • Manwell Toy Importers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "# CASE • Manwell Toy Importers\n",
       "\n",
       "Headquartered in Winnipeg, Manitoba, Manwell Toy Company operated a large toy store on Portage Avenue, near the location where the company had been founded nearly seventy years ago. Above the toy store were offices from which the firm managed its chain of fifteen reta..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary:**  \n",
       "**Scenario:** Manwell Toy Importers, a Canadian toy retailer, is evaluating its Asian supply chain. Currently, toys are consolidated in Hong Kong and shipped to Vancouver.\n",
       "\n",
       "**Entities:**\n",
       "\n",
       "*   Manwell Toy Importers (Importer/Retailer)\n",
       "*   Asian Toy Manufacturers\n",
       "*   Consolidators (Hong Kong)\n",
       "*   Port of Vancouver\n",
       "*   Port of Halifax (potential)\n",
       "\n",
       "**Regulatory Issues:**  The case does not mention any specific regulatory issues.\n",
       "\n",
       "**Problem:** Manwell needs to optimize its inbound logistics to reduce costs and better serve different Canadian markets, specifically exploring the feasibility of shipping via Singapore to Halifax versus the current Hong Kong to Vancouver route. They also need to consider how to cater to the French-speaking Quebec market and the impact of e-commerce on warehousing locations.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 2 - Score: 0.5937"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** example (unknown)  \n",
       "**Title:** CASE • Barbara’s Blouses"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "# CASE • Barbara’s Blouses\n",
       "\n",
       "Barbara Linse buys blouses for a chain of ladies’ wear stores in major U.S. cities west of the Mississippi River. They sell clothing made in both the United States and Asia. Asia supplies an increasing amount and percentage of blouses that the chain sells. Twice yearly Ba..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary:**  \n",
       "*   **Scenario:** Barbara Linse, a blouse buyer for a US ladies' wear chain, primarily sources from Asia. She makes biannual trips to place orders and coordinate with other buyers.\n",
       "*   **Key Entities:** Barbara Linse (buyer), Ladies' wear chain (employer), Asian blouse manufacturers.\n",
       "*   **Core Regulatory Issues:** Ethical sourcing (child/prison labor), import regulations, international trade (tariffs).\n",
       "*   **Problem to be solved:** How to effectively and ethically manage the overseas blouse sourcing process, considering costs, logistics, quality control, and compliance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test query against examples\n",
    "example_query = \"Customs documentation requirements for imports\"\n",
    "example_results = find_similar_references(example_query, content_type=\"example\", limit=2)\n",
    "display_reference_results(example_results, example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_58273/3338464716.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Search results for: 'Maritime logistics challenges in international shipping'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 1 - Score: 0.7137"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** example (unknown)  \n",
       "**Title:** CASE: Global Trust Company"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "# CASE: Global Trust Company\n",
       "\n",
       "Betsy Bertram had worked in the commercial loan office of the Farmers and Merchants’ Bank in Chicago, which recently had become part of an international banking conglomerate headquartered in Amsterdam. She supervised the section that handled loans to carriers, firms tha..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Summary:**  \n",
       "*   **Scenario:** Global Trust Company, an international bank, is evaluating loan and investment opportunities in the transportation and logistics sector.\n",
       "*   **Key Entities:** Betsy Bertram (loan officer), new boss (from IT), potential borrowers/investees in shipping, supply chain, and transportation.\n",
       "*   **Core Regulatory Issues:** Environmental regulations and compliance costs, particularly concerning \"green\" initiatives, and international maritime regulations.\n",
       "*   **Problem to be solved:** How should the bank assess the risks and potential of different investment opportunities in the transportation and logistics industry, considering factors like vessel size optimization, market demand for new services, environmental impacts, infrastructure investments, internet growth and even space exploration.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 2 - Score: 0.7062"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** guideline (general)  \n",
       "**Title:** ations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "ations.\n",
       "    *   Example Scenario Ideas: (Build upon previous ideas, now emphasizing workflow and comprehensive document coverage)\n",
       "        *   Typical container vessel arrival at a major European port (Rotterdam, Hamburg, Antwerp-Bruges, Riga) requiring a *full checklist* covering all essential comme..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Result 3 - Score: 0.7062"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Type:** guideline (maritime)  \n",
       "**Title:** ations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Preview:**  \n",
       "ations.\n",
       "    *   Example Scenario Ideas: (Build upon previous ideas, now emphasizing workflow and comprehensive document coverage)\n",
       "        *   Typical container vessel arrival at a major European port (Rotterdam, Hamburg, Antwerp-Bruges, Riga) requiring a *full checklist* covering all essential comme..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query across all content types\n",
    "general_query = \"Maritime logistics challenges in international shipping\"\n",
    "all_results = find_similar_references(general_query, limit=3)\n",
    "display_reference_results(all_results, general_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Collection Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total items in collection: **46**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Content Types"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **guideline**: 37"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **example**: 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Guideline Types"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **general**: 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **maritime**: 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **unknown**: 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **ocean**: 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **bishou**: 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count items by type\n",
    "client = get_qdrant_client()\n",
    "total_count = client.count(collection_name=COLLECTION_NAME).count\n",
    "\n",
    "# Get all points to analyze\n",
    "points, _ = client.scroll(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    limit=total_count,\n",
    "    with_payload=[\"content_type\", \"guideline_type\"],\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "# Count by type\n",
    "content_types = {}\n",
    "guideline_types = {}\n",
    "\n",
    "for point in points:\n",
    "    c_type = point.payload.get(\"content_type\", \"unknown\")\n",
    "    g_type = point.payload.get(\"guideline_type\", \"unknown\")\n",
    "    \n",
    "    content_types[c_type] = content_types.get(c_type, 0) + 1\n",
    "    guideline_types[g_type] = guideline_types.get(g_type, 0) + 1\n",
    "\n",
    "# Display summary\n",
    "display(Markdown(f\"## Collection Summary\"))\n",
    "display(Markdown(f\"Total items in collection: **{total_count}**\"))\n",
    "\n",
    "display(Markdown(f\"### Content Types\"))\n",
    "for ctype, count in content_types.items():\n",
    "    display(Markdown(f\"- **{ctype}**: {count}\"))\n",
    "\n",
    "display(Markdown(f\"### Guideline Types\"))\n",
    "for gtype, count in guideline_types.items():\n",
    "    if gtype != \"example\":\n",
    "        display(Markdown(f\"- **{gtype}**: {count}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
