{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant Cloud Setup for Logistics Datapoints\n",
    "\n",
    "This notebook sets up the Qdrant Cloud collection for storing and retrieving normalized datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (1.13.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: google-generativeai in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (0.8.3)\n",
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (4.67.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (1.71.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (1.71.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (2.1.3)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (2.9.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from qdrant-client) (2.3.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (2.24.1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (2.160.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (5.28.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-core->google-generativeai) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: setuptools in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.1.0)\n",
      "Requirement already satisfied: anyio in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.8.0)\n",
      "Requirement already satisfied: certifi in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from pydantic>=1.10.8->qdrant-client) (2.23.4)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install qdrant-client python-dotenv google-generativeai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All required environment variables are set\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.qdrant_client import get_qdrant_client, get_embedding, COLLECTION_NAME\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if environment variables are set\n",
    "required_vars = ['QDRANT_URL', 'QDRANT_API_KEY', 'GOOGLE_API_KEY']\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"Please set these in your .env file\")\n",
    "else:\n",
    "    print(\"✅ All required environment variables are set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Configure Google Generative AI\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "# Function to generate embeddings using Google's text-embedding-004\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate embeddings using Google's text-embedding-004 model\"\"\"\n",
    "    try:\n",
    "        embedding_response = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=text,\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        \n",
    "        return embedding_response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test embedding generation\n",
    "test_embedding = get_embedding(\"This is a test for embedding generation\")\n",
    "print(f\"Test embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Qdrant Cloud\n",
      "Available collections: ['logistics_datapoints']\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    collections = client.get_collections().collections\n",
    "    print(f\"✅ Successfully connected to Qdrant Cloud\")\n",
    "    print(f\"Available collections: {[c.name for c in collections]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to Qdrant Cloud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Collection for Logistics Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'logistics_datapoints' already exists\n",
      "Deleted existing collection 'logistics_datapoints'\n",
      "Created collection 'logistics_datapoints'\n",
      "Created indices for fields: datapoint_id, datapoint_type, port_area, domain_area, relevant_entity, regulation_category, regulation_subcategory, document_type, source_document, keywords\n"
     ]
    }
   ],
   "source": [
    "# Create collection for logistics datapoints\n",
    "collection_name = \"logistics_datapoints\"\n",
    "\n",
    "# Check if collection already exists\n",
    "collections = client.get_collections().collections\n",
    "if any(c.name == collection_name for c in collections):\n",
    "    print(f\"Collection '{collection_name}' already exists\")\n",
    "    recreate = input(\"Do you want to recreate the collection? (y/n): \")\n",
    "    if recreate.lower() == 'y':\n",
    "        client.delete_collection(collection_name)\n",
    "        print(f\"Deleted existing collection '{collection_name}'\")\n",
    "    else:\n",
    "        print(\"Keeping existing collection\")\n",
    "\n",
    "# Create collection if it doesn't exist or was deleted\n",
    "collections = client.get_collections().collections\n",
    "if not any(c.name == collection_name for c in collections):\n",
    "    # Create collection with Google's embedding dimension (768)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=768,  # Google text-embedding-004 dimension\n",
    "            distance=models.Distance.COSINE\n",
    "        ),\n",
    "        on_disk_payload=True,  # Store payload on disk for larger datasets\n",
    "    )\n",
    "    print(f\"Created collection '{collection_name}'\")\n",
    "    \n",
    "    # Create indices for efficient filtering\n",
    "    index_fields = [\n",
    "        \"datapoint_id\", \n",
    "        \"datapoint_type\", \n",
    "        \"port_area\", \n",
    "        \"domain_area\",\n",
    "        \"relevant_entity\", \n",
    "        \"regulation_category\", \n",
    "        \"regulation_subcategory\", \n",
    "        \"document_type\",\n",
    "        \"source_document\",\n",
    "        \"keywords\"\n",
    "    ]\n",
    "    \n",
    "    for field in index_fields:\n",
    "        if field == \"keywords\":\n",
    "            # Keywords is an array field\n",
    "            client.create_payload_index(\n",
    "                collection_name=collection_name,\n",
    "                field_name=field,\n",
    "                field_schema=models.PayloadSchemaType.KEYWORD\n",
    "            )\n",
    "        else:\n",
    "            # Other fields are keyword fields\n",
    "            client.create_payload_index(\n",
    "                collection_name=collection_name,\n",
    "                field_name=field,\n",
    "                field_schema=models.PayloadSchemaType.KEYWORD\n",
    "            )\n",
    "    print(f\"Created indices for fields: {', '.join(index_fields)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Normalized Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 normalized datapoint files\n"
     ]
    }
   ],
   "source": [
    "# Find all normalized datapoint files\n",
    "normalized_dir = \"../Data/Datapoints\"\n",
    "datapoint_files = glob.glob(f\"{normalized_dir}/**/*_Datapoints.json\", recursive=True)\n",
    "print(f\"Found {len(datapoint_files)} normalized datapoint files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datapoints(file_path, batch_size=100, collection_name=COLLECTION_NAME):\n",
    "    \"\"\"Load datapoints from a JSON file and add them to Qdrant collection.\"\"\"\n",
    "    client = get_qdrant_client()\n",
    "    loaded = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            datapoints = json.load(f)\n",
    "            \n",
    "        # Process datapoints in batches\n",
    "        points = []\n",
    "        \n",
    "        for datapoint in datapoints:\n",
    "            if 'regulation_detail' not in datapoint or not datapoint['regulation_detail']:\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            # Generate embedding\n",
    "            embedding = get_embedding(datapoint['regulation_detail'])\n",
    "            if embedding is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            # Convert string ID to valid UUID\n",
    "            datapoint_id = datapoint.get('datapoint_id')\n",
    "            if datapoint_id:\n",
    "                try:\n",
    "                    # Check if datapoint_id is already a valid UUID\n",
    "                    uuid.UUID(datapoint_id)\n",
    "                    valid_id = datapoint_id\n",
    "                except ValueError:\n",
    "                    # Convert the string to a valid UUID using uuid5\n",
    "                    valid_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, datapoint_id))\n",
    "            else:\n",
    "                valid_id = str(uuid.uuid4())\n",
    "                \n",
    "            # Add point to batch\n",
    "            points.append(\n",
    "                models.PointStruct(\n",
    "                    id=valid_id,\n",
    "                    vector=embedding,\n",
    "                    payload=datapoint\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Upload batch if it reaches batch_size\n",
    "            if len(points) >= batch_size:\n",
    "                client.upsert(\n",
    "                    collection_name=collection_name,\n",
    "                    points=points\n",
    "                )\n",
    "                loaded += len(points)\n",
    "                points = []\n",
    "                \n",
    "        # Upload any remaining points\n",
    "        if points:\n",
    "            client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                points=points\n",
    "            )\n",
    "            loaded += len(points)\n",
    "            \n",
    "        return loaded, skipped\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datapoints from {file_path}: {e}\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6c391e86874ed38ba3238ac0b333e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1/8: Rotterdam_Datapoints.json\n",
      "  ✓ Loaded: 82, Skipped: 0\n",
      "File 2/8: Yangshan_Datapoints.json\n",
      "  ✓ Loaded: 290, Skipped: 0\n",
      "File 3/8: Hamburg_Datapoints.json\n",
      "  ✓ Loaded: 65, Skipped: 0\n",
      "File 4/8: Antwerp_Datapoints.json\n",
      "  ✓ Loaded: 68, Skipped: 0\n",
      "File 5/8: Singapore_Datapoints.json\n",
      "  ✓ Loaded: 270, Skipped: 0\n",
      "File 6/8: Riga_Datapoints.json\n",
      "  ✓ Loaded: 128, Skipped: 0\n",
      "File 7/8: IMO_Datapoints.json\n",
      "  ✓ Loaded: 170, Skipped: 0\n",
      "File 8/8: INCOTERMS_Datapoints.json\n",
      "  ✓ Loaded: 273, Skipped: 0\n",
      "\n",
      "====== LOADING SUMMARY ======\n",
      "✅ Successfully loaded 1346 datapoints\n",
      "⚠️ Skipped 0 datapoints (missing detail or embedding failed)\n",
      "\n",
      "✅ All files processed successfully!\n",
      "\n",
      "✓ Collection 'logistics_datapoints' now contains 1345 datapoints\n"
     ]
    }
   ],
   "source": [
    "# Load datapoints with checkpoints and better error handling\n",
    "total_loaded = 0\n",
    "total_skipped = 0\n",
    "failed_files = []\n",
    "\n",
    "# Make sure we're using the correct collection name\n",
    "collection_name = COLLECTION_NAME  # Use the imported constant\n",
    "\n",
    "for i, file_path in enumerate(tqdm(datapoint_files, desc=\"Processing files\")):\n",
    "    try:\n",
    "        loaded, skipped = load_datapoints(file_path, collection_name=collection_name)\n",
    "        total_loaded += loaded\n",
    "        total_skipped += skipped\n",
    "        \n",
    "        # Add checkpoint after each file\n",
    "        print(f\"File {i+1}/{len(datapoint_files)}: {os.path.basename(file_path)}\")\n",
    "        print(f\"  ✓ Loaded: {loaded}, Skipped: {skipped}\")\n",
    "        \n",
    "        # Small pause between files to avoid rate limiting\n",
    "        if i < len(datapoint_files) - 1:\n",
    "            time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {os.path.basename(file_path)}: {e}\")\n",
    "        failed_files.append(file_path)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n====== LOADING SUMMARY ======\")\n",
    "print(f\"✅ Successfully loaded {total_loaded} datapoints\")\n",
    "print(f\"⚠️ Skipped {total_skipped} datapoints (missing detail or embedding failed)\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\n❌ Failed to process {len(failed_files)} files:\")\n",
    "    for failed in failed_files:\n",
    "        print(f\"  - {os.path.basename(failed)}\")\n",
    "else:\n",
    "    print(\"\\n✅ All files processed successfully!\")\n",
    "\n",
    "# Verify points in collection using imported function\n",
    "try:\n",
    "    client = get_qdrant_client()\n",
    "    count = client.count(collection_name=collection_name).count\n",
    "    print(f\"\\n✓ Collection '{collection_name}' now contains {count} datapoints\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error verifying collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Requirements for importing hazardous materials into Singapore\n",
      "\n",
      "Results:\n",
      "\n",
      "1. SINGAPORE_DP_CUSTOMS_PERMIT_APPLICATION_TRADENET_021 (Score: 0.8363)\n",
      "   Type: Procedure\n",
      "   Entity: Importer\n",
      "   Detail: Permit applications for importing goods into Singapore, including containerized cargo, must be submitted via TradeNet®.\n",
      "   Keywords: permit application, import, TradeNet, containerized cargo, electronic submission\n",
      "\n",
      "2. SINGAPORE_DP_STATUTORY_CERTIFICATES_VALID_DEPARTURE_097 (Score: 0.8220)\n",
      "   Type: Requirement\n",
      "   Entity: Vessel\n",
      "   Detail: Vessel must ensure all statutory and mandatory certificates are in force when proceeding to sea from Singapore.\n",
      "   Keywords: statutory certificates, mandatory certificates, valid certificates, seaworthiness, compliance\n",
      "\n",
      "3. SINGAPORE_DP_SHT_NON_COMPLIANCE_DENIAL_ENTRY_066 (Score: 0.8211)\n",
      "   Type: Regulation\n",
      "   Entity: Tanker (Single Hulled)\n",
      "   Detail: Vessels not complying with MARPOL Convention on SHTs may be denied entry into Singapore port.\n",
      "   Keywords: non-compliance, denial of entry, single hulled tanker, SHT, MARPOL, enforcement\n",
      "\n",
      "4. SINGAPORE_DP_DOCS_GENERAL_DECLARATION_008 (Score: 0.8206)\n",
      "   Type: Requirement\n",
      "   Entity: Vessel\n",
      "   Detail: All vessels arriving at Singapore Port are subject to general declaration requirements.\n",
      "   Keywords: general declaration, arrival documents, mandatory, vessel\n",
      "\n",
      "5. SINGAPORE_DP_SHT_NON_COMPLIANCE_DETENTION_067 (Score: 0.8140)\n",
      "   Type: Regulation\n",
      "   Entity: Tanker (Single Hulled)\n",
      "   Detail: Vessels not complying with MARPOL Convention on SHTs may be detained if already in Singapore port.\n",
      "   Keywords: non-compliance, detention, single hulled tanker, SHT, MARPOL, enforcement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_36250/2068890347.py:38: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# Test search by embedding\n",
    "def search_datapoints(query, filter_conditions=None, limit=5):\n",
    "    # Generate embedding for query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Build search filter\n",
    "    search_filter = None\n",
    "    if filter_conditions:\n",
    "        must_conditions = []\n",
    "        \n",
    "        for field, value in filter_conditions.items():\n",
    "            if field == \"keywords\":\n",
    "                if isinstance(value, list):\n",
    "                    must_conditions.append(\n",
    "                        models.FieldCondition(\n",
    "                            key=field,\n",
    "                            match=models.MatchAny(any=value)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    must_conditions.append(\n",
    "                        models.FieldCondition(\n",
    "                            key=field,\n",
    "                            match=models.MatchValue(value=value)\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                must_conditions.append(\n",
    "                    models.FieldCondition(\n",
    "                        key=field,\n",
    "                        match=models.MatchValue(value=value)\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        search_filter = models.Filter(must=must_conditions)\n",
    "    \n",
    "    # Execute search\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=search_filter,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with a sample query\n",
    "test_query = \"Requirements for importing hazardous materials into Singapore\"\n",
    "test_results = search_datapoints(\n",
    "    test_query,\n",
    "    filter_conditions={\"port_area\": \"Singapore\"}\n",
    ")\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nResults:\")\n",
    "for i, result in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. {result.payload.get('datapoint_id', 'No ID')} (Score: {result.score:.4f})\")\n",
    "    print(f\"   Type: {result.payload.get('datapoint_type', 'N/A')}\")\n",
    "    print(f\"   Entity: {result.payload.get('relevant_entity', 'N/A')}\")\n",
    "    print(f\"   Detail: {result.payload.get('regulation_detail', 'N/A')}\")\n",
    "    if 'keywords' in result.payload:\n",
    "        print(f\"   Keywords: {', '.join(result.payload['keywords'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: container customs requirements\n",
      "\n",
      "Results:\n",
      "\n",
      "1. INCO_DP_248_Suitability_General_AnyModeTermsRecommendedContainer (Score: 0.8383)\n",
      "   Type: Guideline\n",
      "   Entity: General\n",
      "   Detail: For container shipments, it is generally recommended to use Incoterms designed for any mode of transport (EXW, FCA, CPT, CIP, DAP, DPU, DDP).\n",
      "   Keywords: suitability, container, any mode terms, exw, fca, cpt, cip, dap, dpu, ddp, recommended\n",
      "\n",
      "2. INCO_DP_236_Suitability_EXW_Container_NotRecommended (Score: 0.8336)\n",
      "   Type: Guideline\n",
      "   Entity: EXW\n",
      "   Detail: EXW (Ex Works) is generally discouraged for international container shipments due to buyer export complexities.\n",
      "   Keywords: exw, ex works, suitability, container, not recommended, discouraged, export\n",
      "\n",
      "3. INCO_DP_013_Obligation_EXW_Buyer_ImportClearance (Score: 0.8156)\n",
      "   Type: Obligation\n",
      "   Entity: Buyer\n",
      "   Detail: Under EXW, the buyer is responsible for handling and paying for import clearance in the destination country.\n",
      "   Keywords: exw, ex works, buyer, obligation, import clearance, formalities, customs\n",
      "\n",
      "4. INCO_DP_239_Suitability_FOB_Container_NotRecommended (Score: 0.8123)\n",
      "   Type: Guideline\n",
      "   Entity: FOB\n",
      "   Detail: FOB (Free On Board) is generally not recommended for containerized goods as containers are usually delivered to yards, not loaded directly.\n",
      "   Keywords: fob, free on board, suitability, container, not recommended, container yard\n",
      "\n",
      "5. INCO_DP_090_Cost_EXW_ImportClearance_Buyer (Score: 0.8098)\n",
      "   Type: Cost Allocation\n",
      "   Entity: Buyer\n",
      "   Detail: Under EXW, the buyer bears the costs associated with import clearance.\n",
      "   Keywords: exw, ex works, cost, buyer, import clearance, formalities, customs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/rpbky_9916s5qf9w4zg2hyv00000gn/T/ipykernel_36250/2068890347.py:38: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid search with keywords\n",
    "keyword_query = \"container customs requirements\"\n",
    "keyword_results = search_datapoints(\n",
    "    keyword_query,\n",
    "    filter_conditions={\"keywords\": [\"customs\", \"container\"]}\n",
    ")\n",
    "\n",
    "print(f\"Query: {keyword_query}\")\n",
    "print(\"\\nResults:\")\n",
    "for i, result in enumerate(keyword_results, 1):\n",
    "    print(f\"\\n{i}. {result.payload.get('datapoint_id', 'No ID')} (Score: {result.score:.4f})\")\n",
    "    print(f\"   Type: {result.payload.get('datapoint_type', 'N/A')}\")\n",
    "    print(f\"   Entity: {result.payload.get('relevant_entity', 'N/A')}\")\n",
    "    print(f\"   Detail: {result.payload.get('regulation_detail', 'N/A')}\")\n",
    "    if 'keywords' in result.payload:\n",
    "        print(f\"   Keywords: {', '.join(result.payload['keywords'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: logistics_datapoints\n",
      "Points count: None\n",
      "Indexed vectors: 0\n",
      "Status: green\n",
      "\n",
      "Datapoint types:\n",
      "  - Requirement: 187\n",
      "  - Cost Allocation: 134\n",
      "  - Fact: 96\n",
      "  - Regulation: 94\n",
      "  - Procedure: 83\n",
      "  - Obligation: 75\n",
      "  - Definition: 49\n",
      "  - Benefit: 35\n",
      "  - Power: 32\n",
      "  - Rule: 25\n",
      "  - Guideline: 21\n",
      "  - Penalty: 18\n",
      "  - Tariff Value: 17\n",
      "  - Purpose: 17\n",
      "  - Document Content - Key Element: 16\n",
      "  - Documentation Requirement: 15\n",
      "  - Challenge: 13\n",
      "  - Document Function: 13\n",
      "  - Clarification: 11\n",
      "  - Risk Transfer: 11\n",
      "  - Instruction: 11\n",
      "  - Digital Platform: 10\n",
      "  - Checklist: 10\n",
      "  - Document Content - Instruction: 10\n",
      "  - Prohibition: 9\n",
      "  - Recommendation: 8\n",
      "  - Document Requirement: 8\n",
      "  - Exemption: 8\n",
      "  - Responsibility: 8\n",
      "  - Rationale: 8\n",
      "  - Notification Requirement: 7\n",
      "  - Standard: 6\n",
      "  - Difference: 6\n",
      "  - Tariff Information: 6\n",
      "  - Checklist Item: 6\n",
      "  - Information Gap: 6\n",
      "  - Service Feature: 6\n",
      "  - Capability: 5\n",
      "  - Guideline - Best Practice: 5\n",
      "  - Advantage: 5\n",
      "  - Customs Procedure: 5\n",
      "  - Feature: 5\n",
      "  - Regulatory Authority: 5\n",
      "  - Summary: 5\n",
      "  - Equipment: 4\n",
      "  - Tariff Charge: 4\n",
      "  - Function: 4\n",
      "  - Fee/Charge: 4\n",
      "  - Practice: 4\n",
      "  - Authority: 4\n",
      "  - Technology: 4\n",
      "  - Scope: 4\n",
      "  - Policy Goal: 3\n",
      "  - Certification Requirement: 3\n",
      "  - Repositioning Strategy: 3\n",
      "  - Best Practice: 3\n",
      "  - Service: 3\n",
      "  - Recommendation - Best Practice: 3\n",
      "  - Port Authority Information: 3\n",
      "  - Mechanism: 3\n",
      "  - Permit Requirement: 3\n",
      "  - Contact Information: 3\n",
      "  - Regulation Reference: 3\n",
      "  - Fact (Inferred Benefit): 3\n",
      "  - Issue/Challenge: 2\n",
      "  - Initiative: 2\n",
      "  - Consequence: 2\n",
      "  - Digital Tool: 2\n",
      "  - Document Type Attribute: 2\n",
      "  - Laytime Exclusion: 2\n",
      "  - Link to Source Document: 2\n",
      "  - Incentive Program: 2\n",
      "  - Requirement - Data Quality: 2\n",
      "  - Technical Feature: 2\n",
      "  - Digital Platform Requirement: 2\n",
      "  - Liability Clause: 2\n",
      "  - Facilitation Measure: 2\n",
      "  - Development History: 2\n",
      "  - Policy: 2\n",
      "  - Infrastructure Feature: 2\n",
      "  - Document: 2\n",
      "  - Instruction - Data Verification: 2\n",
      "  - Principle: 2\n",
      "  - Infrastructure Specification: 2\n",
      "  - System: 2\n",
      "  - Data Exchange Method: 2\n",
      "  - Focus Area: 2\n",
      "  - Requirement - Signature: 2\n",
      "  - Impact: 2\n",
      "  - Ranking: 2\n",
      "  - Context: 2\n",
      "  - Exception: 2\n",
      "  - Information Source: 2\n",
      "  - Example: 2\n",
      "  - Policy Direction: 2\n",
      "  - Documentation Mention: 2\n",
      "  - Document Type: 2\n",
      "  - Opportunity: 2\n",
      "  - Capacity Expansion: 1\n",
      "  - Guideline - Recommendation: 1\n",
      "  - Perspective: 1\n",
      "  - Special Attention: 1\n",
      "  - Deadline: 1\n",
      "  - Guideline - Data Integrity: 1\n",
      "  - Vessel Category: 1\n",
      "  - Tariff Calculation: 1\n",
      "  - Standard (Facilitation): 1\n",
      "  - Liability Exemption: 1\n",
      "  - Customs Document: 1\n",
      "  - Document Content - Operational Info: 1\n",
      "  - Recommendation - Digitalization: 1\n",
      "  - Consideration: 1\n",
      "  - Payment Term: 1\n",
      "  - Projection: 1\n",
      "  - Document Relationship: 1\n",
      "  - Attribution: 1\n",
      "  - Instruction - Prohibition: 1\n",
      "  - Key Document: 1\n",
      "  - Customs Regulation: 1\n",
      "  - Operational Information: 1\n",
      "  - Outlook: 1\n",
      "  - Port Procedure - Customs Control: 1\n",
      "  - Golden Rule: 1\n",
      "  - System Feature: 1\n",
      "  - Policy Objective: 1\n",
      "  - Fact (General SCM/Customs Risk): 1\n",
      "  - Supply Chain - Logistics: 1\n",
      "  - Checklist Category: 1\n",
      "  - Goal (Efficiency): 1\n",
      "  - Usage Statistic: 1\n",
      "  - Administrative Change: 1\n",
      "  - Pilot Program: 1\n",
      "  - Investment: 1\n",
      "  - Platform: 1\n",
      "  - Contact Point: 1\n",
      "  - Customs Concept: 1\n",
      "  - Performance Metric: 1\n",
      "  - Entitlement: 1\n",
      "  - Optional Measure: 1\n",
      "  - Technology Trend: 1\n",
      "  - Guidance: 1\n",
      "  - Access Restriction: 1\n",
      "  - Documentation Update: 1\n",
      "  - Component: 1\n",
      "  - Data Element: 1\n",
      "  - Tariff Example: 1\n",
      "  - Exclusion: 1\n",
      "  - Simplification: 1\n",
      "  - Requirement - Data Integrity: 1\n",
      "  - Status Code: 1\n",
      "  - Provisional Adjustment: 1\n",
      "  - Status: 1\n",
      "  - Vessel Category Definition: 1\n",
      "  - Fact (General SCM/Customs Principle): 1\n",
      "  - Certification: 1\n",
      "  - Implementation: 1\n",
      "  - Process Step: 1\n",
      "  - Commitment: 1\n",
      "  - Document Content: 1\n",
      "  - Port State Control - Procedure: 1\n",
      "  - Mandate (Electronic): 1\n",
      "  - Tariff Policy: 1\n",
      "  - Model: 1\n",
      "  - Amendment: 1\n",
      "  - Guideline (Seamanship): 1\n",
      "  - Real-World Impact: 1\n",
      "  - Enforcement: 1\n",
      "  - Tax Rate: 1\n",
      "  - Training Requirement: 1\n",
      "  - Right: 1\n",
      "  - Objective: 1\n",
      "  - Prediction: 1\n",
      "  - Port Procedure - Free Pratique: 1\n",
      "  - Border Control Requirement: 1\n",
      "  - Facility: 1\n",
      "  - Notice: 1\n",
      "  - Description: 1\n",
      "Error getting port area statistics: unhashable type: 'list'\n"
     ]
    }
   ],
   "source": [
    "# Get collection statistics\n",
    "collection_info = client.get_collection(collection_name)\n",
    "print(f\"Collection: {collection_name}\")\n",
    "print(f\"Points count: {collection_info.vectors_count}\")\n",
    "print(f\"Indexed vectors: {collection_info.indexed_vectors_count}\")\n",
    "print(f\"Status: {collection_info.status}\")\n",
    "\n",
    "# Get counts by datapoint_type\n",
    "try:\n",
    "    collection = client.get_collection(collection_name)\n",
    "    payload_types = {}\n",
    "    for point in client.scroll(collection_name=collection_name, limit=10000)[0]:\n",
    "        datapoint_type = point.payload.get(\"datapoint_type\")\n",
    "        if datapoint_type:\n",
    "            payload_types[datapoint_type] = payload_types.get(datapoint_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nDatapoint types:\")\n",
    "    for dtype, count in sorted(payload_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {dtype}: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting datapoint type statistics: {e}\")\n",
    "\n",
    "# Get counts by port_area\n",
    "try:\n",
    "    port_areas = {}\n",
    "    for point in client.scroll(collection_name=collection_name, limit=10000)[0]:\n",
    "        port_area = point.payload.get(\"port_area\")\n",
    "        if port_area:\n",
    "            port_areas[port_area] = port_areas.get(port_area, 0) + 1\n",
    "    \n",
    "    print(\"\\nPort areas:\")\n",
    "    for area, count in sorted(port_areas.items(), key=lambda x: x[1], reverse=True)[:10]: # Show top 10\n",
    "        print(f\"  - {area}: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting port area statistics: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
